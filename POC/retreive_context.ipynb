{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "184576ba-76ac-4196-9557-3835408a3b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import boto3\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41254706-66c6-41e9-bfe9-24a182494455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8b2722b-9c35-4632-8623-ea754efd6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = ''\n",
    "OPENAI_API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20bf113e-13a0-4835-97eb-340f164f636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY)\n",
    "pinecone_api_key = PINECONE_API_KEY\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ca45a-892d-4c53-8350-34a6a0198dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9f4bd2e-fd9e-460d-9664-2f83fe258ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'langgraph-docs'\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978226b-442d-4c88-a743-381a4143cf30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e81af73-d10f-4596-bf46-1bf9f619644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4283521-6a5c-430e-9bc4-05e893982b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "context = retriever.invoke(\"Give me code to build retreiver tool using Langgraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "454392d4-abc4-494b-982c-996ca6f0f74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='baec1f50-ee29-4e51-ad49-80a238d1009b', metadata={}, page_content='Agentic RAG\\nSkip to content\\nAgentic RAG\\nInitializing search\\nGitHub\\nHome\\nTutorials\\nHow-to Guides\\nConceptual Guides\\nReference\\nGitHub\\nHome\\nTutorials\\nTutorials\\nQuick Start\\nQuick Start\\nQuick Start\\nðŸš€ LangGraph Quick Start\\nQuick Start: Launch Local LangGraph Server\\nLangGraph Cloud Quick Start\\nChatbots\\nChatbots\\nChatbots\\nBuild a Customer Support Bot\\nPrompt Generation from User Requirements\\nCode generation with RAG and self-correction\\nRAG\\nRAG\\nRAG\\nAdaptive RAG\\nLanggraph adaptive rag local\\nAgentic RAG\\nAgentic RAG\\nTable of contents\\nSetup\\nRetriever\\nAgent State\\nNodes and Edges\\nGraph\\nCorrective RAG (CRAG)\\nCorrective RAG (CRAG) using local LLMs\\nSelf-RAG\\nSelf-RAG using local LLMs\\nAn agent for interacting with a SQL database\\nAgent Architectures\\nAgent Architectures\\nAgent Architectures\\nMulti-Agent Systems\\nPlanning Agents\\nReflection & Critique\\nEvaluation & Analysis\\nEvaluation & Analysis\\nEvaluation & Analysis\\nChat Bot Evaluation as Multi-agent Simulation\\nChat Bot Benchmarking using Simulation\\nExperimental\\nExperimental\\nExperimental\\nWeb Research (STORM)\\nTNT-LLM: Text Mining at Scale\\nWeb Voyager\\nCompetitive Programming\\nComplex data extraction with function calling\\nHow-to Guides\\nConceptual Guides\\nReference\\nTable of contents\\nSetup\\nRetriever\\nAgent State\\nNodes and Edges\\nGraph\\nHome\\nTutorials\\nRAG\\nAgentic RAG\\n¶\\nRetrieval Agents\\nare useful when we want to make decisions about whether to retrieve from an index.\\nTo implement a retrieval agent, we simply need to give an LLM access to a retriever tool.\\nWe can incorporate this into\\nLangGraph\\n.\\nSetup\\n¶\\nFirst, let\\'s download the required packages and set our API keys:\\n%%\\ncapture\\n--\\nno\\n-\\nstderr\\n%\\npip\\ninstall\\n-\\nU\\n--\\nquiet\\nlangchain\\n-\\ncommunity\\ntiktoken\\nlangchain\\n-\\nopenai\\nlangchainhub\\nchromadb\\nlangchain\\nlanggraph\\nlangchain\\n-\\ntext\\n-\\nsplitters\\n[CODE FILE: code_extracts/langgraph-docs/82c54a25ce3fbc5f4f6942f0d9cae583.py]\\nThis code defines a function `_set_env` that takes a string `key` as an argument. It checks if the specified key is not already present in the environment variables (`os.environ`). If the key is absent, it prompts the user to input a value for the key using `getpass.getpass`, which securely hides the input. Finally, it sets the environment variable with the provided value. The function is then called with the argument `\"OPENAI_API_KEY\"` to potentially set this specific environment variable.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/82c54a25ce3fbc5f4f6942f0d9cae583.py\\nSet up\\nLangSmith\\nfor LangGraph development\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started\\nhere\\n.\\nRetriever\\n¶\\nFirst, we index 3 blog posts.\\n[CODE FILE: code_extracts/langgraph-docs/7fab044907bf3484ff8c240fc1e426b0.py]\\nThis code snippet is designed to load web content from specified URLs, split the content into manageable chunks, and store those chunks in a vector database using embeddings for efficient retrieval. It uses `WebBaseLoader` to fetch documents from the provided URLs and flattens the list of documents. Then, it employs `RecursiveCharacterTextSplitter` to divide the text into smaller segments based on specified chunk size and overlap. Finally, it initializes a `Chroma` vector store with these document splits, applying OpenAI embeddings to facilitate future retrieval operations.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/7fab044907bf3484ff8c240fc1e426b0.py\\nAPI Reference:\\nWebBaseLoader\\n|\\nChroma\\n|\\nOpenAIEmbeddings\\n|\\nRecursiveCharacterTextSplitter\\nThen we create a retriever tool.\\n[CODE FILE: code_extracts/langgraph-docs/9d74701f61b533efe82f496fd6284b07.py]\\nThe code imports the `create_retriever_tool` function from the `langchain.tools.retriever` module. It then uses this function to create a retriever tool named `retriever_tool`, which is configured to search for and return information specifically about blog posts by Lilian Weng on topics related to LLM agents, prompt engineering, and adversarial attacks on LLMs. Finally, it stores this tool in a list named `tools`.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/9d74701f61b533efe82f496fd6284b07.py\\nAPI Reference:\\ncreate_retriever_tool\\nAgent State\\n¶\\nWe will define a graph.\\nA\\nstate\\nobject that it passes around to each node.\\nOur state will be a list of\\nmessages\\n.\\nEach node in our graph will append to it.\\n[CODE FILE: code_extracts/langgraph-docs/49c27b9f75432d8950e2bb663ada50aa.py]\\nThis code defines a TypedDict named `AgentState`, which represents the state of an agent with a single key, `messages`. The `messages` key is annotated to be a sequence of `BaseMessage` objects, and it uses the `add_messages` function to specify that new messages should be appended rather than replacing the existing ones. This structure allows for organized management of messages in an agent\\'s state while enforcing type safety and custom behavior for message updates.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/49c27b9f75432d8950e2bb663ada50aa.py\\nAPI Reference:\\nBaseMessage\\n|\\nadd_messages\\nNodes and Edges\\n¶\\nWe can lay out an agentic RAG graph like this:\\nThe state is a set of messages\\nEach node will update (append to) state\\nConditional edges decide which node to visit next\\nUsing Pydantic with LangChain\\nThis notebook uses Pydantic v2\\nBaseModel\\n, which requires\\nlangchain-core >= 0.3\\n. Using\\nlangchain-core < 0.3\\nwill result in errors due to mixing of Pydantic v1 and v2\\nBaseModels\\n.\\n[CODE FILE: code_extracts/langgraph-docs/5726bca5429b4a0cf60b105ef3a7c62a.py]\\nThis code defines a system for evaluating the relevance of retrieved documents in relation to a user question, and subsequently generating or rewriting responses based on this evaluation. The `grade_documents` function assesses the relevance of documents using a language model (LLM) and determines whether to generate an answer or rewrite the question. The `agent` function invokes the LLM to generate a response based on the current state, while the `rewrite` function improves the user\\'s question if the documents are deemed irrelevant. Finally, the `generate` function formulates an answer based on relevant documents. The code also includes a mechanism to visualize the prompt structure used for generating responses.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/5726bca5429b4a0cf60b105ef3a7c62a.py\\n[CODE FILE: code_extracts/langgraph-docs/5e8f8dbe9a7d33558e9f9cf530bb5fcd.py]\\nThis code snippet defines a prompt template for a question-answering assistant. It instructs the assistant to utilize provided context to answer a given question succinctly, limiting the response to a maximum of three sentences. If the assistant cannot provide an answer based on the context, it is instructed to respond with \"I don\\'t know.\"\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/5e8f8dbe9a7d33558e9f9cf530bb5fcd.py\\nAPI Reference:\\nBaseMessage\\n|\\nHumanMessage\\n|\\nStrOutputParser\\n|\\nPromptTemplate\\n|\\nChatOpenAI\\n|\\ntools_condition\\nGraph\\n¶\\nStart with an agent,\\ncall_model\\nAgent make a decision to call a function\\nIf so, then\\naction\\nto call tool (retriever)\\nThen call agent with the tool output added to messages (\\nstate\\n)\\n[CODE FILE: code_extracts/langgraph-docs/c42512dcc43ed12c05d5be2ba29504ad.py]\\nThe code defines a state graph workflow for a system that involves an agent, retrieval, rewriting, and response generation. It begins by creating a `StateGraph` object and adding several nodes representing different actions: an agent node, a retrieval tool node, a rewriting node, and a response generation node. Conditional edges are established to determine the flow based on the agent\\'s decisions, including whether to retrieve information or not. Finally, the graph is compiled into a usable state graph structure for execution.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/c42512dcc43ed12c05d5be2ba29504ad.py\\nAPI Reference:\\nEND\\n|\\nStateGraph\\n|\\nSTART\\n|\\nToolNode\\n[CODE FILE: code_extracts/langgraph-docs/0e3872de96bb5dc0452d4dd6d3c2a650.py]\\nThis code attempts to display an image generated from a graph using the IPython display module. It calls a method `get_graph()` with the argument `xray=True` from the `graph` object, which likely returns a graph representation, and then calls `draw_mermaid_png()` to produce a PNG image of that graph. If there is an exception during this process (possibly due to missing dependencies), it catches the exception and simply passes without any further action or error.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/0e3872de96bb5dc0452d4dd6d3c2a650.py\\n[CODE FILE: code_extracts/langgraph-docs/02ad2846aa9ba9a5406e28858195f9a4.py]\\nThe code imports the `pprint` module for pretty-printing data structures. It initializes a dictionary called `inputs` containing a list of messages, specifically one from the user asking about Lilian Weng\\'s views on agent memory. It then iterates over outputs generated from a `graph.stream(inputs)` function, which presumably processes the input messages. For each output, it prints the node key and its corresponding value in a formatted manner, separating each output with dashes for clarity.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/02ad2846aa9ba9a5406e28858195f9a4.py\\n[CODE FILE: code_extracts/langgraph-docs/f76a1caf6200a1db121cf58e6e86575a.py]'),\n",
       " Document(id='09628c95-1c16-4853-af63-7e071c771d38', metadata={}, page_content='Adaptive RAG\\nSkip to content\\nAdaptive RAG\\nInitializing search\\nGitHub\\nHome\\nTutorials\\nHow-to Guides\\nConceptual Guides\\nReference\\nGitHub\\nHome\\nTutorials\\nTutorials\\nQuick Start\\nQuick Start\\nQuick Start\\nðŸš€ LangGraph Quick Start\\nQuick Start: Launch Local LangGraph Server\\nLangGraph Cloud Quick Start\\nChatbots\\nChatbots\\nChatbots\\nBuild a Customer Support Bot\\nPrompt Generation from User Requirements\\nCode generation with RAG and self-correction\\nRAG\\nRAG\\nRAG\\nAdaptive RAG\\nAdaptive RAG\\nTable of contents\\nSetup\\nCreate Index\\nLLMs\\nWeb Search Tool\\nConstruct the Graph\\nDefine Graph State\\nDefine Graph Flow\\nCompile Graph\\nUse Graph\\nLanggraph adaptive rag local\\nAgentic RAG\\nCorrective RAG (CRAG)\\nCorrective RAG (CRAG) using local LLMs\\nSelf-RAG\\nSelf-RAG using local LLMs\\nAn agent for interacting with a SQL database\\nAgent Architectures\\nAgent Architectures\\nAgent Architectures\\nMulti-Agent Systems\\nPlanning Agents\\nReflection & Critique\\nEvaluation & Analysis\\nEvaluation & Analysis\\nEvaluation & Analysis\\nChat Bot Evaluation as Multi-agent Simulation\\nChat Bot Benchmarking using Simulation\\nExperimental\\nExperimental\\nExperimental\\nWeb Research (STORM)\\nTNT-LLM: Text Mining at Scale\\nWeb Voyager\\nCompetitive Programming\\nComplex data extraction with function calling\\nHow-to Guides\\nConceptual Guides\\nReference\\nTable of contents\\nSetup\\nCreate Index\\nLLMs\\nWeb Search Tool\\nConstruct the Graph\\nDefine Graph State\\nDefine Graph Flow\\nCompile Graph\\nUse Graph\\nHome\\nTutorials\\nRAG\\nAdaptive RAG\\n¶\\nAdaptive RAG is a strategy for RAG that unites (1)\\nquery analysis\\nwith (2)\\nactive / self-corrective RAG\\n.\\nIn the\\npaper\\n, they report query analysis to route across:\\nNo Retrieval\\nSingle-shot RAG\\nIterative RAG\\nLet\\'s build on this using LangGraph.\\nIn our implementation, we will route between:\\nWeb search: for questions related to recent events\\nSelf-corrective RAG: for questions related to our index\\nSetup\\n¶\\nFirst, let\\'s install our required packages and set our API keys\\n%%\\ncapture\\n--\\nno\\n-\\nstderr\\n!\\npip\\ninstall\\n-\\nU\\nlangchain_community\\ntiktoken\\nlangchain\\n-\\nopenai\\nlangchain\\n-\\ncohere\\nlangchainhub\\nchromadb\\nlangchain\\nlanggraph\\ntavily\\n-\\npython\\n[CODE FILE: code_extracts/langgraph-docs/95ccb2c89615d5a6f1850801309898d7.py]\\nThis code defines a function `_set_env` that takes a string variable name as an argument. It checks if the environment variable with that name is already set; if not, it prompts the user to input a value securely using `getpass.getpass()`. The inputted value is then stored as an environment variable. The function is called three times to set the environment variables for \"OPENAI_API_KEY\", \"COHERE_API_KEY\", and \"TAVILY_API_KEY\" if they are not already defined.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/95ccb2c89615d5a6f1850801309898d7.py\\nSet up\\nLangSmith\\nfor LangGraph development\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started\\nhere\\n.\\nCreate Index\\n¶\\n[CODE FILE: code_extracts/langgraph-docs/1727d8e66ecceb7f76fbc883766898d7.py]\\nThe provided code snippet builds an index for web documents using the Langchain library. It first retrieves content from specified URLs using the `WebBaseLoader`, then flattens the loaded documents into a single list. The text is split into manageable chunks of 500 characters with no overlap using the `RecursiveCharacterTextSplitter`. Finally, these document chunks are added to a vector store (Chroma) with OpenAI embeddings to facilitate efficient retrieval of information, and a retriever is created from this vector store for querying purposes.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/1727d8e66ecceb7f76fbc883766898d7.py\\nAPI Reference:\\nRecursiveCharacterTextSplitter\\n|\\nWebBaseLoader\\n|\\nChroma\\n|\\nOpenAIEmbeddings\\nLLMs\\n¶\\nUsing Pydantic with LangChain\\nThis notebook uses Pydantic v2\\nBaseModel\\n, which requires\\nlangchain-core >= 0.3\\n. Using\\nlangchain-core < 0.3\\nwill result in errors due to mixing of Pydantic v1 and v2\\nBaseModels\\n.\\n[CODE FILE: code_extracts/langgraph-docs/11a6504a1f65de59b27c949daea8241f.py]\\nThe provided code defines a routing mechanism for user queries, directing them to either a vector store or a web search based on the content of the question. It uses the Langchain framework, specifically the `ChatOpenAI` model, to process queries and determine the appropriate data source. The `RouteQuery` data model specifies the routing options, while a prompt instructs the model on how to categorize questions. Finally, the code invokes the routing mechanism with sample questions, demonstrating its functionality by determining which source to use for different topics.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/11a6504a1f65de59b27c949daea8241f.py\\ndatasource=\\'web_search\\'\\ndatasource=\\'vectorstore\\'\\nAPI Reference:\\nChatPromptTemplate\\n|\\nChatOpenAI\\n[CODE FILE: code_extracts/langgraph-docs/6190970babfceed8e8fda74a9dd40f0b.py]\\nThe code defines a system for grading the relevance of retrieved documents in relation to a user question. It establishes a data model, `GradeDocuments`, which holds a binary score (\"yes\" or \"no\") indicating the document\\'s relevance. A language model (LLM) is configured to process this grading task using a structured output. A prompt is created to guide the LLM in assessing the document based on its content compared to the user question. Finally, the code retrieves a document and invokes the grading process to determine and print the relevance score.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/6190970babfceed8e8fda74a9dd40f0b.py\\nbinary_score=\\'no\\'\\n[CODE FILE: code_extracts/langgraph-docs/af68e039aca6a49140e42c44fa1da4bd.py]\\nThis code snippet utilizes the Langchain library to implement a retrieval-augmented generation (RAG) system. It starts by pulling a prompt template from the Langchain hub and initializes a language model (ChatOpenAI) with specific parameters. The `format_docs` function is defined to format the retrieved documents into a string format suitable for output. A processing chain is created by chaining the prompt, language model, and a string output parser. Finally, the chain is executed with a context (documents) and a question, producing and printing the generated response.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/af68e039aca6a49140e42c44fa1da4bd.py\\nThe design of generative agents combines LLM with memory, planning, and reflection mechanisms to enable agents to behave based on past experience and interact with other agents. Memory stream is a long-term memory module that records agents\\' experiences in natural language. The retrieval model surfaces context to inform the agent\\'s behavior based on relevance, recency, and importance.\\nAPI Reference:\\nStrOutputParser\\n[CODE FILE: code_extracts/langgraph-docs/8ce5c1d8ef11e9aa1e298dc34bdebc9d.py]\\nThe provided code defines a system for evaluating whether a language model (LLM) generation is grounded in a specified set of facts. It creates a data model (`GradeHallucinations`) to store a binary score (\"yes\" or \"no\") indicating the presence of hallucinations in the generation. The LLM is set up to produce structured output according to this model, and a prompt is constructed to instruct the LLM to assess the grounding of its response based on the provided facts. Finally, the `hallucination_grader` is invoked with specific documents and a generation to obtain the binary score indicating if the generation is factually supported.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/8ce5c1d8ef11e9aa1e298dc34bdebc9d.py\\nGradeHallucinations(binary_score=\\'yes\\')\\n[CODE FILE: code_extracts/langgraph-docs/e0a0875b133d83e30d8a981e33ca362d.py]\\nThe code defines a grading system that evaluates whether a given answer addresses a specific question with a binary score of \"yes\" or \"no\". It creates a data model `GradeAnswer` to represent the binary score, and utilizes a language model (LLM) to assess the answer\\'s relevance to the question. A prompt is prepared to instruct the LLM on its grading task, and the grading process is executed using the `answer_grader`, which combines the prompt with the LLM functionality. Finally, the grading function is invoked with the specific question and the answer to be evaluated.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/e0a0875b133d83e30d8a981e33ca362d.py\\nGradeAnswer(binary_score=\\'yes\\')\\n[CODE FILE: code_extracts/langgraph-docs/fd30b87e67ec845bb5cfa5a380c5b4e2.py]\\nThe code defines a question re-writer using the GPT-3.5 Turbo model from OpenAI. It sets up a prompt that instructs the model to improve an input question to optimize it for better retrieval in a vector store. The prompt consists of a system message that explains the task and a human message that specifies the input question format. Finally, the `question_rewriter` object combines the prompt with the language model and a string output parser, and it invokes this process by passing an initial question to be reformulated.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/fd30b87e67ec845bb5cfa5a380c5b4e2.py\\n\"What is the role of memory in an agent\\'s functioning?\"\\nWeb Search Tool\\n¶\\n[CODE FILE: code_extracts/langgraph-docs/8331f873efe21b060f33c7ce134972d8.py]\\nThe code imports the `TavilySearchResults` class from the `langchain_community.tools.tavily_search` module. It then creates an instance of `TavilySearchResults` called `web_search_tool`, initializing it with a parameter `k=3`, which likely specifies that the tool should return the top 3 search results when performing a web search.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/8331f873efe21b060f33c7ce134972d8.py\\nAPI Reference:\\nTavilySearchResults\\nConstruct the Graph'),\n",
       " Document(id='4d394b5e-bdc3-45b9-abaf-bb83c6edeb67', metadata={}, page_content=\"S3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/8331f873efe21b060f33c7ce134972d8.py\\nAPI Reference:\\nTavilySearchResults\\nConstruct the Graph\\n¶\\nCapture the flow in as a graph.\\nDefine Graph State\\n¶\\n[CODE FILE: code_extracts/langgraph-docs/3cbed0118408e18cfb92cb059d47fd89.py]\\nThe provided code defines a `TypedDict` named `GraphState`, which is a structured way to represent a dictionary with specific key-value types in Python. It includes three attributes: `question`, which is a string representing a question; `generation`, a string representing the output of a language model (LLM) generation; and `documents`, a list of strings that contains related documents. This structure helps enforce type checking and improves code clarity when handling graph state data.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/3cbed0118408e18cfb92cb059d47fd89.py\\nDefine Graph Flow\\n¶\\n[CODE FILE: code_extracts/langgraph-docs/ea5801800c16282cc710dc269a110016.py]\\nThe provided code defines a sequence of functions that facilitate a document retrieval and answer generation workflow. The `retrieve` function fetches documents related to a user's question, while `generate` produces an answer based on those documents. The `grade_documents` function filters out irrelevant documents, and `transform_query` refines the original question for better results. The `web_search` function allows for online searching if needed. Routing and decision-making functions determine the next steps based on the relevance of documents and the quality of generated answers, ensuring the system effectively addresses user inquiries.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/ea5801800c16282cc710dc269a110016.py\\nAPI Reference:\\nDocument\\nCompile Graph\\n¶\\n[CODE FILE: code_extracts/langgraph-docs/f95e119450341190b5edef0e98805b87.py]\\nThis code snippet defines a state machine workflow using the `StateGraph` class from the `langgraph` library. It creates a series of nodes representing different steps in a process (such as web searching, retrieving documents, grading them, and generating results). The code establishes directed edges between these nodes to represent the flow of the process, including conditional transitions based on specific criteria. Finally, it compiles the workflow into an executable application.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/f95e119450341190b5edef0e98805b87.py\\nAPI Reference:\\nEND\\n|\\nStateGraph\\n|\\nSTART\\nUse Graph\\n¶\\n[CODE FILE: code_extracts/langgraph-docs/54f7be3ac3518d959bb8539a194049a2.py]\\nThis code is likely part of an application that processes a user query and generates a response through a series of nodes. It sends a question about an NFL draft player to the `app.stream()` method, which yields outputs iteratively. For each output, it prints the node's key and value using the `pprint` function for better readability. After processing all nodes, it prints the final generation value associated with the last output. This setup is typically used for debugging or tracking the flow of data through a multi-step processing pipeline.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/54f7be3ac3518d959bb8539a194049a2.py\\n[CODE FILE: code_extracts/langgraph-docs/c0b56cc317f76d234b55f05fd5fbcb14.py]\\nThe code snippet appears to be part of a decision-making process for generating a response to a user query about the Chicago Bears and the NFL draft. It routes the question to a web search, checks the relevance of the generated response against documents, and confirms that the generation is grounded in reliable information. Ultimately, the code produces a response that discusses the Bears' potential to draft first in the 2024 NFL draft, mentioning both a defensive player and a wide receiver as notable prospects.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/c0b56cc317f76d234b55f05fd5fbcb14.py\\nTrace:\\nhttps://smith.langchain.com/public/7e3aa7e5-c51f-45c2-bc66-b34f17ff2263/r\\n[CODE FILE: code_extracts/langgraph-docs/7411dc1b9d4afc1d06c4e9771bf7068b.py]\\nThis code snippet initiates a process to query an application (`app`) with a specific input question about types of agent memory. It iterates over the streaming output from the application, where each output is a dictionary containing nodes, printing the node keys for each output. Additionally, there is a commented-out option to print detailed information about the node's state. After processing all outputs, it prints the final generation result from the last output received.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/7411dc1b9d4afc1d06c4e9771bf7068b.py\\n[CODE FILE: code_extracts/langgraph-docs/59c6482dd596b3e27d696271ee29cc81.py]\\nThe code snippet describes a process for handling a question and generating a relevant response using a document retrieval and grading system. It begins by routing the question to a retrieval system, which assesses the relevance of documents against the question. Based on the grades assigned to these documents, it decides whether to generate a response. The generation phase checks for accuracy and relevance, ultimately producing a detailed answer regarding types of agent memory, including Sensory Memory, Short-Term Memory, and Long-Term Memory.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/59c6482dd596b3e27d696271ee29cc81.py\\nTrace:\\nhttps://smith.langchain.com/public/fdf0a180-6d15-4d09-bb92-f84f2105ca51/r\\nComments\\nBack to top\\nPrevious\\nCode generation with RAG and self-correction\\nNext\\nLanggraph adaptive rag local\\nMade with\\nMaterial for MkDocs Insiders\"),\n",
       " Document(id='e1f40e30-9b9d-4384-a94c-579d3a38ed5c', metadata={}, page_content='API Reference:\\nDocument\\nCompile Graph\\n¶\\nThe just follows the flow we outlined in the figure above.\\n[CODE FILE: code_extracts/langgraph-docs/765c849424bc526f544dc7925931b174.py]\\nThis code snippet creates a state graph workflow using the `StateGraph` class from the `langgraph` library. It defines several nodes representing different tasks (e.g., retrieving, grading documents, generating outputs) and establishes directed edges between these nodes to represent the workflow\\'s flow. Conditional edges are added based on a decision function (`decide_to_generate`), allowing the workflow to branch into two potential paths. Finally, the graph is compiled into an executable application object (`app`).\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/765c849424bc526f544dc7925931b174.py\\nAPI Reference:\\nEND\\n|\\nStateGraph\\n|\\nSTART\\nUse the graph\\n¶\\n[CODE FILE: code_extracts/langgraph-docs/d0f7207abb694377916889b98d9d488d.py]\\nThis code snippet appears to interact with an application (likely a language model or similar system) to generate responses based on a given input question. It initializes a dictionary with a question about agent memory types and iterates through the streaming outputs from the `app.stream(inputs)` method. For each output received, it prints the key of each node along with an optional commented-out line that could print additional details about the node\\'s state. Finally, it prints the \"generation\" attribute of the last output received, which likely contains the final response or result generated by the application.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/d0f7207abb694377916889b98d9d488d.py\\n[CODE FILE: code_extracts/langgraph-docs/084fb246ed417159209c012c92729de0.py]\\nThe provided code snippet outlines a structured process for retrieving and assessing documents in response to a question. It includes nodes for retrieving documents, grading their relevance, transforming the query if necessary, conducting a web search, and generating responses. The process indicates that multiple documents were evaluated for relevance, leading to a transformation of the query when all documents were deemed not relevant. The final node suggests that the agents have a memory system for in-context and long-term learning, which supports their ability to retain information.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/084fb246ed417159209c012c92729de0.py\\n[CODE FILE: code_extracts/langgraph-docs/6099a04e29c00f06e60e81d14db4c50b.py]\\nThis code imports the `pprint` function for pretty-printing output and initializes a dictionary called `inputs` with a question about the AlphaCodium paper. It then iterates over the outputs generated by a method `app.stream(inputs)`, where it prints the key of each output node in a formatted manner. After processing all nodes, it prints a separator line. Finally, it prints the \"generation\" value from the last processed output node, which likely contains a final result or conclusion related to the initial question.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/6099a04e29c00f06e60e81d14db4c50b.py\\n[CODE FILE: code_extracts/langgraph-docs/05da3a76c380d338ccb6cc0779c4b9e4.py]\\nThe provided code snippet outlines a workflow for processing a query related to document relevance. It includes nodes for retrieving documents, grading their relevance to a specific question, transforming the query if needed, and performing a web search. The grading phase indicates that most documents are not relevant, leading to a transformation of the query. The final output describes the AlphaCodium paper, which details a method for iteratively generating and refining code through testing, aimed at enhancing the effectiveness of Large Language Models on coding tasks.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/05da3a76c380d338ccb6cc0779c4b9e4.py\\nLangSmith Traces -\\nhttps://smith.langchain.com/public/f6b1716c-e842-4282-9112-1026b93e246b/r\\nhttps://smith.langchain.com/public/497c8ed9-d9e2-429e-8ada-e64de3ec26c9/r\\nComments\\nBack to top\\nPrevious\\nAgentic RAG\\nNext\\nCorrective RAG (CRAG) using local LLMs\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='2b0edb1f-74b5-4a7f-9b6a-3e9b143dbadd', metadata={}, page_content='- The `predict_base_case` function invokes a code generation chain with a context and the user\\'s question, returning a dictionary containing any necessary imports and the generated code.\\n- The `predict_langgraph` function interacts with an application to generate a response based on the user\\'s question, also returning imports and generated code from the response. Both functions aim to facilitate code generation by processing user input.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/cd5a97abada52a040d436b04f7ff62c7.py\\n[CODE FILE: code_extracts/langgraph-docs/03ea4440fe694ab570bc408a51fca670.py]\\nThis code imports the `evaluate` function from the `langsmith.evaluation` module, which is likely used for assessing code or models. It then initializes a list called `code_evalulator` containing two functions: `check_import` and `check_execution`, which presumably check for the successful import and execution of code, respectively. Lastly, it defines a variable `dataset_name` and assigns it the string \"lcel-teacher-eval,\" indicating the dataset that will be used for evaluation.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/03ea4440fe694ab570bc408a51fca670.py\\n[CODE FILE: code_extracts/langgraph-docs/877061e888445bfdf8f8c50feaf15ff3.py]\\nThis code snippet attempts to evaluate a predictive model (`predict_base_case`) using a specified dataset (`dataset_name`) and a set of evaluators (`code_evalulator`). It sets up the evaluation with a specific prefix for experiment naming and limits concurrency to 2. If the evaluation fails, it catches the exception and prints a message prompting the user to set up LangSmith.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/877061e888445bfdf8f8c50feaf15ff3.py\\n[CODE FILE: code_extracts/langgraph-docs/a4313253b7965e04db2c7499150c2397.py]\\nThe code attempts to evaluate a language model (likely using the LangSmith framework) with the function `evaluate`, passing in several parameters such as the prediction function `predict_langgraph`, a dataset name, and evaluators. It sets a prefix for the experiment and limits the maximum concurrency to 2. Additionally, it includes metadata about the experiment, such as the language model used and a feedback flag. If an error occurs during the evaluation, it prints a message prompting the user to set up LangSmith.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/a4313253b7965e04db2c7499150c2397.py\\nResults:\\nLangGraph outperforms base case\\n: adding re-try loop improve performance\\nReflection did not help\\n: reflection prior to re-try regression vs just passing errors directly back to the LLM\\nGPT-4 outperforms Claude3\\n: Claude3 had 3 and 1 run fail due to tool-use error for Opus and Haiku, respectively\\nhttps://smith.langchain.com/public/78a3d858-c811-4e46-91cb-0f10ef56260b/d\\nComments\\nBack to top\\nPrevious\\nPrompt Generation from User Requirements\\nNext\\nAdaptive RAG\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='df16bca9-0606-4b3f-b3b3-10a4324ed197', metadata={}, page_content='RunnableConfig\\n|\\nEND\\n|\\nSTART\\n|\\nStateGraph\\nDefine the graph\\n¶\\nWe can now put it all together and define the graph!\\n[CODE FILE: code_extracts/langgraph-docs/57b7570100abe31d60715f8e86eee88a.py]\\nThe code defines a state graph workflow using a `StateGraph` class, which allows for the modeling of a process with nodes and edges. It creates two nodes, \"agent\" and \"tools\", and establishes \"agent\" as the entry point of the workflow. A conditional edge is also defined, allowing the workflow to transition from \"agent\" to either \"tools\" or an end state based on the result of the `should_continue` function. Finally, it creates a cyclic transition from \"tools\" back to \"agent\" and compiles the entire workflow into a runnable application.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/57b7570100abe31d60715f8e86eee88a.py\\nfrom\\nIPython.display\\nimport\\nImage\\n,\\ndisplay\\ndisplay\\n(\\nImage\\n(\\napp\\n.\\nget_graph\\n()\\n.\\ndraw_mermaid_png\\n()))\\nStreaming LLM Tokens\\n¶\\nYou can access the LLM tokens as they are produced by each node. \\nIn this case only the \"agent\" node produces LLM tokens.\\nIn order for this to work properly, you must be using an LLM that supports streaming as well as have set it when constructing the LLM (e.g.\\nChatOpenAI(model=\"gpt-3.5-turbo-1106\", streaming=True)\\n)\\n[CODE FILE: code_extracts/langgraph-docs/6c417721e5ab1a5ffb9a75a0f832e868.py]\\nThis code snippet is designed to interact with an asynchronous message streaming application, presumably to obtain responses from an AI model. It initializes a message from a human asking about the weather in San Francisco and listens for incoming messages in a streaming manner. As messages are received, it checks if they are from the AI (not human) and prints their content. It specifically collects chunks of AI messages, concatenating them, and if there are any tool call chunks associated with the AI message, it prints those as well.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/6c417721e5ab1a5ffb9a75a0f832e868.py\\n[CODE FILE: code_extracts/langgraph-docs/2ebcb2fe3408318bbecb0446439142d1.py]\\nThe code consists of a series of structured JSON-like objects that represent calls to a search tool. Each object includes the name of the tool (\\'search\\'), an associated unique identifier, and arguments which specify the search query. Initially, the search queries are empty or partial, progressively building up to a complete query for \"weather in San Francisco.\" The final output indicates the result of the search, providing a weather report for San Francisco, stating that it is \"cloudy with a chance of hail.\"\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/2ebcb2fe3408318bbecb0446439142d1.py\\nAPI Reference:\\nAIMessageChunk\\n|\\nHumanMessage\\nComments\\nBack to top\\nPrevious\\nHow to stream state updates of your graph\\nNext\\nHow to stream LLM tokens (without LangChain LLMs)\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='36abb7aa-7fdf-4836-b966-8b243d867ae0', metadata={}, page_content='langchain-core >= 0.3\\n. Using\\nlangchain-core < 0.3\\nwill result in errors due to mixing of Pydantic v1 and v2\\nBaseModels\\n.\\n[CODE FILE: code_extracts/langgraph-docs/e74478912915d51d88ded5e1ff65ae34.py]\\nThis code defines a function to select tools based on the last message in a conversation state. It first checks if the last message is from a human; if so, it uses the message content as the query. If the last message is a system message, it generates a query using an LLM (Language Model) to suggest tools. The results are filtered based on a simulated error condition that removes a specific tool from the selection. Additionally, it constructs a state graph to manage the flow between different nodes, incorporating the tool selection process and retry logic for robustness.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/e74478912915d51d88ded5e1ff65ae34.py\\nAPI Reference:\\nHumanMessage\\n|\\nSystemMessage\\n|\\nToolMessage\\n[CODE FILE: code_extracts/langgraph-docs/c38faab00f5abf9556934cad8033a689.py]\\nThe code snippet imports the `Image` and `display` functions from the `IPython.display` module. It attempts to display an image of a graph in PNG format generated by the `draw_mermaid_png()` method of a `graph` object. If an exception occurs (likely due to missing dependencies for the graph rendering), it catches the exception and simply passes, allowing the program to continue without interruption.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/c38faab00f5abf9556934cad8033a689.py\\nuser_input\\n=\\n\"Can you give me some information about AMD in 2022?\"\\nresult\\n=\\ngraph\\n.\\ninvoke\\n({\\n\"messages\"\\n:\\n[(\\n\"user\"\\n,\\nuser_input\\n)]})\\nfor\\nmessage\\nin\\nresult\\n[\\n\"messages\"\\n]:\\nmessage\\n.\\npretty_print\\n()\\n[CODE FILE: code_extracts/langgraph-docs/390498572aba42ab351d1e1410e9cc1b.py]\\nThe code snippet illustrates a sequence of interactions between a user and an AI system regarding financial information for AMD (Advanced Micro Devices) in 2022. Initially, the user requests information about AMD\\'s performance for that year. The AI uses tool calls to gather data, first querying Accenture for general revenue information and then specifically querying Advanced Micro Devices. The final output indicates that AMD had revenues of $100 in 2022.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/390498572aba42ab351d1e1410e9cc1b.py\\nNext steps\\n¶\\nThis guide provides a minimal implementation for dynamically selecting tools. There is a host of possible improvements and optimizations:\\nRepeating tool selection\\n: Here, we repeated tool selection by modifying the\\nselect_tools\\nnode. Another option is to equip the agent with a\\nreselect_tools\\ntool, allowing it to re-select tools at its discretion.\\nOptimizing tool selection\\n: In general, the full scope of\\nretrieval solutions\\nare available for tool selection. Additional options include:\\nGroup tools and retrieve over groups;\\nUse a chat model to select tools or groups of tool.\\nComments\\nBack to top\\nPrevious\\nHow to pass config to tools\\nNext\\nHow to add and use subgraphs\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='e69efaee-bfc5-4eef-bb6e-7d104501b0af', metadata={}, page_content='S3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/2fb8b05bb06380c19e249fdd198a8b36.py\\nComments\\nBack to top\\nPrevious\\nBuild a Customer Support Bot\\nNext\\nCode generation with RAG and self-correction\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='95cd49b3-f897-413f-a577-c481de08fe9a', metadata={}, page_content='Rebuild Graph at Runtime\\nSkip to content\\nRebuild Graph at Runtime\\nInitializing search\\nGitHub\\nHome\\nTutorials\\nHow-to Guides\\nConceptual Guides\\nReference\\nGitHub\\nHome\\nTutorials\\nHow-to Guides\\nHow-to Guides\\nLangGraph\\nLangGraph\\nLangGraph\\nControllability\\nPersistence\\nMemory\\nHuman-in-the-loop\\nStreaming\\nTool calling\\nSubgraphs\\nState Management\\nOther\\nPrebuilt ReAct Agent\\nLangGraph Platform\\nLangGraph Platform\\nLangGraph Platform\\nApplication Structure\\nApplication Structure\\nApplication Structure\\nHow to Set Up a LangGraph Application for Deployment\\nHow to Set Up a LangGraph Application for Deployment\\nHow to Set Up a LangGraph.js Application for Deployment\\nHow to customize Dockerfile\\nHow to test a LangGraph app locally\\nRebuild Graph at Runtime\\nRebuild Graph at Runtime\\nTable of contents\\nPrerequisites\\nDefine graphs\\nNo rebuild\\nRebuild\\nDeployment\\nAssistants\\nThreads\\nRuns\\nStreaming\\nHuman-in-the-loop\\nDouble-texting\\nWebhooks\\nCron Jobs\\nLangGraph Studio\\nTroubleshooting\\nTroubleshooting\\nTroubleshooting\\nGRAPH_RECURSION_LIMIT\\nINVALID_CONCURRENT_GRAPH_UPDATE\\nINVALID_GRAPH_NODE_RETURN_VALUE\\nMULTIPLE_SUBGRAPHS\\nConceptual Guides\\nReference\\nTable of contents\\nPrerequisites\\nDefine graphs\\nNo rebuild\\nRebuild\\nHome\\nHow-to Guides\\nLangGraph Platform\\nApplication Structure\\nRebuild Graph at Runtime\\n¶\\nYou might need to rebuild your graph with a different configuration for a new run. For example, you might need to use a different graph state or graph structure depending on the config. This guide shows how you can do this.\\nNote\\nIn most cases, customizing behavior based on the config should be handled by a single graph where each node can read a config and change its behavior based on it\\nPrerequisites\\n¶\\nMake sure to check out\\nthis how-to guide\\non setting up your app for deployment first.\\nDefine graphs\\n¶\\nLet\\'s say you have an app with a simple graph that calls an LLM and returns the response to the user. The app file directory looks like the following:\\n[CODE FILE: code_extracts/langgraph-docs/f4c047d2c4203a3b88b815bf9cd70588.py]\\nThe provided code structure indicates a simple Python project named \"my-app.\" It contains a `requirements.txt` file that typically lists the dependencies needed for the project, ensuring that the correct packages are installed. The `.env` file is used for environment variables, often to store sensitive information like API keys or configuration settings. The `openai_agent.py` file likely contains the main code for the application, which may involve interacting with the OpenAI API to perform tasks such as generating text or processing data using AI models.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/f4c047d2c4203a3b88b815bf9cd70588.py\\nwhere the graph is defined in\\nopenai_agent.py\\n.\\nNo rebuild\\n¶\\nIn the standard LangGraph API configuration, the server uses the compiled graph instance that\\'s defined at the top level of\\nopenai_agent.py\\n, which looks like the following:\\n[CODE FILE: code_extracts/langgraph-docs/05b25543c3be145bde27abdc877a3ce7.py]\\nThis code snippet sets up a workflow using LangChain and LangGraph to create an AI agent. It initializes a `ChatOpenAI` model with a specified temperature for response variability. A `MessageGraph` is created to represent the workflow, where the agent node is connected to the starting point (START) and the endpoint (END) through edges. Finally, the workflow is compiled into an executable agent.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/05b25543c3be145bde27abdc877a3ce7.py\\nTo make the server aware of your graph, you need to specify a path to the variable that contains the\\nCompiledStateGraph\\ninstance in your LangGraph API configuration (\\nlanggraph.json\\n), e.g.:\\n[CODE FILE: code_extracts/langgraph-docs/b097d15da00ca90ae2a9db6fb1291873.py]\\nThis code snippet appears to be a configuration file, likely in JSON format, used for defining dependencies and environment settings for a project. The \"dependencies\" field specifies that the current directory (represented by \".\") is a dependency. The \"graphs\" section defines a mapping where \"openai_agent\" is associated with a specific function or object, \"agent\", located in the \"openai_agent.py\" file. Finally, the \"env\" field indicates the path to an environment variable file (\".env\") that may contain configuration settings for the environment in which the code will run.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/b097d15da00ca90ae2a9db6fb1291873.py\\nRebuild\\n¶\\nTo make your graph rebuild on each new run with custom configuration, you need to rewrite\\nopenai_agent.py\\nto instead provide a\\nfunction\\nthat takes a config and returns a graph (or compiled graph) instance. Let\\'s say we want to return our existing graph for user ID \\'1\\', and a tool-calling agent for other users. We can modify\\nopenai_agent.py\\nas follows:\\n[CODE FILE: code_extracts/langgraph-docs/b026bc925d5f26ac627341181b767e6b.py]\\nThe code defines two functions to create different graph workflows for a language model agent using the Langchain and Langgraph libraries. The `make_default_graph` function sets up a simple state graph where the agent processes messages using the `ChatOpenAI` model. The `make_alternative_graph` function creates a more complex graph that incorporates a tool for adding numbers, allowing the agent to handle tool calls within the message flow. Finally, the `make_graph` function decides which graph to create based on a user ID from the provided configuration, returning either the default or alternative graph.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/b026bc925d5f26ac627341181b767e6b.py\\nFinally, you need to specify the path to your graph-making function (\\nmake_graph\\n) in\\nlanggraph.json\\n:\\n[CODE FILE: code_extracts/langgraph-docs/0b48a39852e679e71b1b842ac3002e97.py]\\nThis code snippet is a configuration file in JSON format, likely used for defining dependencies and environment settings for a project. It specifies that the project depends on the current directory (\".\"), includes a graph definition that maps the key \"openai_agent\" to a function called `make_graph` located in the `openai_agent.py` file, and indicates that the environment variables should be loaded from a file named `.env`.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/0b48a39852e679e71b1b842ac3002e97.py\\nSee more info on LangGraph API configuration file\\nhere\\nComments\\nBack to top\\nPrevious\\nHow to test a LangGraph app locally\\nNext\\nHow to Deploy to LangGraph Cloud\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='0a1c493f-ccc4-4691-8a7a-580b4d35607c', metadata={}, page_content='S3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/3cbed0118408e18cfb92cb059d47fd89.py\\n[CODE FILE: code_extracts/langgraph-docs/1267d0272a5060ede55730f6d9bb4d5a.py]\\nThe provided code defines a series of functions that work together to process a query by retrieving relevant documents, assessing their relevance, and generating an answer based on those documents. The `retrieve` function fetches documents related to a question, while `grade_documents` filters out irrelevant documents. The `generate` function creates a response using the relevant documents and the original question. Additionally, `transform_query` can rephrase the question if no relevant documents are found. Finally, the `decide_to_generate` and `grade_generation_v_documents_and_question` functions determine whether to generate an answer or rephrase the question based on the relevance and quality of the generated response.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/1267d0272a5060ede55730f6d9bb4d5a.py\\nBuild Graph\\n¶\\nThis just follows the flow we outlined in the figure above.\\n[CODE FILE: code_extracts/langgraph-docs/6c1edbd2002a6cf03d009edf8d5cf6c0.py]\\nThis code snippet creates a workflow using a state graph model from the `langgraph` library. It defines nodes representing various tasks like retrieving data, grading documents, generating responses, and transforming queries. The edges between nodes establish the flow of the workflow, including conditional paths based on decisions made during the grading process. Finally, the workflow is compiled into an executable application that can be used to process inputs according to the defined state transitions.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/6c1edbd2002a6cf03d009edf8d5cf6c0.py\\nAPI Reference:\\nEND\\n|\\nStateGraph\\n|\\nSTART\\nRun\\n¶\\n[CODE FILE: code_extracts/langgraph-docs/7fa5933e898c238917b32c72fe8ee4d3.py]\\nThis code snippet is designed to interact with an application (presumably a language model or similar AI) that processes a given input question. It initializes a dictionary with a question about agent memory types and then streams the output from the application. For each output received, it prints the key (representing a node) along with its details. After processing all nodes, it prints the final generated response associated with the last node. The use of `pprint` helps in formatting the output for better readability.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/7fa5933e898c238917b32c72fe8ee4d3.py\\n[CODE FILE: code_extracts/langgraph-docs/aca2a2bb45d660a1240ad30a4d8d3e9c.py]\\nThe code snippet appears to represent a sequence of operations in a large language model (LLM)-powered autonomous agent system. It includes a retrieval process where documents are checked for relevance to a question, resulting in multiple documents being deemed relevant. Following this, the system assesses these graded documents and decides to generate a response. The generation is confirmed to be grounded in the retrieved documents and addresses the initial question, leading to a final output that discusses the role of memory in such agent systems, comparing it to human memory types and suggesting mechanisms for synthesizing past information into future behavior.\\nS3 Location: s3://python-open-source-coding-assistant/code_extracts/langgraph-docs/aca2a2bb45d660a1240ad30a4d8d3e9c.py\\nTrace:\\nhttps://smith.langchain.com/public/4163a342-5260-4852-8602-bda3f95177e7/r\\nComments\\nBack to top\\nPrevious\\nSelf-RAG\\nNext\\nAn agent for interacting with a SQL database\\nMade with\\nMaterial for MkDocs Insiders')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f0d1347-7730-4c67-a0bb-5e97f6f240d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_s3_locations_with_content(docs):\n",
    " \n",
    "    s3_client =  boto3.client('s3', aws_access_key_id='',\n",
    "                      aws_secret_access_key='')\n",
    "    \n",
    "\n",
    "    for doc in docs:\n",
    "        s3_locations = re.findall(r's3://[^\\s\\n]+', doc.page_content)\n",
    "        for s3_location in s3_locations:\n",
    "            try:\n",
    "                bucket, key = s3_location.replace('s3://', '').split('/', 1)\n",
    "                response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "                file_content = response['Body'].read().decode('utf-8')\n",
    "                doc.page_content = doc.page_content.replace(s3_location, file_content)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {s3_location}: {e}\")\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72f085e6-334e-42e3-92d4-5ff17e8eec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_context = replace_s3_locations_with_content(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0081040-8be1-4390-9219-15b3ab2a737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_code_file_placeholders(docs):\n",
    "    \"\"\"\n",
    "    Remove code file placeholders from documents\n",
    "    \n",
    "    Args:\n",
    "        docs (list): List of documents\n",
    "    \n",
    "    Returns:\n",
    "        list: Documents with code file placeholders removed\n",
    "    \"\"\"\n",
    "    for doc in docs:\n",
    "        doc.page_content = re.sub(r'\\[CODE FILE: [^\\]]+\\]', '', doc.page_content)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "baabe581-b3c1-4370-85fe-979b96dd829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_context_1 = remove_code_file_placeholders(updated_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a0d494a-385b-497e-be8f-02cac25cb89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='baec1f50-ee29-4e51-ad49-80a238d1009b', metadata={}, page_content='Agentic RAG\\nSkip to content\\nAgentic RAG\\nInitializing search\\nGitHub\\nHome\\nTutorials\\nHow-to Guides\\nConceptual Guides\\nReference\\nGitHub\\nHome\\nTutorials\\nTutorials\\nQuick Start\\nQuick Start\\nQuick Start\\nðŸš€ LangGraph Quick Start\\nQuick Start: Launch Local LangGraph Server\\nLangGraph Cloud Quick Start\\nChatbots\\nChatbots\\nChatbots\\nBuild a Customer Support Bot\\nPrompt Generation from User Requirements\\nCode generation with RAG and self-correction\\nRAG\\nRAG\\nRAG\\nAdaptive RAG\\nLanggraph adaptive rag local\\nAgentic RAG\\nAgentic RAG\\nTable of contents\\nSetup\\nRetriever\\nAgent State\\nNodes and Edges\\nGraph\\nCorrective RAG (CRAG)\\nCorrective RAG (CRAG) using local LLMs\\nSelf-RAG\\nSelf-RAG using local LLMs\\nAn agent for interacting with a SQL database\\nAgent Architectures\\nAgent Architectures\\nAgent Architectures\\nMulti-Agent Systems\\nPlanning Agents\\nReflection & Critique\\nEvaluation & Analysis\\nEvaluation & Analysis\\nEvaluation & Analysis\\nChat Bot Evaluation as Multi-agent Simulation\\nChat Bot Benchmarking using Simulation\\nExperimental\\nExperimental\\nExperimental\\nWeb Research (STORM)\\nTNT-LLM: Text Mining at Scale\\nWeb Voyager\\nCompetitive Programming\\nComplex data extraction with function calling\\nHow-to Guides\\nConceptual Guides\\nReference\\nTable of contents\\nSetup\\nRetriever\\nAgent State\\nNodes and Edges\\nGraph\\nHome\\nTutorials\\nRAG\\nAgentic RAG\\n¶\\nRetrieval Agents\\nare useful when we want to make decisions about whether to retrieve from an index.\\nTo implement a retrieval agent, we simply need to give an LLM access to a retriever tool.\\nWe can incorporate this into\\nLangGraph\\n.\\nSetup\\n¶\\nFirst, let\\'s download the required packages and set our API keys:\\n%%\\ncapture\\n--\\nno\\n-\\nstderr\\n%\\npip\\ninstall\\n-\\nU\\n--\\nquiet\\nlangchain\\n-\\ncommunity\\ntiktoken\\nlangchain\\n-\\nopenai\\nlangchainhub\\nchromadb\\nlangchain\\nlanggraph\\nlangchain\\n-\\ntext\\n-\\nsplitters\\n\\nThis code defines a function `_set_env` that takes a string `key` as an argument. It checks if the specified key is not already present in the environment variables (`os.environ`). If the key is absent, it prompts the user to input a value for the key using `getpass.getpass`, which securely hides the input. Finally, it sets the environment variable with the provided value. The function is then called with the argument `\"OPENAI_API_KEY\"` to potentially set this specific environment variable.\\nS3 Location: import getpass\\nimport os\\n\\n\\ndef _set_env(key: str):\\n    if key not in os.environ:\\n        os.environ[key] = getpass.getpass(f\"{key}:\")\\n\\n\\n_set_env(\"OPENAI_API_KEY\")\\nSet up\\nLangSmith\\nfor LangGraph development\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started\\nhere\\n.\\nRetriever\\n¶\\nFirst, we index 3 blog posts.\\n\\nThis code snippet is designed to load web content from specified URLs, split the content into manageable chunks, and store those chunks in a vector database using embeddings for efficient retrieval. It uses `WebBaseLoader` to fetch documents from the provided URLs and flattens the list of documents. Then, it employs `RecursiveCharacterTextSplitter` to divide the text into smaller segments based on specified chunk size and overlap. Finally, it initializes a `Chroma` vector store with these document splits, applying OpenAI embeddings to facilitate future retrieval operations.\\nS3 Location: from langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\nurls = [\\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\\n]\\n\\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=100, chunk_overlap=50\\n)\\ndoc_splits = text_splitter.split_documents(docs_list)\\n\\n# Add to vectorDB\\nvectorstore = Chroma.from_documents(\\n    documents=doc_splits,\\n    collection_name=\"rag-chroma\",\\n    embedding=OpenAIEmbeddings(),\\n)\\nretriever = vectorstore.as_retriever()\\nAPI Reference:\\nWebBaseLoader\\n|\\nChroma\\n|\\nOpenAIEmbeddings\\n|\\nRecursiveCharacterTextSplitter\\nThen we create a retriever tool.\\n\\nThe code imports the `create_retriever_tool` function from the `langchain.tools.retriever` module. It then uses this function to create a retriever tool named `retriever_tool`, which is configured to search for and return information specifically about blog posts by Lilian Weng on topics related to LLM agents, prompt engineering, and adversarial attacks on LLMs. Finally, it stores this tool in a list named `tools`.\\nS3 Location: from langchain.tools.retriever import create_retriever_tool\\n\\nretriever_tool = create_retriever_tool(\\n    retriever,\\n    \"retrieve_blog_posts\",\\n    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\\n)\\n\\ntools = [retriever_tool]\\nAPI Reference:\\ncreate_retriever_tool\\nAgent State\\n¶\\nWe will define a graph.\\nA\\nstate\\nobject that it passes around to each node.\\nOur state will be a list of\\nmessages\\n.\\nEach node in our graph will append to it.\\n\\nThis code defines a TypedDict named `AgentState`, which represents the state of an agent with a single key, `messages`. The `messages` key is annotated to be a sequence of `BaseMessage` objects, and it uses the `add_messages` function to specify that new messages should be appended rather than replacing the existing ones. This structure allows for organized management of messages in an agent\\'s state while enforcing type safety and custom behavior for message updates.\\nS3 Location: from typing import Annotated, Sequence\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain_core.messages import BaseMessage\\n\\nfrom langgraph.graph.message import add_messages\\n\\n\\nclass AgentState(TypedDict):\\n    # The add_messages function defines how an update should be processed\\n    # Default is to replace. add_messages says \"append\"\\n    messages: Annotated[Sequence[BaseMessage], add_messages]\\nAPI Reference:\\nBaseMessage\\n|\\nadd_messages\\nNodes and Edges\\n¶\\nWe can lay out an agentic RAG graph like this:\\nThe state is a set of messages\\nEach node will update (append to) state\\nConditional edges decide which node to visit next\\nUsing Pydantic with LangChain\\nThis notebook uses Pydantic v2\\nBaseModel\\n, which requires\\nlangchain-core >= 0.3\\n. Using\\nlangchain-core < 0.3\\nwill result in errors due to mixing of Pydantic v1 and v2\\nBaseModels\\n.\\n\\nThis code defines a system for evaluating the relevance of retrieved documents in relation to a user question, and subsequently generating or rewriting responses based on this evaluation. The `grade_documents` function assesses the relevance of documents using a language model (LLM) and determines whether to generate an answer or rewrite the question. The `agent` function invokes the LLM to generate a response based on the current state, while the `rewrite` function improves the user\\'s question if the documents are deemed irrelevant. Finally, the `generate` function formulates an answer based on relevant documents. The code also includes a mechanism to visualize the prompt structure used for generating responses.\\nS3 Location: from typing import Annotated, Literal, Sequence\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain import hub\\nfrom langchain_core.messages import BaseMessage, HumanMessage\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import ChatOpenAI\\n\\nfrom pydantic import BaseModel, Field\\n\\n\\nfrom langgraph.prebuilt import tools_condition\\n\\n### Edges\\n\\n\\ndef grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\\n    \"\"\"\\n    Determines whether the retrieved documents are relevant to the question.\\n\\n    Args:\\n        state (messages): The current state\\n\\n    Returns:\\n        str: A decision for whether the documents are relevant or not\\n    \"\"\"\\n\\n    print(\"---CHECK RELEVANCE---\")\\n\\n    # Data model\\n    class grade(BaseModel):\\n        \"\"\"Binary score for relevance check.\"\"\"\\n\\n        binary_score: str = Field(description=\"Relevance score \\'yes\\' or \\'no\\'\")\\n\\n    # LLM\\n    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\\n\\n    # LLM with tool and validation\\n    llm_with_tool = model.with_structured_output(grade)\\n\\n    # Prompt\\n    prompt = PromptTemplate(\\n        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\\\n \\n        Here is the retrieved document: \\\\n\\\\n {context} \\\\n\\\\n\\n        Here is the user question: {question} \\\\n\\n        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\\\n\\n        Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the document is relevant to the question.\"\"\",\\n        input_variables=[\"context\", \"question\"],\\n    )\\n\\n    # Chain\\n    chain = prompt | llm_with_tool\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    question = messages[0].content\\n    docs = last_message.content\\n\\n    scored_result = chain.invoke({\"question\": question, \"context\": docs})\\n\\n    score = scored_result.binary_score\\n\\n    if score == \"yes\":\\n        print(\"---DECISION: DOCS RELEVANT---\")\\n        return \"generate\"\\n\\n    else:\\n        print(\"---DECISION: DOCS NOT RELEVANT---\")\\n        print(score)\\n        return \"rewrite\"\\n\\n\\n### Nodes\\n\\n\\ndef agent(state):\\n    \"\"\"\\n    Invokes the agent model to generate a response based on the current state. Given\\n    the question, it will decide to retrieve using the retriever tool, or simply end.\\n\\n    Args:\\n        state (messages): The current state\\n\\n    Returns:\\n        dict: The updated state with the agent response appended to messages\\n    \"\"\"\\n    print(\"---CALL AGENT---\")\\n    messages = state[\"messages\"]\\n    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\")\\n    model = model.bind_tools(tools)\\n    response = model.invoke(messages)\\n    # We return a list, because this will get added to the existing list\\n    return {\"messages\": [response]}\\n\\n\\ndef rewrite(state):\\n    \"\"\"\\n    Transform the query to produce a better question.\\n\\n    Args:\\n        state (messages): The current state\\n\\n    Returns:\\n        dict: The updated state with re-phrased question\\n    \"\"\"\\n\\n    print(\"---TRANSFORM QUERY---\")\\n    messages = state[\"messages\"]\\n    question = messages[0].content\\n\\n    msg = [\\n        HumanMessage(\\n            content=f\"\"\" \\\\n \\n    Look at the input and try to reason about the underlying semantic intent / meaning. \\\\n \\n    Here is the initial question:\\n    \\\\n ------- \\\\n\\n    {question} \\n    \\\\n ------- \\\\n\\n    Formulate an improved question: \"\"\",\\n        )\\n    ]\\n\\n    # Grader\\n    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\\n    response = model.invoke(msg)\\n    return {\"messages\": [response]}\\n\\n\\ndef generate(state):\\n    \"\"\"\\n    Generate answer\\n\\n    Args:\\n        state (messages): The current state\\n\\n    Returns:\\n         dict: The updated state with re-phrased question\\n    \"\"\"\\n    print(\"---GENERATE---\")\\n    messages = state[\"messages\"]\\n    question = messages[0].content\\n    last_message = messages[-1]\\n\\n    docs = last_message.content\\n\\n    # Prompt\\n    prompt = hub.pull(\"rlm/rag-prompt\")\\n\\n    # LLM\\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\\n\\n    # Post-processing\\n    def format_docs(docs):\\n        return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)\\n\\n    # Chain\\n    rag_chain = prompt | llm | StrOutputParser()\\n\\n    # Run\\n    response = rag_chain.invoke({\"context\": docs, \"question\": question})\\n    return {\"messages\": [response]}\\n\\n\\nprint(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\\nprompt = hub.pull(\"rlm/rag-prompt\").pretty_print()  # Show what the prompt looks like\\n\\nThis code snippet defines a prompt template for a question-answering assistant. It instructs the assistant to utilize provided context to answer a given question succinctly, limiting the response to a maximum of three sentences. If the assistant cannot provide an answer based on the context, it is instructed to respond with \"I don\\'t know.\"\\nS3 Location: ********************Prompt[rlm/rag-prompt]********************\\n================================\\x1b[1m Human Message \\x1b[0m=================================\\n\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\nQuestion: \\x1b[33;1m\\x1b[1;3m{question}\\x1b[0m \\nContext: \\x1b[33;1m\\x1b[1;3m{context}\\x1b[0m \\nAnswer:\\nAPI Reference:\\nBaseMessage\\n|\\nHumanMessage\\n|\\nStrOutputParser\\n|\\nPromptTemplate\\n|\\nChatOpenAI\\n|\\ntools_condition\\nGraph\\n¶\\nStart with an agent,\\ncall_model\\nAgent make a decision to call a function\\nIf so, then\\naction\\nto call tool (retriever)\\nThen call agent with the tool output added to messages (\\nstate\\n)\\n\\nThe code defines a state graph workflow for a system that involves an agent, retrieval, rewriting, and response generation. It begins by creating a `StateGraph` object and adding several nodes representing different actions: an agent node, a retrieval tool node, a rewriting node, and a response generation node. Conditional edges are established to determine the flow based on the agent\\'s decisions, including whether to retrieve information or not. Finally, the graph is compiled into a usable state graph structure for execution.\\nS3 Location: from langgraph.graph import END, StateGraph, START\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Define a new graph\\nworkflow = StateGraph(AgentState)\\n\\n# Define the nodes we will cycle between\\nworkflow.add_node(\"agent\", agent)  # agent\\nretrieve = ToolNode([retriever_tool])\\nworkflow.add_node(\"retrieve\", retrieve)  # retrieval\\nworkflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\\nworkflow.add_node(\\n    \"generate\", generate\\n)  # Generating a response after we know the documents are relevant\\n# Call agent node to decide to retrieve or not\\nworkflow.add_edge(START, \"agent\")\\n\\n# Decide whether to retrieve\\nworkflow.add_conditional_edges(\\n    \"agent\",\\n    # Assess agent decision\\n    tools_condition,\\n    {\\n        # Translate the condition outputs to nodes in our graph\\n        \"tools\": \"retrieve\",\\n        END: END,\\n    },\\n)\\n\\n# Edges taken after the `action` node is called.\\nworkflow.add_conditional_edges(\\n    \"retrieve\",\\n    # Assess agent decision\\n    grade_documents,\\n)\\nworkflow.add_edge(\"generate\", END)\\nworkflow.add_edge(\"rewrite\", \"agent\")\\n\\n# Compile\\ngraph = workflow.compile()\\nAPI Reference:\\nEND\\n|\\nStateGraph\\n|\\nSTART\\n|\\nToolNode\\n\\nThis code attempts to display an image generated from a graph using the IPython display module. It calls a method `get_graph()` with the argument `xray=True` from the `graph` object, which likely returns a graph representation, and then calls `draw_mermaid_png()` to produce a PNG image of that graph. If there is an exception during this process (possibly due to missing dependencies), it catches the exception and simply passes without any further action or error.\\nS3 Location: from IPython.display import Image, display\\n\\ntry:\\n    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\\nexcept Exception:\\n    # This requires some extra dependencies and is optional\\n    pass\\n\\nThe code imports the `pprint` module for pretty-printing data structures. It initializes a dictionary called `inputs` containing a list of messages, specifically one from the user asking about Lilian Weng\\'s views on agent memory. It then iterates over outputs generated from a `graph.stream(inputs)` function, which presumably processes the input messages. For each output, it prints the node key and its corresponding value in a formatted manner, separating each output with dashes for clarity.\\nS3 Location: import pprint\\n\\ninputs = {\\n    \"messages\": [\\n        (\"user\", \"What does Lilian Weng say about the types of agent memory?\"),\\n    ]\\n}\\nfor output in graph.stream(inputs):\\n    for key, value in output.items():\\n        pprint.pprint(f\"Output from node \\'{key}\\':\")\\n        pprint.pprint(\"---\")\\n        pprint.pprint(value, indent=2, width=80, depth=None)\\n    pprint.pprint(\"\\\\n---\\\\n\")\\n'),\n",
       " Document(id='09628c95-1c16-4853-af63-7e071c771d38', metadata={}, page_content='Adaptive RAG\\nSkip to content\\nAdaptive RAG\\nInitializing search\\nGitHub\\nHome\\nTutorials\\nHow-to Guides\\nConceptual Guides\\nReference\\nGitHub\\nHome\\nTutorials\\nTutorials\\nQuick Start\\nQuick Start\\nQuick Start\\nðŸš€ LangGraph Quick Start\\nQuick Start: Launch Local LangGraph Server\\nLangGraph Cloud Quick Start\\nChatbots\\nChatbots\\nChatbots\\nBuild a Customer Support Bot\\nPrompt Generation from User Requirements\\nCode generation with RAG and self-correction\\nRAG\\nRAG\\nRAG\\nAdaptive RAG\\nAdaptive RAG\\nTable of contents\\nSetup\\nCreate Index\\nLLMs\\nWeb Search Tool\\nConstruct the Graph\\nDefine Graph State\\nDefine Graph Flow\\nCompile Graph\\nUse Graph\\nLanggraph adaptive rag local\\nAgentic RAG\\nCorrective RAG (CRAG)\\nCorrective RAG (CRAG) using local LLMs\\nSelf-RAG\\nSelf-RAG using local LLMs\\nAn agent for interacting with a SQL database\\nAgent Architectures\\nAgent Architectures\\nAgent Architectures\\nMulti-Agent Systems\\nPlanning Agents\\nReflection & Critique\\nEvaluation & Analysis\\nEvaluation & Analysis\\nEvaluation & Analysis\\nChat Bot Evaluation as Multi-agent Simulation\\nChat Bot Benchmarking using Simulation\\nExperimental\\nExperimental\\nExperimental\\nWeb Research (STORM)\\nTNT-LLM: Text Mining at Scale\\nWeb Voyager\\nCompetitive Programming\\nComplex data extraction with function calling\\nHow-to Guides\\nConceptual Guides\\nReference\\nTable of contents\\nSetup\\nCreate Index\\nLLMs\\nWeb Search Tool\\nConstruct the Graph\\nDefine Graph State\\nDefine Graph Flow\\nCompile Graph\\nUse Graph\\nHome\\nTutorials\\nRAG\\nAdaptive RAG\\n¶\\nAdaptive RAG is a strategy for RAG that unites (1)\\nquery analysis\\nwith (2)\\nactive / self-corrective RAG\\n.\\nIn the\\npaper\\n, they report query analysis to route across:\\nNo Retrieval\\nSingle-shot RAG\\nIterative RAG\\nLet\\'s build on this using LangGraph.\\nIn our implementation, we will route between:\\nWeb search: for questions related to recent events\\nSelf-corrective RAG: for questions related to our index\\nSetup\\n¶\\nFirst, let\\'s install our required packages and set our API keys\\n%%\\ncapture\\n--\\nno\\n-\\nstderr\\n!\\npip\\ninstall\\n-\\nU\\nlangchain_community\\ntiktoken\\nlangchain\\n-\\nopenai\\nlangchain\\n-\\ncohere\\nlangchainhub\\nchromadb\\nlangchain\\nlanggraph\\ntavily\\n-\\npython\\n\\nThis code defines a function `_set_env` that takes a string variable name as an argument. It checks if the environment variable with that name is already set; if not, it prompts the user to input a value securely using `getpass.getpass()`. The inputted value is then stored as an environment variable. The function is called three times to set the environment variables for \"OPENAI_API_KEY\", \"COHERE_API_KEY\", and \"TAVILY_API_KEY\" if they are not already defined.\\nS3 Location: import getpass\\nimport os\\n\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"OPENAI_API_KEY\")\\n_set_env(\"COHERE_API_KEY\")\\n_set_env(\"TAVILY_API_KEY\")\\nSet up\\nLangSmith\\nfor LangGraph development\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started\\nhere\\n.\\nCreate Index\\n¶\\n\\nThe provided code snippet builds an index for web documents using the Langchain library. It first retrieves content from specified URLs using the `WebBaseLoader`, then flattens the loaded documents into a single list. The text is split into manageable chunks of 500 characters with no overlap using the `RecursiveCharacterTextSplitter`. Finally, these document chunks are added to a vector store (Chroma) with OpenAI embeddings to facilitate efficient retrieval of information, and a retriever is created from this vector store for querying purposes.\\nS3 Location: ### Build Index\\n\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_openai import OpenAIEmbeddings\\n\\n### from langchain_cohere import CohereEmbeddings\\n\\n# Set embeddings\\nembd = OpenAIEmbeddings()\\n\\n# Docs to index\\nurls = [\\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\\n]\\n\\n# Load\\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n\\n# Split\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=500, chunk_overlap=0\\n)\\ndoc_splits = text_splitter.split_documents(docs_list)\\n\\n# Add to vectorstore\\nvectorstore = Chroma.from_documents(\\n    documents=doc_splits,\\n    collection_name=\"rag-chroma\",\\n    embedding=embd,\\n)\\nretriever = vectorstore.as_retriever()\\nAPI Reference:\\nRecursiveCharacterTextSplitter\\n|\\nWebBaseLoader\\n|\\nChroma\\n|\\nOpenAIEmbeddings\\nLLMs\\n¶\\nUsing Pydantic with LangChain\\nThis notebook uses Pydantic v2\\nBaseModel\\n, which requires\\nlangchain-core >= 0.3\\n. Using\\nlangchain-core < 0.3\\nwill result in errors due to mixing of Pydantic v1 and v2\\nBaseModels\\n.\\n\\nThe provided code defines a routing mechanism for user queries, directing them to either a vector store or a web search based on the content of the question. It uses the Langchain framework, specifically the `ChatOpenAI` model, to process queries and determine the appropriate data source. The `RouteQuery` data model specifies the routing options, while a prompt instructs the model on how to categorize questions. Finally, the code invokes the routing mechanism with sample questions, demonstrating its functionality by determining which source to use for different topics.\\nS3 Location: ### Router\\n\\nfrom typing import Literal\\n\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\n\\nfrom pydantic import BaseModel, Field\\n\\n\\n# Data model\\nclass RouteQuery(BaseModel):\\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\\n\\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\\n        ...,\\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\\n    )\\n\\n\\n# LLM with function call\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\nstructured_llm_router = llm.with_structured_output(RouteQuery)\\n\\n# Prompt\\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\\nroute_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\"human\", \"{question}\"),\\n    ]\\n)\\n\\nquestion_router = route_prompt | structured_llm_router\\nprint(\\n    question_router.invoke(\\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\\n    )\\n)\\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\\ndatasource=\\'web_search\\'\\ndatasource=\\'vectorstore\\'\\nAPI Reference:\\nChatPromptTemplate\\n|\\nChatOpenAI\\n\\nThe code defines a system for grading the relevance of retrieved documents in relation to a user question. It establishes a data model, `GradeDocuments`, which holds a binary score (\"yes\" or \"no\") indicating the document\\'s relevance. A language model (LLM) is configured to process this grading task using a structured output. A prompt is created to guide the LLM in assessing the document based on its content compared to the user question. Finally, the code retrieves a document and invokes the grading process to determine and print the relevance score.\\nS3 Location: ### Retrieval Grader\\n\\n\\n# Data model\\nclass GradeDocuments(BaseModel):\\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\\n\\n    binary_score: str = Field(\\n        description=\"Documents are relevant to the question, \\'yes\\' or \\'no\\'\"\\n    )\\n\\n\\n# LLM with function call\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\\n\\n# Prompt\\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\\\n \\n    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\\\n\\n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\\\n\\n    Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the document is relevant to the question.\"\"\"\\ngrade_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\"human\", \"Retrieved document: \\\\n\\\\n {document} \\\\n\\\\n User question: {question}\"),\\n    ]\\n)\\n\\nretrieval_grader = grade_prompt | structured_llm_grader\\nquestion = \"agent memory\"\\ndocs = retriever.invoke(question)\\ndoc_txt = docs[1].page_content\\nprint(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\\nbinary_score=\\'no\\'\\n\\nThis code snippet utilizes the Langchain library to implement a retrieval-augmented generation (RAG) system. It starts by pulling a prompt template from the Langchain hub and initializes a language model (ChatOpenAI) with specific parameters. The `format_docs` function is defined to format the retrieved documents into a string format suitable for output. A processing chain is created by chaining the prompt, language model, and a string output parser. Finally, the chain is executed with a context (documents) and a question, producing and printing the generated response.\\nS3 Location: ### Generate\\n\\nfrom langchain import hub\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n# Prompt\\nprompt = hub.pull(\"rlm/rag-prompt\")\\n\\n# LLM\\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\\n\\n\\n# Post-processing\\ndef format_docs(docs):\\n    return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)\\n\\n\\n# Chain\\nrag_chain = prompt | llm | StrOutputParser()\\n\\n# Run\\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\\nprint(generation)\\nThe design of generative agents combines LLM with memory, planning, and reflection mechanisms to enable agents to behave based on past experience and interact with other agents. Memory stream is a long-term memory module that records agents\\' experiences in natural language. The retrieval model surfaces context to inform the agent\\'s behavior based on relevance, recency, and importance.\\nAPI Reference:\\nStrOutputParser\\n\\nThe provided code defines a system for evaluating whether a language model (LLM) generation is grounded in a specified set of facts. It creates a data model (`GradeHallucinations`) to store a binary score (\"yes\" or \"no\") indicating the presence of hallucinations in the generation. The LLM is set up to produce structured output according to this model, and a prompt is constructed to instruct the LLM to assess the grounding of its response based on the provided facts. Finally, the `hallucination_grader` is invoked with specific documents and a generation to obtain the binary score indicating if the generation is factually supported.\\nS3 Location: ### Hallucination Grader\\n\\n\\n# Data model\\nclass GradeHallucinations(BaseModel):\\n    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\\n\\n    binary_score: str = Field(\\n        description=\"Answer is grounded in the facts, \\'yes\\' or \\'no\\'\"\\n    )\\n\\n\\n# LLM with function call\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\nstructured_llm_grader = llm.with_structured_output(GradeHallucinations)\\n\\n# Prompt\\nsystem = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\\\n \\n     Give a binary score \\'yes\\' or \\'no\\'. \\'Yes\\' means that the answer is grounded in / supported by the set of facts.\"\"\"\\nhallucination_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\"human\", \"Set of facts: \\\\n\\\\n {documents} \\\\n\\\\n LLM generation: {generation}\"),\\n    ]\\n)\\n\\nhallucination_grader = hallucination_prompt | structured_llm_grader\\nhallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\\nGradeHallucinations(binary_score=\\'yes\\')\\n\\nThe code defines a grading system that evaluates whether a given answer addresses a specific question with a binary score of \"yes\" or \"no\". It creates a data model `GradeAnswer` to represent the binary score, and utilizes a language model (LLM) to assess the answer\\'s relevance to the question. A prompt is prepared to instruct the LLM on its grading task, and the grading process is executed using the `answer_grader`, which combines the prompt with the LLM functionality. Finally, the grading function is invoked with the specific question and the answer to be evaluated.\\nS3 Location: ### Answer Grader\\n\\n\\n# Data model\\nclass GradeAnswer(BaseModel):\\n    \"\"\"Binary score to assess answer addresses question.\"\"\"\\n\\n    binary_score: str = Field(\\n        description=\"Answer addresses the question, \\'yes\\' or \\'no\\'\"\\n    )\\n\\n\\n# LLM with function call\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\nstructured_llm_grader = llm.with_structured_output(GradeAnswer)\\n\\n# Prompt\\nsystem = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\\\n \\n     Give a binary score \\'yes\\' or \\'no\\'. Yes\\' means that the answer resolves the question.\"\"\"\\nanswer_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\"human\", \"User question: \\\\n\\\\n {question} \\\\n\\\\n LLM generation: {generation}\"),\\n    ]\\n)\\n\\nanswer_grader = answer_prompt | structured_llm_grader\\nanswer_grader.invoke({\"question\": question, \"generation\": generation})\\nGradeAnswer(binary_score=\\'yes\\')\\n\\nThe code defines a question re-writer using the GPT-3.5 Turbo model from OpenAI. It sets up a prompt that instructs the model to improve an input question to optimize it for better retrieval in a vector store. The prompt consists of a system message that explains the task and a human message that specifies the input question format. Finally, the `question_rewriter` object combines the prompt with the language model and a string output parser, and it invokes this process by passing an initial question to be reformulated.\\nS3 Location: ### Question Re-writer\\n\\n# LLM\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\n\\n# Prompt\\nsystem = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\\\n \\n     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\\nre_write_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\\n            \"human\",\\n            \"Here is the initial question: \\\\n\\\\n {question} \\\\n Formulate an improved question.\",\\n        ),\\n    ]\\n)\\n\\nquestion_rewriter = re_write_prompt | llm | StrOutputParser()\\nquestion_rewriter.invoke({\"question\": question})\\n\"What is the role of memory in an agent\\'s functioning?\"\\nWeb Search Tool\\n¶\\n\\nThe code imports the `TavilySearchResults` class from the `langchain_community.tools.tavily_search` module. It then creates an instance of `TavilySearchResults` called `web_search_tool`, initializing it with a parameter `k=3`, which likely specifies that the tool should return the top 3 search results when performing a web search.\\nS3 Location: ### Search\\n\\nfrom langchain_community.tools.tavily_search import TavilySearchResults\\n\\nweb_search_tool = TavilySearchResults(k=3)\\nAPI Reference:\\nTavilySearchResults\\nConstruct the Graph'),\n",
       " Document(id='4d394b5e-bdc3-45b9-abaf-bb83c6edeb67', metadata={}, page_content='S3 Location: ### Search\\n\\nfrom langchain_community.tools.tavily_search import TavilySearchResults\\n\\nweb_search_tool = TavilySearchResults(k=3)\\nAPI Reference:\\nTavilySearchResults\\nConstruct the Graph\\n¶\\nCapture the flow in as a graph.\\nDefine Graph State\\n¶\\n\\nThe provided code defines a `TypedDict` named `GraphState`, which is a structured way to represent a dictionary with specific key-value types in Python. It includes three attributes: `question`, which is a string representing a question; `generation`, a string representing the output of a language model (LLM) generation; and `documents`, a list of strings that contains related documents. This structure helps enforce type checking and improves code clarity when handling graph state data.\\nS3 Location: from typing import List\\n\\nfrom typing_extensions import TypedDict\\n\\n\\nclass GraphState(TypedDict):\\n    \"\"\"\\n    Represents the state of our graph.\\n\\n    Attributes:\\n        question: question\\n        generation: LLM generation\\n        documents: list of documents\\n    \"\"\"\\n\\n    question: str\\n    generation: str\\n    documents: List[str]\\nDefine Graph Flow\\n¶\\n\\nThe provided code defines a sequence of functions that facilitate a document retrieval and answer generation workflow. The `retrieve` function fetches documents related to a user\\'s question, while `generate` produces an answer based on those documents. The `grade_documents` function filters out irrelevant documents, and `transform_query` refines the original question for better results. The `web_search` function allows for online searching if needed. Routing and decision-making functions determine the next steps based on the relevance of documents and the quality of generated answers, ensuring the system effectively addresses user inquiries.\\nS3 Location: from langchain.schema import Document\\n\\n\\ndef retrieve(state):\\n    \"\"\"\\n    Retrieve documents\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, documents, that contains retrieved documents\\n    \"\"\"\\n    print(\"---RETRIEVE---\")\\n    question = state[\"question\"]\\n\\n    # Retrieval\\n    documents = retriever.invoke(question)\\n    return {\"documents\": documents, \"question\": question}\\n\\n\\ndef generate(state):\\n    \"\"\"\\n    Generate answer\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, generation, that contains LLM generation\\n    \"\"\"\\n    print(\"---GENERATE---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # RAG generation\\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\\n\\n\\ndef grade_documents(state):\\n    \"\"\"\\n    Determines whether the retrieved documents are relevant to the question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with only filtered relevant documents\\n    \"\"\"\\n\\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Score each doc\\n    filtered_docs = []\\n    for d in documents:\\n        score = retrieval_grader.invoke(\\n            {\"question\": question, \"document\": d.page_content}\\n        )\\n        grade = score.binary_score\\n        if grade == \"yes\":\\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\\n            filtered_docs.append(d)\\n        else:\\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\\n            continue\\n    return {\"documents\": filtered_docs, \"question\": question}\\n\\n\\ndef transform_query(state):\\n    \"\"\"\\n    Transform the query to produce a better question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates question key with a re-phrased question\\n    \"\"\"\\n\\n    print(\"---TRANSFORM QUERY---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Re-write question\\n    better_question = question_rewriter.invoke({\"question\": question})\\n    return {\"documents\": documents, \"question\": better_question}\\n\\n\\ndef web_search(state):\\n    \"\"\"\\n    Web search based on the re-phrased question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with appended web results\\n    \"\"\"\\n\\n    print(\"---WEB SEARCH---\")\\n    question = state[\"question\"]\\n\\n    # Web search\\n    docs = web_search_tool.invoke({\"query\": question})\\n    web_results = \"\\\\n\".join([d[\"content\"] for d in docs])\\n    web_results = Document(page_content=web_results)\\n\\n    return {\"documents\": web_results, \"question\": question}\\n\\n\\n### Edges ###\\n\\n\\ndef route_question(state):\\n    \"\"\"\\n    Route question to web search or RAG.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Next node to call\\n    \"\"\"\\n\\n    print(\"---ROUTE QUESTION---\")\\n    question = state[\"question\"]\\n    source = question_router.invoke({\"question\": question})\\n    if source.datasource == \"web_search\":\\n        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\\n        return \"web_search\"\\n    elif source.datasource == \"vectorstore\":\\n        print(\"---ROUTE QUESTION TO RAG---\")\\n        return \"vectorstore\"\\n\\n\\ndef decide_to_generate(state):\\n    \"\"\"\\n    Determines whether to generate an answer, or re-generate a question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Binary decision for next node to call\\n    \"\"\"\\n\\n    print(\"---ASSESS GRADED DOCUMENTS---\")\\n    state[\"question\"]\\n    filtered_documents = state[\"documents\"]\\n\\n    if not filtered_documents:\\n        # All documents have been filtered check_relevance\\n        # We will re-generate a new query\\n        print(\\n            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\\n        )\\n        return \"transform_query\"\\n    else:\\n        # We have relevant documents, so generate answer\\n        print(\"---DECISION: GENERATE---\")\\n        return \"generate\"\\n\\n\\ndef grade_generation_v_documents_and_question(state):\\n    \"\"\"\\n    Determines whether the generation is grounded in the document and answers question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Decision for next node to call\\n    \"\"\"\\n\\n    print(\"---CHECK HALLUCINATIONS---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n    generation = state[\"generation\"]\\n\\n    score = hallucination_grader.invoke(\\n        {\"documents\": documents, \"generation\": generation}\\n    )\\n    grade = score.binary_score\\n\\n    # Check hallucination\\n    if grade == \"yes\":\\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\\n        # Check question-answering\\n        print(\"---GRADE GENERATION vs QUESTION---\")\\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\\n        grade = score.binary_score\\n        if grade == \"yes\":\\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\\n            return \"useful\"\\n        else:\\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\\n            return \"not useful\"\\n    else:\\n        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\\n        return \"not supported\"\\nAPI Reference:\\nDocument\\nCompile Graph\\n¶\\n\\nThis code snippet defines a state machine workflow using the `StateGraph` class from the `langgraph` library. It creates a series of nodes representing different steps in a process (such as web searching, retrieving documents, grading them, and generating results). The code establishes directed edges between these nodes to represent the flow of the process, including conditional transitions based on specific criteria. Finally, it compiles the workflow into an executable application.\\nS3 Location: from langgraph.graph import END, StateGraph, START\\n\\nworkflow = StateGraph(GraphState)\\n\\n# Define the nodes\\nworkflow.add_node(\"web_search\", web_search)  # web search\\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\\nworkflow.add_node(\"generate\", generate)  # generatae\\nworkflow.add_node(\"transform_query\", transform_query)  # transform_query\\n\\n# Build graph\\nworkflow.add_conditional_edges(\\n    START,\\n    route_question,\\n    {\\n        \"web_search\": \"web_search\",\\n        \"vectorstore\": \"retrieve\",\\n    },\\n)\\nworkflow.add_edge(\"web_search\", \"generate\")\\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\\nworkflow.add_conditional_edges(\\n    \"grade_documents\",\\n    decide_to_generate,\\n    {\\n        \"transform_query\": \"transform_query\",\\n        \"generate\": \"generate\",\\n    },\\n)\\nworkflow.add_edge(\"transform_query\", \"retrieve\")\\nworkflow.add_conditional_edges(\\n    \"generate\",\\n    grade_generation_v_documents_and_question,\\n    {\\n        \"not supported\": \"generate\",\\n        \"useful\": END,\\n        \"not useful\": \"transform_query\",\\n    },\\n)\\n\\n# Compile\\napp = workflow.compile()\\nAPI Reference:\\nEND\\n|\\nStateGraph\\n|\\nSTART\\nUse Graph\\n¶\\n\\nThis code is likely part of an application that processes a user query and generates a response through a series of nodes. It sends a question about an NFL draft player to the `app.stream()` method, which yields outputs iteratively. For each output, it prints the node\\'s key and value using the `pprint` function for better readability. After processing all nodes, it prints the final generation value associated with the last output. This setup is typically used for debugging or tracking the flow of data through a multi-step processing pipeline.\\nS3 Location: from pprint import pprint\\n\\n# Run\\ninputs = {\\n    \"question\": \"What player at the Bears expected to draft first in the 2024 NFL draft?\"\\n}\\nfor output in app.stream(inputs):\\n    for key, value in output.items():\\n        # Node\\n        pprint(f\"Node \\'{key}\\':\")\\n        # Optional: print full state at each node\\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\\n    pprint(\"\\\\n---\\\\n\")\\n\\n# Final generation\\npprint(value[\"generation\"])\\n\\nThe code snippet appears to be part of a decision-making process for generating a response to a user query about the Chicago Bears and the NFL draft. It routes the question to a web search, checks the relevance of the generated response against documents, and confirms that the generation is grounded in reliable information. Ultimately, the code produces a response that discusses the Bears\\' potential to draft first in the 2024 NFL draft, mentioning both a defensive player and a wide receiver as notable prospects.\\nS3 Location: ---ROUTE QUESTION---\\n---ROUTE QUESTION TO WEB SEARCH---\\n---WEB SEARCH---\\n\"Node \\'web_search\\':\"\\n\\'\\\\n---\\\\n\\'\\n---GENERATE---\\n---CHECK HALLUCINATIONS---\\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\n---GRADE GENERATION vs QUESTION---\\n---DECISION: GENERATION ADDRESSES QUESTION---\\n\"Node \\'generate\\':\"\\n\\'\\\\n---\\\\n\\'\\n(\\'It is expected that the Chicago Bears could have the opportunity to draft \\'\\n \\'the first defensive player in the 2024 NFL draft. The Bears have the first \\'\\n \\'overall pick in the draft, giving them a prime position to select top \\'\\n \\'talent. The top wide receiver Marvin Harrison Jr. from Ohio State is also \\'\\n \\'mentioned as a potential pick for the Cardinals.\\')\\nTrace:\\nhttps://smith.langchain.com/public/7e3aa7e5-c51f-45c2-bc66-b34f17ff2263/r\\n\\nThis code snippet initiates a process to query an application (`app`) with a specific input question about types of agent memory. It iterates over the streaming output from the application, where each output is a dictionary containing nodes, printing the node keys for each output. Additionally, there is a commented-out option to print detailed information about the node\\'s state. After processing all outputs, it prints the final generation result from the last output received.\\nS3 Location: # Run\\ninputs = {\"question\": \"What are the types of agent memory?\"}\\nfor output in app.stream(inputs):\\n    for key, value in output.items():\\n        # Node\\n        pprint(f\"Node \\'{key}\\':\")\\n        # Optional: print full state at each node\\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\\n    pprint(\"\\\\n---\\\\n\")\\n\\n# Final generation\\npprint(value[\"generation\"])\\n\\nThe code snippet describes a process for handling a question and generating a relevant response using a document retrieval and grading system. It begins by routing the question to a retrieval system, which assesses the relevance of documents against the question. Based on the grades assigned to these documents, it decides whether to generate a response. The generation phase checks for accuracy and relevance, ultimately producing a detailed answer regarding types of agent memory, including Sensory Memory, Short-Term Memory, and Long-Term Memory.\\nS3 Location: ---ROUTE QUESTION---\\n---ROUTE QUESTION TO RAG---\\n---RETRIEVE---\\n\"Node \\'retrieve\\':\"\\n\\'\\\\n---\\\\n\\'\\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT NOT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---ASSESS GRADED DOCUMENTS---\\n---DECISION: GENERATE---\\n\"Node \\'grade_documents\\':\"\\n\\'\\\\n---\\\\n\\'\\n---GENERATE---\\n---CHECK HALLUCINATIONS---\\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\n---GRADE GENERATION vs QUESTION---\\n---DECISION: GENERATION ADDRESSES QUESTION---\\n\"Node \\'generate\\':\"\\n\\'\\\\n---\\\\n\\'\\n(\\'The types of agent memory include Sensory Memory, Short-Term Memory (STM) or \\'\\n \\'Working Memory, and Long-Term Memory (LTM) with subtypes of Explicit / \\'\\n \\'declarative memory and Implicit / procedural memory. Sensory memory retains \\'\\n \\'sensory information briefly, STM stores information for cognitive tasks, and \\'\\n \\'LTM stores information for a long time with different types of memories.\\')\\nTrace:\\nhttps://smith.langchain.com/public/fdf0a180-6d15-4d09-bb92-f84f2105ca51/r\\nComments\\nBack to top\\nPrevious\\nCode generation with RAG and self-correction\\nNext\\nLanggraph adaptive rag local\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='e1f40e30-9b9d-4384-a94c-579d3a38ed5c', metadata={}, page_content='API Reference:\\nDocument\\nCompile Graph\\n¶\\nThe just follows the flow we outlined in the figure above.\\n\\nThis code snippet creates a state graph workflow using the `StateGraph` class from the `langgraph` library. It defines several nodes representing different tasks (e.g., retrieving, grading documents, generating outputs) and establishes directed edges between these nodes to represent the workflow\\'s flow. Conditional edges are added based on a decision function (`decide_to_generate`), allowing the workflow to branch into two potential paths. Finally, the graph is compiled into an executable application object (`app`).\\nS3 Location: from langgraph.graph import END, StateGraph, START\\n\\nworkflow = StateGraph(GraphState)\\n\\n# Define the nodes\\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\\nworkflow.add_node(\"generate\", generate)  # generatae\\nworkflow.add_node(\"transform_query\", transform_query)  # transform_query\\nworkflow.add_node(\"web_search_node\", web_search)  # web search\\n\\n# Build graph\\nworkflow.add_edge(START, \"retrieve\")\\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\\nworkflow.add_conditional_edges(\\n    \"grade_documents\",\\n    decide_to_generate,\\n    {\\n        \"transform_query\": \"transform_query\",\\n        \"generate\": \"generate\",\\n    },\\n)\\nworkflow.add_edge(\"transform_query\", \"web_search_node\")\\nworkflow.add_edge(\"web_search_node\", \"generate\")\\nworkflow.add_edge(\"generate\", END)\\n\\n# Compile\\napp = workflow.compile()\\nAPI Reference:\\nEND\\n|\\nStateGraph\\n|\\nSTART\\nUse the graph\\n¶\\n\\nThis code snippet appears to interact with an application (likely a language model or similar system) to generate responses based on a given input question. It initializes a dictionary with a question about agent memory types and iterates through the streaming outputs from the `app.stream(inputs)` method. For each output received, it prints the key of each node along with an optional commented-out line that could print additional details about the node\\'s state. Finally, it prints the \"generation\" attribute of the last output received, which likely contains the final response or result generated by the application.\\nS3 Location: from pprint import pprint\\n\\n# Run\\ninputs = {\"question\": \"What are the types of agent memory?\"}\\nfor output in app.stream(inputs):\\n    for key, value in output.items():\\n        # Node\\n        pprint(f\"Node \\'{key}\\':\")\\n        # Optional: print full state at each node\\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\\n    pprint(\"\\\\n---\\\\n\")\\n\\n# Final generation\\npprint(value[\"generation\"])\\n\\nThe provided code snippet outlines a structured process for retrieving and assessing documents in response to a question. It includes nodes for retrieving documents, grading their relevance, transforming the query if necessary, conducting a web search, and generating responses. The process indicates that multiple documents were evaluated for relevance, leading to a transformation of the query when all documents were deemed not relevant. The final node suggests that the agents have a memory system for in-context and long-term learning, which supports their ability to retain information.\\nS3 Location: ---RETRIEVE---\\n\"Node \\'retrieve\\':\"\\n\\'\\\\n---\\\\n\\'\\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\\n---GRADE: DOCUMENT NOT RELEVANT---\\n---GRADE: DOCUMENT NOT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n\"Node \\'grade_documents\\':\"\\n\\'\\\\n---\\\\n\\'\\n---ASSESS GRADED DOCUMENTS---\\n---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\\n---TRANSFORM QUERY---\\n\"Node \\'transform_query\\':\"\\n\\'\\\\n---\\\\n\\'\\n---WEB SEARCH---\\n\"Node \\'web_search_node\\':\"\\n\\'\\\\n---\\\\n\\'\\n---GENERATE---\\n\"Node \\'generate\\':\"\\n\\'\\\\n---\\\\n\\'\\n\"Node \\'__end__\\':\"\\n\\'\\\\n---\\\\n\\'\\n(\\'Agents possess short-term memory, which is utilized for in-context learning, \\'\\n \\'and long-term memory, allowing them to retain and recall vast amounts of \\'\\n \\'information over extended periods. Some experts also classify working memory \\'\\n \\'as a distinct type, although it can be considered a part of short-term \\'\\n \\'memory in many cases.\\')\\n\\nThis code imports the `pprint` function for pretty-printing output and initializes a dictionary called `inputs` with a question about the AlphaCodium paper. It then iterates over the outputs generated by a method `app.stream(inputs)`, where it prints the key of each output node in a formatted manner. After processing all nodes, it prints a separator line. Finally, it prints the \"generation\" value from the last processed output node, which likely contains a final result or conclusion related to the initial question.\\nS3 Location: from pprint import pprint\\n\\n# Run\\ninputs = {\"question\": \"How does the AlphaCodium paper work?\"}\\nfor output in app.stream(inputs):\\n    for key, value in output.items():\\n        # Node\\n        pprint(f\"Node \\'{key}\\':\")\\n        # Optional: print full state at each node\\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\\n    pprint(\"\\\\n---\\\\n\")\\n\\n# Final generation\\npprint(value[\"generation\"])\\n\\nThe provided code snippet outlines a workflow for processing a query related to document relevance. It includes nodes for retrieving documents, grading their relevance to a specific question, transforming the query if needed, and performing a web search. The grading phase indicates that most documents are not relevant, leading to a transformation of the query. The final output describes the AlphaCodium paper, which details a method for iteratively generating and refining code through testing, aimed at enhancing the effectiveness of Large Language Models on coding tasks.\\nS3 Location: ---RETRIEVE---\\n\"Node \\'retrieve\\':\"\\n\\'\\\\n---\\\\n\\'\\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\\n---GRADE: DOCUMENT NOT RELEVANT---\\n---GRADE: DOCUMENT NOT RELEVANT---\\n---GRADE: DOCUMENT NOT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n\"Node \\'grade_documents\\':\"\\n\\'\\\\n---\\\\n\\'\\n---ASSESS GRADED DOCUMENTS---\\n---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\\n---TRANSFORM QUERY---\\n\"Node \\'transform_query\\':\"\\n\\'\\\\n---\\\\n\\'\\n---WEB SEARCH---\\n\"Node \\'web_search_node\\':\"\\n\\'\\\\n---\\\\n\\'\\n---GENERATE---\\n\"Node \\'generate\\':\"\\n\\'\\\\n---\\\\n\\'\\n\"Node \\'__end__\\':\"\\n\\'\\\\n---\\\\n\\'\\n(\\'The AlphaCodium paper functions by proposing a code-oriented iterative flow \\'\\n \\'that involves repeatedly running and fixing generated code against \\'\\n \\'input-output tests. Its key mechanisms include generating additional data \\'\\n \\'like problem reflection and test reasoning to aid the iterative process, as \\'\\n \\'well as enriching the code generation process. AlphaCodium aims to improve \\'\\n \\'the performance of Large Language Models on code problems by following a \\'\\n \\'test-based, multi-stage approach.\\')\\nLangSmith Traces -\\nhttps://smith.langchain.com/public/f6b1716c-e842-4282-9112-1026b93e246b/r\\nhttps://smith.langchain.com/public/497c8ed9-d9e2-429e-8ada-e64de3ec26c9/r\\nComments\\nBack to top\\nPrevious\\nAgentic RAG\\nNext\\nCorrective RAG (CRAG) using local LLMs\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='2b0edb1f-74b5-4a7f-9b6a-3e9b143dbadd', metadata={}, page_content='- The `predict_base_case` function invokes a code generation chain with a context and the user\\'s question, returning a dictionary containing any necessary imports and the generated code.\\n- The `predict_langgraph` function interacts with an application to generate a response based on the user\\'s question, also returning imports and generated code from the response. Both functions aim to facilitate code generation by processing user input.\\nS3 Location: def predict_base_case(example: dict):\\n    \"\"\"Context stuffing\"\"\"\\n    solution = code_gen_chain.invoke(\\n        {\"context\": concatenated_content, \"messages\": [(\"user\", example[\"question\"])]}\\n    )\\n    return {\"imports\": solution.imports, \"code\": solution.code}\\n\\n\\ndef predict_langgraph(example: dict):\\n    \"\"\"LangGraph\"\"\"\\n    graph = app.invoke(\\n        {\"messages\": [(\"user\", example[\"question\"])], \"iterations\": 0, \"error\": \"\"}\\n    )\\n    solution = graph[\"generation\"]\\n    return {\"imports\": solution.imports, \"code\": solution.code}\\n\\nThis code imports the `evaluate` function from the `langsmith.evaluation` module, which is likely used for assessing code or models. It then initializes a list called `code_evalulator` containing two functions: `check_import` and `check_execution`, which presumably check for the successful import and execution of code, respectively. Lastly, it defines a variable `dataset_name` and assigns it the string \"lcel-teacher-eval,\" indicating the dataset that will be used for evaluation.\\nS3 Location: from langsmith.evaluation import evaluate\\n\\n# Evaluator\\ncode_evalulator = [check_import, check_execution]\\n\\n# Dataset\\ndataset_name = \"lcel-teacher-eval\"\\n\\nThis code snippet attempts to evaluate a predictive model (`predict_base_case`) using a specified dataset (`dataset_name`) and a set of evaluators (`code_evalulator`). It sets up the evaluation with a specific prefix for experiment naming and limits concurrency to 2. If the evaluation fails, it catches the exception and prints a message prompting the user to set up LangSmith.\\nS3 Location: # Run base case\\ntry:\\n    experiment_results_ = evaluate(\\n        predict_base_case,\\n        data=dataset_name,\\n        evaluators=code_evalulator,\\n        experiment_prefix=f\"test-without-langgraph-{expt_llm}\",\\n        max_concurrency=2,\\n        metadata={\\n            \"llm\": expt_llm,\\n        },\\n    )\\nexcept:\\n    print(\"Please setup LangSmith\")\\n\\nThe code attempts to evaluate a language model (likely using the LangSmith framework) with the function `evaluate`, passing in several parameters such as the prediction function `predict_langgraph`, a dataset name, and evaluators. It sets a prefix for the experiment and limits the maximum concurrency to 2. Additionally, it includes metadata about the experiment, such as the language model used and a feedback flag. If an error occurs during the evaluation, it prints a message prompting the user to set up LangSmith.\\nS3 Location: # Run with langgraph\\ntry:\\n    experiment_results = evaluate(\\n        predict_langgraph,\\n        data=dataset_name,\\n        evaluators=code_evalulator,\\n        experiment_prefix=f\"test-with-langgraph-{expt_llm}-{flag}\",\\n        max_concurrency=2,\\n        metadata={\\n            \"llm\": expt_llm,\\n            \"feedback\": flag,\\n        },\\n    )\\nexcept:\\n    print(\"Please setup LangSmith\")\\nResults:\\nLangGraph outperforms base case\\n: adding re-try loop improve performance\\nReflection did not help\\n: reflection prior to re-try regression vs just passing errors directly back to the LLM\\nGPT-4 outperforms Claude3\\n: Claude3 had 3 and 1 run fail due to tool-use error for Opus and Haiku, respectively\\nhttps://smith.langchain.com/public/78a3d858-c811-4e46-91cb-0f10ef56260b/d\\nComments\\nBack to top\\nPrevious\\nPrompt Generation from User Requirements\\nNext\\nAdaptive RAG\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='df16bca9-0606-4b3f-b3b3-10a4324ed197', metadata={}, page_content='RunnableConfig\\n|\\nEND\\n|\\nSTART\\n|\\nStateGraph\\nDefine the graph\\n¶\\nWe can now put it all together and define the graph!\\n\\nThe code defines a state graph workflow using a `StateGraph` class, which allows for the modeling of a process with nodes and edges. It creates two nodes, \"agent\" and \"tools\", and establishes \"agent\" as the entry point of the workflow. A conditional edge is also defined, allowing the workflow to transition from \"agent\" to either \"tools\" or an end state based on the result of the `should_continue` function. Finally, it creates a cyclic transition from \"tools\" back to \"agent\" and compiles the entire workflow into a runnable application.\\nS3 Location: # Define a new graph\\nworkflow = StateGraph(State)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", call_model)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Set the entrypoint as `agent`\\n# This means that this node is the first one called\\nworkflow.add_edge(START, \"agent\")\\n\\n# We now add a conditional edge\\nworkflow.add_conditional_edges(\\n    # First, we define the start node. We use `agent`.\\n    # This means these are the edges taken after the `agent` node is called.\\n    \"agent\",\\n    # Next, we pass in the function that will determine which node is called next.\\n    should_continue,\\n    # Next we pass in the path map - all the nodes this edge could go to\\n    [\"tools\", END],\\n)\\n\\nworkflow.add_edge(\"tools\", \"agent\")\\n\\n# Finally, we compile it!\\n# This compiles it into a LangChain Runnable,\\n# meaning you can use it as you would any other runnable\\napp = workflow.compile()\\nfrom\\nIPython.display\\nimport\\nImage\\n,\\ndisplay\\ndisplay\\n(\\nImage\\n(\\napp\\n.\\nget_graph\\n()\\n.\\ndraw_mermaid_png\\n()))\\nStreaming LLM Tokens\\n¶\\nYou can access the LLM tokens as they are produced by each node. \\nIn this case only the \"agent\" node produces LLM tokens.\\nIn order for this to work properly, you must be using an LLM that supports streaming as well as have set it when constructing the LLM (e.g.\\nChatOpenAI(model=\"gpt-3.5-turbo-1106\", streaming=True)\\n)\\n\\nThis code snippet is designed to interact with an asynchronous message streaming application, presumably to obtain responses from an AI model. It initializes a message from a human asking about the weather in San Francisco and listens for incoming messages in a streaming manner. As messages are received, it checks if they are from the AI (not human) and prints their content. It specifically collects chunks of AI messages, concatenating them, and if there are any tool call chunks associated with the AI message, it prints those as well.\\nS3 Location: from langchain_core.messages import AIMessageChunk, HumanMessage\\n\\ninputs = [HumanMessage(content=\"what is the weather in sf\")]\\nfirst = True\\nasync for msg, metadata in app.astream({\"messages\": inputs}, stream_mode=\"messages\"):\\n    if msg.content and not isinstance(msg, HumanMessage):\\n        print(msg.content, end=\"|\", flush=True)\\n\\n    if isinstance(msg, AIMessageChunk):\\n        if first:\\n            gathered = msg\\n            first = False\\n        else:\\n            gathered = gathered + msg\\n\\n        if msg.tool_call_chunks:\\n            print(gathered.tool_calls)\\n\\nThe code consists of a series of structured JSON-like objects that represent calls to a search tool. Each object includes the name of the tool (\\'search\\'), an associated unique identifier, and arguments which specify the search query. Initially, the search queries are empty or partial, progressively building up to a complete query for \"weather in San Francisco.\" The final output indicates the result of the search, providing a weather report for San Francisco, stating that it is \"cloudy with a chance of hail.\"\\nS3 Location: [{\\'name\\': \\'search\\', \\'args\\': {}, \\'id\\': \\'call_lfwgOci165GXplBjSDBeD4sE\\', \\'type\\': \\'tool_call\\'}]\\n[{\\'name\\': \\'search\\', \\'args\\': {}, \\'id\\': \\'call_lfwgOci165GXplBjSDBeD4sE\\', \\'type\\': \\'tool_call\\'}]\\n[{\\'name\\': \\'search\\', \\'args\\': {}, \\'id\\': \\'call_lfwgOci165GXplBjSDBeD4sE\\', \\'type\\': \\'tool_call\\'}]\\n[{\\'name\\': \\'search\\', \\'args\\': {\\'query\\': \\'\\'}, \\'id\\': \\'call_lfwgOci165GXplBjSDBeD4sE\\', \\'type\\': \\'tool_call\\'}]\\n[{\\'name\\': \\'search\\', \\'args\\': {\\'query\\': \\'weather\\'}, \\'id\\': \\'call_lfwgOci165GXplBjSDBeD4sE\\', \\'type\\': \\'tool_call\\'}]\\n[{\\'name\\': \\'search\\', \\'args\\': {\\'query\\': \\'weather in\\'}, \\'id\\': \\'call_lfwgOci165GXplBjSDBeD4sE\\', \\'type\\': \\'tool_call\\'}]\\n[{\\'name\\': \\'search\\', \\'args\\': {\\'query\\': \\'weather in San\\'}, \\'id\\': \\'call_lfwgOci165GXplBjSDBeD4sE\\', \\'type\\': \\'tool_call\\'}]\\n[{\\'name\\': \\'search\\', \\'args\\': {\\'query\\': \\'weather in San Francisco\\'}, \\'id\\': \\'call_lfwgOci165GXplBjSDBeD4sE\\', \\'type\\': \\'tool_call\\'}]\\n[{\\'name\\': \\'search\\', \\'args\\': {\\'query\\': \\'weather in San Francisco\\'}, \\'id\\': \\'call_lfwgOci165GXplBjSDBeD4sE\\', \\'type\\': \\'tool_call\\'}]\\n[\"Cloudy with a chance of hail.\"]|The| weather| in| San| Francisco| is| currently| cloudy| with| a| chance| of| hail|.|\\nAPI Reference:\\nAIMessageChunk\\n|\\nHumanMessage\\nComments\\nBack to top\\nPrevious\\nHow to stream state updates of your graph\\nNext\\nHow to stream LLM tokens (without LangChain LLMs)\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='36abb7aa-7fdf-4836-b966-8b243d867ae0', metadata={}, page_content='langchain-core >= 0.3\\n. Using\\nlangchain-core < 0.3\\nwill result in errors due to mixing of Pydantic v1 and v2\\nBaseModels\\n.\\n\\nThis code defines a function to select tools based on the last message in a conversation state. It first checks if the last message is from a human; if so, it uses the message content as the query. If the last message is a system message, it generates a query using an LLM (Language Model) to suggest tools. The results are filtered based on a simulated error condition that removes a specific tool from the selection. Additionally, it constructs a state graph to manage the flow between different nodes, incorporating the tool selection process and retry logic for robustness.\\nS3 Location: from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\\nfrom langgraph.pregel.retry import RetryPolicy\\n\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass QueryForTools(BaseModel):\\n    \"\"\"Generate a query for additional tools.\"\"\"\\n\\n    query: str = Field(..., description=\"Query for additional tools.\")\\n\\n\\ndef select_tools(state: State):\\n    \"\"\"Selects tools based on the last message in the conversation state.\\n\\n    If the last message is from a human, directly uses the content of the message\\n    as the query. Otherwise, constructs a query using a system message and invokes\\n    the LLM to generate tool suggestions.\\n    \"\"\"\\n    last_message = state[\"messages\"][-1]\\n    hack_remove_tool_condition = False  # Simulate an error in the first tool selection\\n\\n    if isinstance(last_message, HumanMessage):\\n        query = last_message.content\\n        hack_remove_tool_condition = True  # Simulate wrong tool selection\\n    else:\\n        assert isinstance(last_message, ToolMessage)\\n        system = SystemMessage(\\n            \"Given this conversation, generate a query for additional tools. \"\\n            \"The query should be a short string containing what type of information \"\\n            \"is needed. If no further information is needed, \"\\n            \"set more_information_needed False and populate a blank string for the query.\"\\n        )\\n        input_messages = [system] + state[\"messages\"]\\n        response = llm.bind_tools([QueryForTools], tool_choice=True).invoke(\\n            input_messages\\n        )\\n        query = response.tool_calls[0][\"args\"][\"query\"]\\n\\n    # Search the tool vector store using the generated query\\n    tool_documents = vector_store.similarity_search(query)\\n    if hack_remove_tool_condition:\\n        # Simulate error by removing the correct tool from the selection\\n        selected_tools = [\\n            document.id\\n            for document in tool_documents\\n            if document.metadata[\"tool_name\"] != \"Advanced_Micro_Devices\"\\n        ]\\n    else:\\n        selected_tools = [document.id for document in tool_documents]\\n    return {\"selected_tools\": selected_tools}\\n\\n\\ngraph_builder = StateGraph(State)\\ngraph_builder.add_node(\"agent\", agent)\\ngraph_builder.add_node(\"select_tools\", select_tools, retry=RetryPolicy(max_attempts=3))\\n\\ntool_node = ToolNode(tools=tools)\\ngraph_builder.add_node(\"tools\", tool_node)\\n\\ngraph_builder.add_conditional_edges(\\n    \"agent\",\\n    tools_condition,\\n)\\ngraph_builder.add_edge(\"tools\", \"select_tools\")\\ngraph_builder.add_edge(\"select_tools\", \"agent\")\\ngraph_builder.add_edge(START, \"select_tools\")\\ngraph = graph_builder.compile()\\nAPI Reference:\\nHumanMessage\\n|\\nSystemMessage\\n|\\nToolMessage\\n\\nThe code snippet imports the `Image` and `display` functions from the `IPython.display` module. It attempts to display an image of a graph in PNG format generated by the `draw_mermaid_png()` method of a `graph` object. If an exception occurs (likely due to missing dependencies for the graph rendering), it catches the exception and simply passes, allowing the program to continue without interruption.\\nS3 Location: from IPython.display import Image, display\\n\\ntry:\\n    display(Image(graph.get_graph().draw_mermaid_png()))\\nexcept Exception:\\n    # This requires some extra dependencies and is optional\\n    pass\\nuser_input\\n=\\n\"Can you give me some information about AMD in 2022?\"\\nresult\\n=\\ngraph\\n.\\ninvoke\\n({\\n\"messages\"\\n:\\n[(\\n\"user\"\\n,\\nuser_input\\n)]})\\nfor\\nmessage\\nin\\nresult\\n[\\n\"messages\"\\n]:\\nmessage\\n.\\npretty_print\\n()\\n\\nThe code snippet illustrates a sequence of interactions between a user and an AI system regarding financial information for AMD (Advanced Micro Devices) in 2022. Initially, the user requests information about AMD\\'s performance for that year. The AI uses tool calls to gather data, first querying Accenture for general revenue information and then specifically querying Advanced Micro Devices. The final output indicates that AMD had revenues of $100 in 2022.\\nS3 Location: ================================\\x1b[1m Human Message \\x1b[0m=================================\\n\\nCan you give me some information about AMD in 2022?\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================\\nTool Calls:\\n  Accenture (call_qGmwFnENwwzHOYJXiCAaY5Mx)\\n Call ID: call_qGmwFnENwwzHOYJXiCAaY5Mx\\n  Args:\\n    year: 2022\\n=================================\\x1b[1m Tool Message \\x1b[0m=================================\\nName: Accenture\\n\\nAccenture had revenues of $100 in 2022.\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================\\nTool Calls:\\n  Advanced_Micro_Devices (call_u9e5UIJtiieXVYi7Y9GgyDpn)\\n Call ID: call_u9e5UIJtiieXVYi7Y9GgyDpn\\n  Args:\\n    year: 2022\\n=================================\\x1b[1m Tool Message \\x1b[0m=================================\\nName: Advanced_Micro_Devices\\n\\nAdvanced Micro Devices had revenues of $100 in 2022.\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================\\n\\nIn 2022, AMD had revenues of $100.\\nNext steps\\n¶\\nThis guide provides a minimal implementation for dynamically selecting tools. There is a host of possible improvements and optimizations:\\nRepeating tool selection\\n: Here, we repeated tool selection by modifying the\\nselect_tools\\nnode. Another option is to equip the agent with a\\nreselect_tools\\ntool, allowing it to re-select tools at its discretion.\\nOptimizing tool selection\\n: In general, the full scope of\\nretrieval solutions\\nare available for tool selection. Additional options include:\\nGroup tools and retrieve over groups;\\nUse a chat model to select tools or groups of tool.\\nComments\\nBack to top\\nPrevious\\nHow to pass config to tools\\nNext\\nHow to add and use subgraphs\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='e69efaee-bfc5-4eef-bb6e-7d104501b0af', metadata={}, page_content=\"S3 Location: User (q/Q to quit): hi!\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================\\n\\nHello! How can I assist you today?\\nUser (q/Q to quit): rag prompt\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================\\n\\nSure! I can help you create a prompt template. To get started, could you please provide me with the following information:\\n\\n1. What is the objective of the prompt?\\n2. What variables will be passed into the prompt template?\\n3. Any constraints for what the output should NOT do?\\n4. Any requirements that the output MUST adhere to?\\n\\nOnce I have this information, I can assist you in creating the prompt template.\\nUser (q/Q to quit): 1 rag, 2 none, 3 no, 4 no\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================\\nTool Calls:\\n  PromptInstructions (call_tcz0foifsaGKPdZmsZxNnepl)\\n Call ID: call_tcz0foifsaGKPdZmsZxNnepl\\n  Args:\\n    objective: rag\\n    variables: ['none']\\n    constraints: ['no']\\n    requirements: ['no']\\n=================================\\x1b[1m Tool Message \\x1b[0m=================================\\n\\nPrompt generated!\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================\\n\\nPlease write a response using the RAG (Red, Amber, Green) rating system.\\nDone!\\nUser (q/Q to quit): red\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================\\n\\nResponse: The status is RED.\\nUser (q/Q to quit): q\\nAI: Byebye\\nComments\\nBack to top\\nPrevious\\nBuild a Customer Support Bot\\nNext\\nCode generation with RAG and self-correction\\nMade with\\nMaterial for MkDocs Insiders\"),\n",
       " Document(id='95cd49b3-f897-413f-a577-c481de08fe9a', metadata={}, page_content='Rebuild Graph at Runtime\\nSkip to content\\nRebuild Graph at Runtime\\nInitializing search\\nGitHub\\nHome\\nTutorials\\nHow-to Guides\\nConceptual Guides\\nReference\\nGitHub\\nHome\\nTutorials\\nHow-to Guides\\nHow-to Guides\\nLangGraph\\nLangGraph\\nLangGraph\\nControllability\\nPersistence\\nMemory\\nHuman-in-the-loop\\nStreaming\\nTool calling\\nSubgraphs\\nState Management\\nOther\\nPrebuilt ReAct Agent\\nLangGraph Platform\\nLangGraph Platform\\nLangGraph Platform\\nApplication Structure\\nApplication Structure\\nApplication Structure\\nHow to Set Up a LangGraph Application for Deployment\\nHow to Set Up a LangGraph Application for Deployment\\nHow to Set Up a LangGraph.js Application for Deployment\\nHow to customize Dockerfile\\nHow to test a LangGraph app locally\\nRebuild Graph at Runtime\\nRebuild Graph at Runtime\\nTable of contents\\nPrerequisites\\nDefine graphs\\nNo rebuild\\nRebuild\\nDeployment\\nAssistants\\nThreads\\nRuns\\nStreaming\\nHuman-in-the-loop\\nDouble-texting\\nWebhooks\\nCron Jobs\\nLangGraph Studio\\nTroubleshooting\\nTroubleshooting\\nTroubleshooting\\nGRAPH_RECURSION_LIMIT\\nINVALID_CONCURRENT_GRAPH_UPDATE\\nINVALID_GRAPH_NODE_RETURN_VALUE\\nMULTIPLE_SUBGRAPHS\\nConceptual Guides\\nReference\\nTable of contents\\nPrerequisites\\nDefine graphs\\nNo rebuild\\nRebuild\\nHome\\nHow-to Guides\\nLangGraph Platform\\nApplication Structure\\nRebuild Graph at Runtime\\n¶\\nYou might need to rebuild your graph with a different configuration for a new run. For example, you might need to use a different graph state or graph structure depending on the config. This guide shows how you can do this.\\nNote\\nIn most cases, customizing behavior based on the config should be handled by a single graph where each node can read a config and change its behavior based on it\\nPrerequisites\\n¶\\nMake sure to check out\\nthis how-to guide\\non setting up your app for deployment first.\\nDefine graphs\\n¶\\nLet\\'s say you have an app with a simple graph that calls an LLM and returns the response to the user. The app file directory looks like the following:\\n\\nThe provided code structure indicates a simple Python project named \"my-app.\" It contains a `requirements.txt` file that typically lists the dependencies needed for the project, ensuring that the correct packages are installed. The `.env` file is used for environment variables, often to store sensitive information like API keys or configuration settings. The `openai_agent.py` file likely contains the main code for the application, which may involve interacting with the OpenAI API to perform tasks such as generating text or processing data using AI models.\\nS3 Location: my-app/\\n|-- requirements.txt\\n|-- .env\\n|-- openai_agent.py     # code for your graph\\nwhere the graph is defined in\\nopenai_agent.py\\n.\\nNo rebuild\\n¶\\nIn the standard LangGraph API configuration, the server uses the compiled graph instance that\\'s defined at the top level of\\nopenai_agent.py\\n, which looks like the following:\\n\\nThis code snippet sets up a workflow using LangChain and LangGraph to create an AI agent. It initializes a `ChatOpenAI` model with a specified temperature for response variability. A `MessageGraph` is created to represent the workflow, where the agent node is connected to the starting point (START) and the endpoint (END) through edges. Finally, the workflow is compiled into an executable agent.\\nS3 Location: from langchain_openai import ChatOpenAI\\nfrom langgraph.graph import END, START, MessageGraph\\n\\nmodel = ChatOpenAI(temperature=0)\\n\\ngraph_workflow = MessageGraph()\\n\\ngraph_workflow.add_node(\"agent\", model)\\ngraph_workflow.add_edge(\"agent\", END)\\ngraph_workflow.add_edge(START, \"agent\")\\n\\nagent = graph_workflow.compile()\\nTo make the server aware of your graph, you need to specify a path to the variable that contains the\\nCompiledStateGraph\\ninstance in your LangGraph API configuration (\\nlanggraph.json\\n), e.g.:\\n\\nThis code snippet appears to be a configuration file, likely in JSON format, used for defining dependencies and environment settings for a project. The \"dependencies\" field specifies that the current directory (represented by \".\") is a dependency. The \"graphs\" section defines a mapping where \"openai_agent\" is associated with a specific function or object, \"agent\", located in the \"openai_agent.py\" file. Finally, the \"env\" field indicates the path to an environment variable file (\".env\") that may contain configuration settings for the environment in which the code will run.\\nS3 Location: {\\n    \"dependencies\": [\".\"],\\n    \"graphs\": {\\n        \"openai_agent\": \"./openai_agent.py:agent\",\\n    },\\n    \"env\": \"./.env\"\\n}\\nRebuild\\n¶\\nTo make your graph rebuild on each new run with custom configuration, you need to rewrite\\nopenai_agent.py\\nto instead provide a\\nfunction\\nthat takes a config and returns a graph (or compiled graph) instance. Let\\'s say we want to return our existing graph for user ID \\'1\\', and a tool-calling agent for other users. We can modify\\nopenai_agent.py\\nas follows:\\n\\nThe code defines two functions to create different graph workflows for a language model agent using the Langchain and Langgraph libraries. The `make_default_graph` function sets up a simple state graph where the agent processes messages using the `ChatOpenAI` model. The `make_alternative_graph` function creates a more complex graph that incorporates a tool for adding numbers, allowing the agent to handle tool calls within the message flow. Finally, the `make_graph` function decides which graph to create based on a user ID from the provided configuration, returning either the default or alternative graph.\\nS3 Location: from typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.graph import END, START, MessageGraph\\nfrom langgraph.graph.state import StateGraph\\nfrom langgraph.graph.message import add_messages\\nfrom langgraph.prebuilt import ToolNode\\nfrom langchain_core.tools import tool\\nfrom langchain_core.messages import BaseMessage\\nfrom langchain_core.runnables import RunnableConfig\\n\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[BaseMessage], add_messages]\\n\\n\\nmodel = ChatOpenAI(temperature=0)\\n\\ndef make_default_graph():\\n    \"\"\"Make a simple LLM agent\"\"\"\\n    graph_workflow = StateGraph(State)\\n    def call_model(state):\\n        return {\"messages\": [model.invoke(state[\"messages\"])]}\\n\\n    graph_workflow.add_node(\"agent\", call_model)\\n    graph_workflow.add_edge(\"agent\", END)\\n    graph_workflow.add_edge(START, \"agent\")\\n\\n    agent = graph_workflow.compile()\\n    return agent\\n\\n\\ndef make_alternative_graph():\\n    \"\"\"Make a tool-calling agent\"\"\"\\n\\n    @tool\\n    def add(a: float, b: float):\\n        \"\"\"Adds two numbers.\"\"\"\\n        return a + b\\n\\n    tool_node = ToolNode([add])\\n    model_with_tools = model.bind_tools([add])\\n    def call_model(state):\\n        return {\"messages\": [model_with_tools.invoke(state[\"messages\"])]}\\n\\n    def should_continue(state: State):\\n        if state[\"messages\"][-1].tool_calls:\\n            return \"tools\"\\n        else:\\n            return END\\n\\n    graph_workflow = StateGraph(State)\\n\\n    graph_workflow.add_node(\"agent\", call_model)\\n    graph_workflow.add_node(\"tools\", tool_node)\\n    graph_workflow.add_edge(\"tools\", \"agent\")\\n    graph_workflow.add_edge(START, \"agent\")\\n    graph_workflow.add_conditional_edges(\"agent\", should_continue)\\n\\n    agent = graph_workflow.compile()\\n    return agent\\n\\n\\n# this is the graph making function that will decide which graph to\\n# build based on the provided config\\ndef make_graph(config: RunnableConfig):\\n    user_id = config.get(\"configurable\", {}).get(\"user_id\")\\n    # route to different graph state / structure based on the user ID\\n    if user_id == \"1\":\\n        return make_default_graph()\\n    else:\\n        return make_alternative_graph()\\nFinally, you need to specify the path to your graph-making function (\\nmake_graph\\n) in\\nlanggraph.json\\n:\\n\\nThis code snippet is a configuration file in JSON format, likely used for defining dependencies and environment settings for a project. It specifies that the project depends on the current directory (\".\"), includes a graph definition that maps the key \"openai_agent\" to a function called `make_graph` located in the `openai_agent.py` file, and indicates that the environment variables should be loaded from a file named `.env`.\\nS3 Location: {\\n    \"dependencies\": [\".\"],\\n    \"graphs\": {\\n        \"openai_agent\": \"./openai_agent.py:make_graph\",\\n    },\\n    \"env\": \"./.env\"\\n}\\nSee more info on LangGraph API configuration file\\nhere\\nComments\\nBack to top\\nPrevious\\nHow to test a LangGraph app locally\\nNext\\nHow to Deploy to LangGraph Cloud\\nMade with\\nMaterial for MkDocs Insiders'),\n",
       " Document(id='0a1c493f-ccc4-4691-8a7a-580b4d35607c', metadata={}, page_content='S3 Location: from typing import List\\n\\nfrom typing_extensions import TypedDict\\n\\n\\nclass GraphState(TypedDict):\\n    \"\"\"\\n    Represents the state of our graph.\\n\\n    Attributes:\\n        question: question\\n        generation: LLM generation\\n        documents: list of documents\\n    \"\"\"\\n\\n    question: str\\n    generation: str\\n    documents: List[str]\\n\\nThe provided code defines a series of functions that work together to process a query by retrieving relevant documents, assessing their relevance, and generating an answer based on those documents. The `retrieve` function fetches documents related to a question, while `grade_documents` filters out irrelevant documents. The `generate` function creates a response using the relevant documents and the original question. Additionally, `transform_query` can rephrase the question if no relevant documents are found. Finally, the `decide_to_generate` and `grade_generation_v_documents_and_question` functions determine whether to generate an answer or rephrase the question based on the relevance and quality of the generated response.\\nS3 Location: ### Nodes\\n\\n\\ndef retrieve(state):\\n    \"\"\"\\n    Retrieve documents\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, documents, that contains retrieved documents\\n    \"\"\"\\n    print(\"---RETRIEVE---\")\\n    question = state[\"question\"]\\n\\n    # Retrieval\\n    documents = retriever.invoke(question)\\n    return {\"documents\": documents, \"question\": question}\\n\\n\\ndef generate(state):\\n    \"\"\"\\n    Generate answer\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, generation, that contains LLM generation\\n    \"\"\"\\n    print(\"---GENERATE---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # RAG generation\\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\\n\\n\\ndef grade_documents(state):\\n    \"\"\"\\n    Determines whether the retrieved documents are relevant to the question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with only filtered relevant documents\\n    \"\"\"\\n\\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Score each doc\\n    filtered_docs = []\\n    for d in documents:\\n        score = retrieval_grader.invoke(\\n            {\"question\": question, \"document\": d.page_content}\\n        )\\n        grade = score[\"score\"]\\n        if grade == \"yes\":\\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\\n            filtered_docs.append(d)\\n        else:\\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\\n            continue\\n    return {\"documents\": filtered_docs, \"question\": question}\\n\\n\\ndef transform_query(state):\\n    \"\"\"\\n    Transform the query to produce a better question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates question key with a re-phrased question\\n    \"\"\"\\n\\n    print(\"---TRANSFORM QUERY---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Re-write question\\n    better_question = question_rewriter.invoke({\"question\": question})\\n    return {\"documents\": documents, \"question\": better_question}\\n\\n\\n### Edges\\n\\n\\ndef decide_to_generate(state):\\n    \"\"\"\\n    Determines whether to generate an answer, or re-generate a question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Binary decision for next node to call\\n    \"\"\"\\n\\n    print(\"---ASSESS GRADED DOCUMENTS---\")\\n    state[\"question\"]\\n    filtered_documents = state[\"documents\"]\\n\\n    if not filtered_documents:\\n        # All documents have been filtered check_relevance\\n        # We will re-generate a new query\\n        print(\\n            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\\n        )\\n        return \"transform_query\"\\n    else:\\n        # We have relevant documents, so generate answer\\n        print(\"---DECISION: GENERATE---\")\\n        return \"generate\"\\n\\n\\ndef grade_generation_v_documents_and_question(state):\\n    \"\"\"\\n    Determines whether the generation is grounded in the document and answers question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Decision for next node to call\\n    \"\"\"\\n\\n    print(\"---CHECK HALLUCINATIONS---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n    generation = state[\"generation\"]\\n\\n    score = hallucination_grader.invoke(\\n        {\"documents\": documents, \"generation\": generation}\\n    )\\n    grade = score[\"score\"]\\n\\n    # Check hallucination\\n    if grade == \"yes\":\\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\\n        # Check question-answering\\n        print(\"---GRADE GENERATION vs QUESTION---\")\\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\\n        grade = score[\"score\"]\\n        if grade == \"yes\":\\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\\n            return \"useful\"\\n        else:\\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\\n            return \"not useful\"\\n    else:\\n        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\\n        return \"not supported\"\\nBuild Graph\\n¶\\nThis just follows the flow we outlined in the figure above.\\n\\nThis code snippet creates a workflow using a state graph model from the `langgraph` library. It defines nodes representing various tasks like retrieving data, grading documents, generating responses, and transforming queries. The edges between nodes establish the flow of the workflow, including conditional paths based on decisions made during the grading process. Finally, the workflow is compiled into an executable application that can be used to process inputs according to the defined state transitions.\\nS3 Location: from langgraph.graph import END, StateGraph, START\\n\\nworkflow = StateGraph(GraphState)\\n\\n# Define the nodes\\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\\nworkflow.add_node(\"generate\", generate)  # generatae\\nworkflow.add_node(\"transform_query\", transform_query)  # transform_query\\n\\n# Build graph\\nworkflow.add_edge(START, \"retrieve\")\\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\\nworkflow.add_conditional_edges(\\n    \"grade_documents\",\\n    decide_to_generate,\\n    {\\n        \"transform_query\": \"transform_query\",\\n        \"generate\": \"generate\",\\n    },\\n)\\nworkflow.add_edge(\"transform_query\", \"retrieve\")\\nworkflow.add_conditional_edges(\\n    \"generate\",\\n    grade_generation_v_documents_and_question,\\n    {\\n        \"not supported\": \"generate\",\\n        \"useful\": END,\\n        \"not useful\": \"transform_query\",\\n    },\\n)\\n\\n# Compile\\napp = workflow.compile()\\nAPI Reference:\\nEND\\n|\\nStateGraph\\n|\\nSTART\\nRun\\n¶\\n\\nThis code snippet is designed to interact with an application (presumably a language model or similar AI) that processes a given input question. It initializes a dictionary with a question about agent memory types and then streams the output from the application. For each output received, it prints the key (representing a node) along with its details. After processing all nodes, it prints the final generated response associated with the last node. The use of `pprint` helps in formatting the output for better readability.\\nS3 Location: from pprint import pprint\\n\\n# Run\\ninputs = {\"question\": \"Explain how the different types of agent memory work?\"}\\nfor output in app.stream(inputs):\\n    for key, value in output.items():\\n        # Node\\n        pprint(f\"Node \\'{key}\\':\")\\n        # Optional: print full state at each node\\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\\n    pprint(\"\\\\n---\\\\n\")\\n\\n# Final generation\\npprint(value[\"generation\"])\\n\\nThe code snippet appears to represent a sequence of operations in a large language model (LLM)-powered autonomous agent system. It includes a retrieval process where documents are checked for relevance to a question, resulting in multiple documents being deemed relevant. Following this, the system assesses these graded documents and decides to generate a response. The generation is confirmed to be grounded in the retrieved documents and addresses the initial question, leading to a final output that discusses the role of memory in such agent systems, comparing it to human memory types and suggesting mechanisms for synthesizing past information into future behavior.\\nS3 Location: ---RETRIEVE---\\n\"Node \\'retrieve\\':\"\\n\\'\\\\n---\\\\n\\'\\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n\"Node \\'grade_documents\\':\"\\n\\'\\\\n---\\\\n\\'\\n---ASSESS GRADED DOCUMENTS---\\n---DECISION: GENERATE---\\n---GENERATE---\\n\"Node \\'generate\\':\"\\n\\'\\\\n---\\\\n\\'\\n---CHECK HALLUCINATIONS---\\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\n---GRADE GENERATION vs QUESTION---\\n---DECISION: GENERATION ADDRESSES QUESTION---\\n\"Node \\'__end__\\':\"\\n\\'\\\\n---\\\\n\\'\\n(\\' In a LLM-powered autonomous agent system, memory is a key component that \\'\\n \\'enables agents to store and retrieve information. There are different types \\'\\n \\'of memory in human brains, such as sensory memory which retains impressions \\'\\n \\'of sensory information for a few seconds, and long-term memory which records \\'\\n \"experiences for extended periods (Lil\\'Log, 2023). In the context of LLM \"\\n \\'agents, memory is often implemented as an external database or memory stream \\'\\n \"(Lil\\'Log, 2023). The agent can consult this memory to inform its behavior \"\\n \\'based on relevance, recency, and importance. Additionally, reflection \\'\\n \\'mechanisms synthesize memories into higher-level inferences over time and \\'\\n \"guide the agent\\'s future behavior (Lil\\'Log, 2023).\")\\nTrace:\\nhttps://smith.langchain.com/public/4163a342-5260-4852-8602-bda3f95177e7/r\\nComments\\nBack to top\\nPrevious\\nSelf-RAG\\nNext\\nAn agent for interacting with a SQL database\\nMade with\\nMaterial for MkDocs Insiders')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_context_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f61bfa-1373-45e9-ba69-69525b0122c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c973b-b08a-4c06-a85c-4b2b92e25e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32afe4-9ba9-4a47-94c5-7caf687155f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b16217-b6eb-412f-a15a-c9724fdd2bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2228027-2b0b-4aa3-b7c8-7595e33b5ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='To build a retriever tool using Langgraph, you need to first set up a vector store with indexed documents and then create a retriever tool that can search through these documents. The following code demonstrates how to achieve this using Langgraph and related libraries. It involves loading documents from URLs, splitting them into chunks, storing them in a vector database, and then creating a retriever tool to query these documents.', imports='from langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain.tools.retriever import create_retriever_tool', code='# Define URLs to load documents from\\nurls = [\\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\\n]\\n\\n# Load documents from URLs\\nloader = WebBaseLoader()\\ndocs = [loader.load(url) for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n\\n# Split documents into chunks\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=100, chunk_overlap=50\\n)\\ndoc_splits = text_splitter.split_documents(docs_list)\\n\\n# Create a vector store and add document splits\\nvectorstore = Chroma.from_documents(\\n    documents=doc_splits,\\n    collection_name=\"rag-chroma\",\\n    embedding=OpenAIEmbeddings(),\\n)\\n\\n# Create a retriever tool\\nretriever = vectorstore.as_retriever()\\nretriever_tool = create_retriever_tool(\\n    retriever,\\n    \"retrieve_blog_posts\",\\n    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\"\\n)\\n\\ntools = [retriever_tool]')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a coding assistant with expertise in LCEL, Langgraph language. \\n \n",
    "    Here is a subset set of Langgraph documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n",
    "    question based only on the above provided documentation. Ensure any code you provide can be executed \\n \n",
    "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
    "    Then list the imports. And finally list the functioning code block. If you are unable to answer from the context give I don't know Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "\n",
    "expt_llm = \"gpt-4o\"\n",
    "llm = ChatOpenAI(temperature=0, model=expt_llm, api_key=OPENAI_API_KEY)\n",
    "code_gen_chain_oai = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"Give me code to build retreiver tool using Langgraph\"\n",
    "solution = code_gen_chain_oai.invoke(\n",
    "    {\"context\": updated_context_1, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58f3b1-336b-4a10-b082-4dd70b46b086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7856c79-5d91-4698-9f82-c69c9028a4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f400cbd-6c28-4f36-b3f5-54e88e8ec3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34e3cf-1296-4bd6-83fb-fc67f529c8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
