{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def convert_html_to_markdown(html_content):\n",
    "    \"\"\"\n",
    "    Converts HTML content in markdown cells to plain markdown and extracts links.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    plain_text = soup.get_text()  # Extract text without HTML tags\n",
    "\n",
    "    # Extract hyperlinks and format them\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        links.append(f\"[{a_tag.text}]({a_tag['href']})\")\n",
    "\n",
    "    # Combine text and hyperlinks into a markdown-like format\n",
    "    markdown_content = plain_text.strip() + \"\\n\" + \"\\n\".join(links)\n",
    "    return markdown_content\n",
    "\n",
    "def extract_and_format_with_full_markdown_context(file_path):\n",
    "    \"\"\"\n",
    "    Process a single .ipynb file, extracting code cells with surrounding markdown context.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        notebook_data = json.load(f)\n",
    "\n",
    "    processed_cells = []\n",
    "    markdown_buffer = []\n",
    "\n",
    "    # Iterate through cells and process them\n",
    "    for index, cell in enumerate(notebook_data.get('cells', [])):\n",
    "        cell_type = cell.get('cell_type')\n",
    "        source_content = ''.join(cell.get('source', []))\n",
    "\n",
    "        if cell_type == 'markdown':\n",
    "            markdown_text = convert_html_to_markdown(source_content)\n",
    "            markdown_buffer.append(markdown_text)  # Add to the markdown buffer\n",
    "        elif cell_type == 'code':\n",
    "            # Determine markdown above and below the code cell\n",
    "            markdown_above = '\\n'.join(markdown_buffer) if markdown_buffer else \"Markdown not found\"\n",
    "\n",
    "            # Look ahead to find markdown below the code cell\n",
    "            markdown_below = \"Markdown not found\"\n",
    "            for next_index in range(index + 1, len(notebook_data['cells'])):\n",
    "                next_cell = notebook_data['cells'][next_index]\n",
    "                if next_cell['cell_type'] == 'markdown':\n",
    "                    markdown_below = convert_html_to_markdown(''.join(next_cell.get('source', [])))\n",
    "                    break\n",
    "                elif next_cell['cell_type'] == 'code':\n",
    "                    break\n",
    "\n",
    "            # Add the processed cell to the list\n",
    "            processed_cells.append({\n",
    "                \"file_path\": str(file_path),\n",
    "                \"file_name\": file_path.name,  # Add file name here\n",
    "                \"cell_number\": index + 1,  # Cell number (1-indexed)\n",
    "                \"code\": source_content,\n",
    "                \"markdown_above\": markdown_above,\n",
    "                \"markdown_below\": markdown_below\n",
    "            })\n",
    "\n",
    "            # Clear the markdown buffer after processing a code cell\n",
    "            markdown_buffer = []\n",
    "\n",
    "    return processed_cells\n",
    "\n",
    "def extract_notebooks_from_repo(repo_path):\n",
    "    \"\"\"\n",
    "    Process all .ipynb files in a GitHub repository.\n",
    "    \"\"\"\n",
    "    repo_path = Path(repo_path)\n",
    "    notebook_files = list(repo_path.glob('**/*.ipynb'))\n",
    "\n",
    "    print(f\"Found {len(notebook_files)} Jupyter notebooks in the repository.\")\n",
    "\n",
    "    all_cells = []\n",
    "    for notebook_file in notebook_files:\n",
    "        processed_cells = extract_and_format_with_full_markdown_context(notebook_file)\n",
    "        all_cells.extend(processed_cells)\n",
    "\n",
    "    print(f\"Extracted {len(all_cells)} cells from notebooks.\")\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(all_cells)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import git\n",
    "repo_url = \"https://github.com/langchain-ai/langgraph\"\n",
    "repo_dir = \"./langgraph\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_repo(repo_url, repo_dir):\n",
    "    \"\"\"\n",
    "    Clone a GitHub repository with basic error handling.\n",
    "    \n",
    "    Args:\n",
    "        repo_url (str): URL of the GitHub repository to clone\n",
    "        repo_dir (str): Local directory to clone the repository into\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If repo_url is invalid\n",
    "        git.exc.GitCommandError: If cloning fails\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if not repo_url or not isinstance(repo_url, str):\n",
    "        raise ValueError(\"Invalid repository URL\")\n",
    "    \n",
    "    # Ensure clean destination\n",
    "    try:\n",
    "        if os.path.exists(repo_dir):\n",
    "            shutil.rmtree(repo_dir)\n",
    "        os.makedirs(repo_dir, exist_ok=True)\n",
    "        \n",
    "        # Clone repository\n",
    "        print(f\"Cloning repository from {repo_url} to {repo_dir}\")\n",
    "        repo = git.Repo.clone_from(repo_url, repo_dir)\n",
    "        print(\"Repository cloned successfully\")\n",
    "        return repo\n",
    "    \n",
    "    except git.exc.GitCommandError as e:\n",
    "        print(f\"Git clone failed: {e}\")\n",
    "        raise\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied when creating or accessing directory\")\n",
    "        raise\n",
    "\n",
    "def process_github_repo(repo_url, repo_dir):\n",
    "    \"\"\"\n",
    "    Comprehensive GitHub repository processing function.\n",
    "    \n",
    "    Args:\n",
    "        repo_url (str): URL of the GitHub repository\n",
    "        repo_dir (str): Local directory to clone and process the repository\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame or None: Processed repository data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if not repo_url or not repo_dir:\n",
    "            print(\"Invalid repository URL or directory\")\n",
    "            return None\n",
    "\n",
    "        # Clone repository\n",
    "        try:\n",
    "            clone_repo(repo_url, repo_dir)\n",
    "        except (ValueError, git.exc.GitCommandError, PermissionError) as clone_error:\n",
    "            print(f\"Repository cloning failed: {clone_error}\")\n",
    "            return None\n",
    "\n",
    "        # Convert to Path object for consistency\n",
    "        repo_dir_path = Path(repo_dir)\n",
    "\n",
    "        # Extract structures (assuming extract_structures_from_repo is defined elsewhere)\n",
    "        try:\n",
    "            print(\"Extracting structures from the repository...\")\n",
    "            results = extract_notebooks_from_repo(repo_dir_path)\n",
    "            \n",
    "            if results is None or len(results) == 0:\n",
    "                print(\"No structures extracted from the repository\")\n",
    "                return None\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        except Exception as extract_error:\n",
    "            print(f\"Error extracting structures: {extract_error}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as unexpected_error:\n",
    "        print(f\"Unexpected error in repository processing: {unexpected_error}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup - Always attempt to remove the repository directory\n",
    "        try:\n",
    "            if os.path.exists(repo_dir):\n",
    "                shutil.rmtree(repo_dir)\n",
    "                print(\"Cloned repository directory cleaned up\")\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"Error during cleanup: {cleanup_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning repository from https://github.com/langchain-ai/langgraph to ./langgraph\n",
      "Repository cloned successfully\n",
      "Extracting structures from the repository...\n",
      "Found 165 Jupyter notebooks in the repository.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v0/txz046_j1755wwktnwp4g4gh0000gn/T/ipykernel_11736/3037362284.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_content, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1191 cells from notebooks.\n",
      "Cloned repository directory cleaned up\n"
     ]
    }
   ],
   "source": [
    "all_functions=process_github_repo(repo_url, repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_name</th>\n",
       "      <th>cell_number</th>\n",
       "      <th>code</th>\n",
       "      <th>markdown_above</th>\n",
       "      <th>markdown_below</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>langgraph/examples/code_assistant/langgraph_co...</td>\n",
       "      <td>langgraph_code_assistant_mistral.ipynb</td>\n",
       "      <td>2</td>\n",
       "      <td>! pip install -U langchain_community langchain...</td>\n",
       "      <td># Code generation with self-correction\\n\\nAlph...</td>\n",
       "      <td>### LLM\\n\\nWe'll use the Mistral API and `Code...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>langgraph/examples/code_assistant/langgraph_co...</td>\n",
       "      <td>langgraph_code_assistant_mistral.ipynb</td>\n",
       "      <td>4</td>\n",
       "      <td>import os\\n\\nos.environ[\"TOKENIZERS_PARALLELIS...</td>\n",
       "      <td>### LLM\\n\\nWe'll use the Mistral API and `Code...</td>\n",
       "      <td>### Tracing\\n\\nOptionally, we'll use LangSmith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>langgraph/examples/code_assistant/langgraph_co...</td>\n",
       "      <td>langgraph_code_assistant_mistral.ipynb</td>\n",
       "      <td>6</td>\n",
       "      <td>os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\no...</td>\n",
       "      <td>### Tracing\\n\\nOptionally, we'll use LangSmith...</td>\n",
       "      <td>## Code Generation\\n\\nTest with structured out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>langgraph/examples/code_assistant/langgraph_co...</td>\n",
       "      <td>langgraph_code_assistant_mistral.ipynb</td>\n",
       "      <td>8</td>\n",
       "      <td># Select LLM\\nfrom langchain_core.prompts impo...</td>\n",
       "      <td>## Code Generation\\n\\nTest with structured out...</td>\n",
       "      <td>Markdown not found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>langgraph/examples/code_assistant/langgraph_co...</td>\n",
       "      <td>langgraph_code_assistant_mistral.ipynb</td>\n",
       "      <td>9</td>\n",
       "      <td>question = \"Write a function for fibonacci.\"\\n...</td>\n",
       "      <td>Markdown not found</td>\n",
       "      <td>Markdown not found</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  langgraph/examples/code_assistant/langgraph_co...   \n",
       "1  langgraph/examples/code_assistant/langgraph_co...   \n",
       "2  langgraph/examples/code_assistant/langgraph_co...   \n",
       "3  langgraph/examples/code_assistant/langgraph_co...   \n",
       "4  langgraph/examples/code_assistant/langgraph_co...   \n",
       "\n",
       "                                file_name  cell_number  \\\n",
       "0  langgraph_code_assistant_mistral.ipynb            2   \n",
       "1  langgraph_code_assistant_mistral.ipynb            4   \n",
       "2  langgraph_code_assistant_mistral.ipynb            6   \n",
       "3  langgraph_code_assistant_mistral.ipynb            8   \n",
       "4  langgraph_code_assistant_mistral.ipynb            9   \n",
       "\n",
       "                                                code  \\\n",
       "0  ! pip install -U langchain_community langchain...   \n",
       "1  import os\\n\\nos.environ[\"TOKENIZERS_PARALLELIS...   \n",
       "2  os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\no...   \n",
       "3  # Select LLM\\nfrom langchain_core.prompts impo...   \n",
       "4  question = \"Write a function for fibonacci.\"\\n...   \n",
       "\n",
       "                                      markdown_above  \\\n",
       "0  # Code generation with self-correction\\n\\nAlph...   \n",
       "1  ### LLM\\n\\nWe'll use the Mistral API and `Code...   \n",
       "2  ### Tracing\\n\\nOptionally, we'll use LangSmith...   \n",
       "3  ## Code Generation\\n\\nTest with structured out...   \n",
       "4                                 Markdown not found   \n",
       "\n",
       "                                      markdown_below  \n",
       "0  ### LLM\\n\\nWe'll use the Mistral API and `Code...  \n",
       "1  ### Tracing\\n\\nOptionally, we'll use LangSmith...  \n",
       "2  ## Code Generation\\n\\nTest with structured out...  \n",
       "3                                 Markdown not found  \n",
       "4                                 Markdown not found  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_functions)\n",
    "data = df.to_dict('records')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter the OpenAI API key securely\n",
    "api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_dict('records')\n",
    "df.head()\n",
    "from langchain.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=item['code'],\n",
    "        metadata={\n",
    "            'cell_number': item['cell_number'],\n",
    "            'code': item['code'],\n",
    "            'markdown_above': item['markdown_above'],\n",
    "            'markdown_below': item['markdown_below'],\n",
    "            'file_name': item['file_name'],\n",
    "        }\n",
    "    ) for item in data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\n\\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\\nmistral_api_key = os.getenv(\"MISTRAL_API_KEY\")  # Ensure this is set'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['code'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['# Code generation with self-correction\\n\\nAlphaCodium presented an approach for code generation that uses control flow.\\n\\nMain idea: [construct an answer to a coding question iteratively.](https://x.com/karpathy/status/1748043513156272416?s=20). \\n\\n[AlphaCodium](https://github.com/Codium-ai/AlphaCodium) iteravely tests and improves an answer on public and AI-generated tests for a particular question. \\n\\nWe will implement some of these ideas from scratch using [LangGraph](https://langchain-ai.github.io/langgraph/):\\n\\n1. We show how to route user questions to different types of documentation\\n2. We we will show how to perform inline unit tests to confirm imports and code execution work\\n3. We will show how to use LangGraph to orchestrate this\\n\\n![Screenshot 2024-05-23 at 2.17.51 PM.png](attachment:15d3ac32-cdf3-4800-a30c-f26d828d69c8.png)\\n',\n",
       "       \"### LLM\\n\\nWe'll use the Mistral API and `Codestral` instruct model, which support tool use!\\n\",\n",
       "       \"### Tracing\\n\\nOptionally, we'll use LangSmith for tracing.\\n\",\n",
       "       ..., 'Markdown not found',\n",
       "       '## Steaming events\\n\\nFor streaming events, in LangGraph you need to use `.astream` method on the `CompiledGraph`. In LangGraph Cloud this is done via passing `stream_mode=\"events\"`\\n\\n### With LangGraph\\n',\n",
       "       '### With LangGraph Cloud\\n'], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['markdown_above'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "YYou are an expert coding assistant. Based on the following context, provide a detailed answer including executable Python code:\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Make sure your response includes:\n",
    "    1. A clear explanation of the solution.\n",
    "    2. Relevant import statements.\n",
    "    3. A complete code example.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Initialize the Claude 3.5 Sonnet model\n",
    "anthropic_model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0.2, max_tokens=2048)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    retriever=retriever,\n",
    "    llm=anthropic_model,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": chat_prompt,\n",
    "        \"document_variable_name\": \"context\"\n",
    "    },\n",
    "    return_source_documents=True,  # Include source documents for reference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Code Explanation:\n",
      "I'll explain how streaming events work in Python and provide some practical examples. Streaming allows you to process data in real-time or sequentially without loading everything into memory at once.\n",
      "\n",
      "Here's a comprehensive explanation with examples:\n",
      "\n",
      "1. Basic Event Streaming\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "from typing import AsyncGenerator, Any\n",
      "from datetime import datetime\n",
      "\n",
      "class EventStream:\n",
      "    \"\"\"A simple event streaming implementation\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.listeners = []\n",
      "        \n",
      "    async def emit(self, event: Any):\n",
      "        \"\"\"Emit an event to all listeners\"\"\"\n",
      "        for listener in self.listeners:\n",
      "            await listener(event)\n",
      "            \n",
      "    def add_listener(self, listener):\n",
      "        \"\"\"Add a new listener to the stream\"\"\"\n",
      "        self.listeners.append(listener)\n",
      "        \n",
      "    def remove_listener(self, listener):\n",
      "        \"\"\"Remove a listener from the stream\"\"\"\n",
      "        self.listeners.remove(listener)\n",
      "\n",
      "# Example usage\n",
      "async def event_generator() -> AsyncGenerator[dict, None]:\n",
      "    \"\"\"Generates sample events\"\"\"\n",
      "    events = [\n",
      "        {\"type\": \"user_login\", \"user_id\": 1, \"timestamp\": datetime.now()},\n",
      "        {\"type\": \"page_view\", \"page\": \"/home\", \"timestamp\": datetime.now()},\n",
      "        {\"type\": \"button_click\", \"element\": \"submit\", \"timestamp\": datetime.now()}\n",
      "    ]\n",
      "    \n",
      "    for event in events:\n",
      "        yield event\n",
      "        await asyncio.sleep(1)  # Simulate delay between events\n",
      "\n",
      "async def event_processor(event: dict):\n",
      "    \"\"\"Processes incoming events\"\"\"\n",
      "    print(f\"Processing event: {event['type']} at {event['timestamp']}\")\n",
      "\n",
      "async def main():\n",
      "    # Create event stream\n",
      "    stream = EventStream()\n",
      "    \n",
      "    # Add event processor as listener\n",
      "    stream.add_listener(event_processor)\n",
      "    \n",
      "    # Generate and emit events\n",
      "    async for event in event_generator():\n",
      "        await stream.emit(event)\n",
      "\n",
      "# Run the example\n",
      "if __name__ == \"__main__\":\n",
      "    asyncio.run(main())\n",
      "```\n",
      "\n",
      "2. Real-time Data Streaming with Queue\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "from asyncio import Queue\n",
      "from typing import AsyncGenerator\n",
      "import random\n",
      "\n",
      "class DataStream:\n",
      "    \"\"\"A data streaming implementation using Queue\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.queue = Queue()\n",
      "        \n",
      "    async def producer(self):\n",
      "        \"\"\"Produces data for the stream\"\"\"\n",
      "        for i in range(5):\n",
      "            data = {\n",
      "                \"id\": i,\n",
      "                \"value\": random.randint(1, 100),\n",
      "                \"timestamp\": datetime.now()\n",
      "            }\n",
      "            await self.queue.put(data)\n",
      "            await asyncio.sleep(0.5)  # Simulate data generation delay\n",
      "            \n",
      "        # Signal the end of stream\n",
      "        await self.queue.put(None)\n",
      "        \n",
      "    async def consumer(self):\n",
      "        \"\"\"Consumes data from the stream\"\"\"\n",
      "        while True:\n",
      "            data = await self.queue.get()\n",
      "            \n",
      "            # Check for stream end\n",
      "            if data is None:\n",
      "                break\n",
      "                \n",
      "            # Process the data\n",
      "            print(f\"Received data: {data}\")\n",
      "            \n",
      "            self.queue.task_done()\n",
      "\n",
      "async def run_stream():\n",
      "    stream = DataStream()\n",
      "    \n",
      "    # Create producer and consumer tasks\n",
      "    producer = asyncio.create_task(stream.producer())\n",
      "    consumer = asyncio.create_task(stream.consumer())\n",
      "    \n",
      "    # Wait for both tasks to complete\n",
      "    await asyncio.gather(producer, consumer)\n",
      "\n",
      "# Run the streaming example\n",
      "if __name__ == \"__main__\":\n",
      "    asyncio.run(run_stream())\n",
      "```\n",
      "\n",
      "3. File Streaming Example\n",
      "\n",
      "```python\n",
      "def stream_large_file(filename: str, chunk_size: int = 1024):\n",
      "    \"\"\"Stream a large file in chunks\"\"\"\n",
      "    with open(filename, 'rb') as file:\n",
      "        while True:\n",
      "            chunk = file.read(chunk_size)\n",
      "            if not chunk:\n",
      "                break\n",
      "            yield chunk\n",
      "\n",
      "# Example usage for file streaming\n",
      "def process_file_stream():\n",
      "    total_bytes = 0\n",
      "    \n",
      "    # Stream the file in chunks\n",
      "    for chunk in stream_large_file('large_file.txt'):\n",
      "        # Process the chunk\n",
      "        total_bytes += len(chunk)\n",
      "        print(f\"Processed chunk of size: {len(chunk)} bytes\")\n",
      "    \n",
      "    print(f\"Total bytes processed: {total_bytes}\")\n",
      "```\n",
      "\n",
      "Key concepts to understand:\n",
      "\n",
      "1. **Asynchronous Processing**: The examples use Python's asyncio for handling asynchronous operations, which is crucial for efficient streaming.\n",
      "\n",
      "2. **Generators**: Python generators (using `yield`) are fundamental to streaming as they allow data to be generated on-demand.\n",
      "\n",
      "3. **Queue**: AsyncIO's Queue is used for managing the flow of data between producers and consumers.\n",
      "\n",
      "4. **Event Listeners**: The EventStream example shows how to implement a publish-subscribe pattern for event handling.\n",
      "\n",
      "Common use cases for streaming:\n",
      "\n",
      "- Real-time data processing\n",
      "- Large file processing\n",
      "- Message queues\n",
      "- Event-driven applications\n",
      "- Sensor data processing\n",
      "- Live data feeds\n",
      "\n",
      "To use these examples, you'll need Python 3.7+ for the asyncio features. The code can be modified based on specific needs, such as:\n",
      "\n",
      "- Adding error handling\n",
      "- Implementing backpressure\n",
      "- Adding data transformation\n",
      "- Implementing filtering\n",
      "- Adding persistence\n",
      "- Implementing retry logic\n",
      "\n",
      "Remember to handle exceptions and cleanup resources properly in production code. Also, consider using existing streaming libraries like RxPY or Apache Kafka for Python if you need more robust streaming capabilities.\n",
      "\n",
      "Referenced Documents:\n",
      "for event in app.stream(None, to_replay.config):\n",
      "    for v in event.values():\n",
      "        print(v)\n",
      "for event in app.stream(None, thread, stream_mode=\"values\"):\n",
      "    event[\"messages\"][-1].pretty_print()\n",
      "for event in app.stream(None, thread, stream_mode=\"values\"):\n",
      "    event[\"messages\"][-1].pretty_print()\n",
      "events = graph.stream(\n",
      "    {\"messages\": [(\"user\", \"How should we handle the climate crisis?\")]},\n",
      "    stream_mode=\"values\",\n",
      ")\n",
      "for i, step in enumerate(events):\n",
      "    print(f\"Step {i}\")\n",
      "    step[\"messages\n",
      "for event in app.stream(None, branch_config):\n",
      "    for v in event.values():\n",
      "        print(v)\n"
     ]
    }
   ],
   "source": [
    "# Define your query\n",
    "# query = \"How to install langchain_community library?\"\n",
    "query=\"how Steaming events in python works? can you show me the code as well\"\n",
    "# Use the chain to get a response\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the generated response\n",
    "print(\"Generated Code Explanation:\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "# Optionally, include the retrieved documents\n",
    "print(\"\\nReferenced Documents:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc.page_content[:200])  # Print a snippet of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Similar Code Snippets:\n",
      "markdown_below: ## Branch off a past state\n",
      "\n",
      "Using LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user \"version control\" changes in a workflow.\n",
      "\n",
      "Let's show how to do this to edit the state at a particular point in time. Let's update the state to instead of playing the song on Apple to play it on Spotify:\n",
      "\n",
      "markdown_above: To replay from this place we just need to pass its config back to the agent. Notice that it just resumes from right where it left all - making a tool call.\n",
      "\n",
      "cell_number: 19\n",
      "Code:\n",
      "for event in app.stream(None, to_replay.config):\n",
      "    for v in event.values():\n",
      "        print(v)\n",
      "\n",
      "markdown_below: Markdown not found\n",
      "markdown_above: **Resume**\n",
      "\n",
      "We can now call the agent again with no inputs to continue.\n",
      "\n",
      "This will run the tool as requested.\n",
      "\n",
      "Running an interrupted graph with `None` in the inputs means to `proceed as if the interruption didn't occur.`\n",
      "\n",
      "cell_number: 16\n",
      "Code:\n",
      "for event in app.stream(None, thread, stream_mode=\"values\"):\n",
      "    event[\"messages\"][-1].pretty_print()\n",
      "\n",
      "markdown_below: Markdown not found\n",
      "markdown_above: **Resume**\n",
      "\n",
      "We can now call the agent again with no inputs to continue, ie. run the tool as requested. We can see from the logs that it passes in the update args to the tool.\n",
      "\n",
      "cell_number: 22\n",
      "Code:\n",
      "for event in app.stream(None, thread, stream_mode=\"values\"):\n",
      "    event[\"messages\"][-1].pretty_print()\n",
      "\n",
      "markdown_below: ## Conclusion\n",
      "\n",
      "Congrats on building a Reflexion actor! I'll leave you with a few observations to save you some time when choosing which parts of this agent to adapt to your workflow:\n",
      "1. This agent trades off execution time for quality. It explicitly forces the agent to critique and revise the output over several steps, which usually (not always) increases the response quality but takes much longer to return a final answer\n",
      "2. The 'reflections' can be paired with additional external feedback (such as validators), to further guide the actor.\n",
      "3. In the paper, 1 environment (AlfWorld) uses external memory. It does this by storing summaries of the reflections to an external store and using them in subsequent trials/invocations.\n",
      "\n",
      "markdown_above: Markdown not found\n",
      "cell_number: 22\n",
      "Code:\n",
      "events = graph.stream(\n",
      "    {\"messages\": [(\"user\", \"How should we handle the climate crisis?\")]},\n",
      "    stream_mode=\"values\",\n",
      ")\n",
      "for i, step in enumerate(events):\n",
      "    print(f\"Step {i}\")\n",
      "    step[\"messages\"][-1].pretty_print()\n",
      "\n",
      "markdown_below: Alternatively, we could update the state to not even call a tool!\n",
      "\n",
      "markdown_above: We can then invoke with this new `branch_config` to resume running from here with changed state. We can see from the log that the tool was called with different input.\n",
      "\n",
      "cell_number: 23\n",
      "Code:\n",
      "for event in app.stream(None, branch_config):\n",
      "    for v in event.values():\n",
      "        print(v)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 5 Similar Code Snippets:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"markdown_below: {doc.metadata['markdown_below']}\")\n",
    "    print(f\"markdown_above: {doc.metadata['markdown_above']}\")\n",
    "    print(f\"cell_number: {doc.metadata[\"cell_number\"]}\")\n",
    "    print(f\"Code:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'cell_number': 2, 'code': '! pip install -U langchain_community langchain-mistralai langchain langgraph', 'markdown_above': '# Code generation with self-correction\\n\\nAlphaCodium presented an approach for code generation that uses control flow.\\n\\nMain idea: [construct an answer to a coding question iteratively.](https://x.com/karpathy/status/1748043513156272416?s=20). \\n\\n[AlphaCodium](https://github.com/Codium-ai/AlphaCodium) iteravely tests and improves an answer on public and AI-generated tests for a particular question. \\n\\nWe will implement some of these ideas from scratch using [LangGraph](https://langchain-ai.github.io/langgraph/):\\n\\n1. We show how to route user questions to different types of documentation\\n2. We we will show how to perform inline unit tests to confirm imports and code execution work\\n3. We will show how to use LangGraph to orchestrate this\\n\\n![Screenshot 2024-05-23 at 2.17.51 PM.png](attachment:15d3ac32-cdf3-4800-a30c-f26d828d69c8.png)\\n', 'markdown_below': \"### LLM\\n\\nWe'll use the Mistral API and `Codestral` instruct model, which support tool use!\\n\"}, page_content='! pip install -U langchain_community langchain-mistralai langchain langgraph'), Document(metadata={'cell_number': 3, 'code': '! pip install -U langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph', 'markdown_above': \"# Self-RAG\\n\\nSelf-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. \\n\\nIn the [paper](https://arxiv.org/abs/2310.11511), a few decisions are made:\\n\\n1. Should I retrieve from retriever, `R` -\\n\\n* Input: `x (question)` OR `x (question)`, `y (generation)`\\n* Decides when to retrieve `D` chunks with `R`\\n* Output: `yes, no, continue`\\n\\n2. Are the retrieved passages `D` relevant to the question `x` -\\n\\n* * Input: (`x (question)`, `d (chunk)`) for `d` in `D`\\n* `d` provides useful information to solve `x`\\n* Output: `relevant, irrelevant`\\n\\n3. Are the LLM generation from each chunk in `D` is relevant to the chunk (hallucinations, etc)  -\\n\\n* Input: `x (question)`, `d (chunk)`,  `y (generation)` for `d` in `D`\\n* All of the verification-worthy statements in `y (generation)` are supported by `d`\\n* Output: `{fully supported, partially supported, no support`\\n\\n4. The LLM generation from each chunk in `D` is a useful response to `x (question)` -\\n\\n* Input: `x (question)`, `y (generation)` for `d` in `D`\\n* `y (generation)` is a useful response to `x (question)`.\\n* Output: `{5, 4, 3, 2, 1}`\\n\\nWe will implement some of these ideas from scratch using [LangGraph](https://langchain-ai.github.io/langgraph/).\\n\\n![Screenshot 2024-04-01 at 12.41.50 PM.png](attachment:15cba0ab-a549-4909-8373-fb761e384eff.png)\\n\\n## Setup\\n\\nFirst let's install our required packages and set our API keys\\n\", 'markdown_below': 'Markdown not found'}, page_content='! pip install -U langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph'), Document(metadata={'cell_number': 3, 'code': '! pip install -U langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph', 'markdown_above': \"# Self-RAG\\n\\nSelf-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. \\n\\nIn the [paper](https://arxiv.org/abs/2310.11511), a few decisions are made:\\n\\n1. Should I retrieve from retriever, `R` -\\n\\n* Input: `x (question)` OR `x (question)`, `y (generation)`\\n* Decides when to retrieve `D` chunks with `R`\\n* Output: `yes, no, continue`\\n\\n2. Are the retrieved passages `D` relevant to the question `x` -\\n\\n* * Input: (`x (question)`, `d (chunk)`) for `d` in `D`\\n* `d` provides useful information to solve `x`\\n* Output: `relevant, irrelevant`\\n\\n3. Are the LLM generation from each chunk in `D` is relevant to the chunk (hallucinations, etc)  -\\n\\n* Input: `x (question)`, `d (chunk)`,  `y (generation)` for `d` in `D`\\n* All of the verification-worthy statements in `y (generation)` are supported by `d`\\n* Output: `{fully supported, partially supported, no support`\\n\\n4. The LLM generation from each chunk in `D` is a useful response to `x (question)` -\\n\\n* Input: `x (question)`, `y (generation)` for `d` in `D`\\n* `y (generation)` is a useful response to `x (question)`.\\n* Output: `{5, 4, 3, 2, 1}`\\n\\nWe will implement some of these ideas from scratch using [LangGraph](https://langchain-ai.github.io/langgraph/).\\n\\n![Screenshot 2024-04-01 at 12.41.50 PM.png](attachment:15cba0ab-a549-4909-8373-fb761e384eff.png)\\n\\n## Setup\\n\\nFirst let's install our required packages and set our API keys\\n\", 'markdown_below': 'Markdown not found'}, page_content='! pip install -U langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph'), Document(metadata={'cell_number': 3, 'code': '! pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph bs4', 'markdown_above': \"# Code generation with RAG and self-correction\\n\\nAlphaCodium presented an approach for code generation that uses control flow.\\n\\nMain idea: [construct an answer to a coding question iteratively.](https://x.com/karpathy/status/1748043513156272416?s=20). \\n\\n[AlphaCodium](https://github.com/Codium-ai/AlphaCodium) iteravely tests and improves an answer on public and AI-generated tests for a particular question. \\n\\nWe will implement some of these ideas from scratch using [LangGraph](https://langchain-ai.github.io/langgraph/):\\n\\n1. We start with a set of documentation specified by a user\\n2. We use a long context LLM to ingest it and perform RAG to answer a question based upon it\\n3. We will invoke a tool to produce a structured output\\n4. We will perform two unit tests (check imports and code execution) prior returning the solution to the user \\n\\n![Screenshot 2024-05-23 at 2.17.42 PM.png](attachment:67b615fe-0c25-4410-9d58-835982547001.png)\\n\\n## Setup\\n\\nFirst, let's install our required packages and set the API keys we will need\\n\", 'markdown_below': 'Markdown not found'}, page_content='! pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph bs4'), Document(metadata={'cell_number': 3, 'code': '! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph tavily-python', 'markdown_above': '# Corrective RAG (CRAG)\\n\\nCorrective-RAG (CRAG) is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents. \\n\\nIn the paper [here](https://arxiv.org/pdf/2401.15884.pdf), a few steps are taken:\\n\\n* If at least one document exceeds the threshold for relevance, then it proceeds to generation\\n* Before generation, it performs knowledge refinement\\n* This partitions the document into \"knowledge strips\"\\n* It grades each strip, and filters our irrelevant ones\\n* If all documents fall below the relevance threshold or if the grader is unsure, then the framework seeks an additional datasource\\n* It will use web search to supplement retrieval\\n \\nWe will implement some of these ideas from scratch using [LangGraph](https://langchain-ai.github.io/langgraph/):\\n\\n* Let\\'s skip the knowledge refinement phase as a first pass. This can be added back as a node, if desired. \\n* If *any* documents are irrelevant, let\\'s opt to supplement retrieval with web search. \\n* We\\'ll use [Tavily Search](https://python.langchain.com/docs/integrations/tools/tavily_search/) for web search.\\n* Let\\'s use query re-writing to optimize the query for web search.\\n\\n![Screenshot 2024-04-01 at 9.28.30 AM.png](attachment:683fae34-980f-43f0-a9c2-9894bebd9157.png)\\n\\n## Setup\\n\\nFirst, let\\'s download our required packages and set our API keys\\n', 'markdown_below': 'Markdown not found'}, page_content='! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph tavily-python')]\n"
     ]
    }
   ],
   "source": [
    "print(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
