{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe40ed16-345c-4aef-af06-11557dff78dd",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "This notebook details the process of extracting content from a GitHub repository of a Python library as well as web scraping the documentation of Python libraries for loading into Pinecone and answering context-specific questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9f2e5-d106-4088-ad44-4742d5d5b37b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Langgraph GitHub Repo\n",
    "\n",
    "Details the process of extracting content from the Langgraph GitHub repository, including functions from .py files, text from .md files, and cell data from .ipynb files. Additionally, we are also scraping the Langgraph docs and doing POC by loading the embeddings into Pinecone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444aeb95-b6a3-4846-b42f-957aeb645b4e",
   "metadata": {},
   "source": [
    "Setting up of OpenAI API key..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa10735f-f182-44c6-8d8c-b7671f5ad397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter the OpenAI API key securely\n",
    "api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2baff-f5fa-44cb-ac82-8c3cad6afd51",
   "metadata": {},
   "source": [
    "### Extracting all functions from Python files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b493fd8c-68fb-4c29-bcde-2cc62f8db088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DEF_PREFIXES = ['def ', 'async def ']\n",
    "NEWLINE = '\\n'\n",
    "\n",
    "def get_function_name(code):\n",
    "    \"\"\"\n",
    "    Extract function name from a line beginning with 'def' or 'async def'.\n",
    "    \"\"\"\n",
    "    for prefix in DEF_PREFIXES:\n",
    "        if code.startswith(prefix):\n",
    "            return code[len(prefix): code.index('(')]\n",
    "\n",
    "\n",
    "def get_until_no_space(all_lines, i):\n",
    "    \"\"\"\n",
    "    Get all lines until a line outside the function definition is found.\n",
    "    \"\"\"\n",
    "    ret = [all_lines[i]]\n",
    "    for j in range(i + 1, len(all_lines)):\n",
    "        if len(all_lines[j]) == 0 or all_lines[j][0] in [' ', '\\t', ')']:\n",
    "            ret.append(all_lines[j])\n",
    "        else:\n",
    "            break\n",
    "    return NEWLINE.join(ret)\n",
    "\n",
    "\n",
    "def get_functions(filepath):\n",
    "    \"\"\"\n",
    "    Get all functions in a Python file.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        all_lines = file.read().replace('\\r', NEWLINE).split(NEWLINE)\n",
    "        for i, l in enumerate(all_lines):\n",
    "            for prefix in DEF_PREFIXES:\n",
    "                if l.startswith(prefix):\n",
    "                    code = get_until_no_space(all_lines, i)\n",
    "                    function_name = get_function_name(code)\n",
    "                    yield {\n",
    "                        'code': code,\n",
    "                        'function_name': function_name,\n",
    "                        'filepath': filepath,\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "\n",
    "def extract_functions_from_repo(code_root):\n",
    "    \"\"\"\n",
    "    Extract all .py functions from the repository.\n",
    "    \"\"\"\n",
    "    code_files = list(code_root.glob('**/*.py'))\n",
    "\n",
    "    num_files = len(code_files)\n",
    "    print(f'Total number of .py files: {num_files}')\n",
    "\n",
    "    if num_files == 0:\n",
    "        print('Verify langgraph-python repo exists and code_root is set correctly.')\n",
    "        return None\n",
    "\n",
    "    all_funcs = [\n",
    "        func\n",
    "        for code_file in code_files\n",
    "        for func in get_functions(str(code_file))\n",
    "    ]\n",
    "\n",
    "    num_funcs = len(all_funcs)\n",
    "    print(f'Total number of functions extracted: {num_funcs}')\n",
    "\n",
    "    return all_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b2ad3-9396-4bcf-805e-062b07db5473",
   "metadata": {},
   "source": [
    "For testing purposes we are cloning the repo onto our local system and processing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4e48e8c-85ce-48d1-9fba-8acdde8eb453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of .py files: 189\n",
      "Total number of functions extracted: 620\n"
     ]
    }
   ],
   "source": [
    "# Define the root directory for the cloned repository\n",
    "root_dir = Path(\"/Users/pragneshanekal/Documents/Local/Northeastern-Fall-2024/Big-Data\")\n",
    "\n",
    "# Adjust the path to the specific directory within the cloned repository\n",
    "code_root = root_dir / 'langgraph'\n",
    "\n",
    "# Extract all functions from the repository\n",
    "all_funcs = extract_functions_from_repo(code_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3e02665-db83-400d-98c7-ed2e6640db6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>function_name</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def _make_regular_expression(pkg_prefix: str) ...</td>\n",
       "      <td>_make_regular_expression</td>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def _get_full_module_name(module_path, class_n...</td>\n",
       "      <td>_get_full_module_name</td>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def _get_doc_title(data: str, file_name: str) ...</td>\n",
       "      <td>_get_doc_title</td>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def _get_imports(\\n    code: str, doc_title: s...</td>\n",
       "      <td>_get_imports</td>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def comment_install_cells(notebook: nbformat.N...</td>\n",
       "      <td>comment_install_cells</td>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  def _make_regular_expression(pkg_prefix: str) ...   \n",
       "1  def _get_full_module_name(module_path, class_n...   \n",
       "2  def _get_doc_title(data: str, file_name: str) ...   \n",
       "3  def _get_imports(\\n    code: str, doc_title: s...   \n",
       "4  def comment_install_cells(notebook: nbformat.N...   \n",
       "\n",
       "              function_name                                           filepath  \n",
       "0  _make_regular_expression  /Users/pragneshanekal/Documents/Local/Northeas...  \n",
       "1     _get_full_module_name  /Users/pragneshanekal/Documents/Local/Northeas...  \n",
       "2            _get_doc_title  /Users/pragneshanekal/Documents/Local/Northeas...  \n",
       "3              _get_imports  /Users/pragneshanekal/Documents/Local/Northeas...  \n",
       "4     comment_install_cells  /Users/pragneshanekal/Documents/Local/Northeas...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_funcs)\n",
    "data = df.to_dict('records')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "669d31c2-564d-465c-8752-31e1b24b8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=item['code'],\n",
    "        metadata={\n",
    "            'function_name': item['function_name'],\n",
    "            'file_path': item['filepath']\n",
    "        }\n",
    "    ) for item in data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "091edfd5-1ca8-4ec6-8dae-60e36315d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111b149-17b7-431c-8242-244838bc0e54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create embeddings and store into Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b766dc75-7768-4447-bde4-efcb9731d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46e52324-a2a3-4889-ae42-9a0b38e82ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant specialized in generating code based on requirements. \n",
    "Your task is to analyze the given Python code snippets and generate new Python code that meets the specified requirements.\n",
    "The response MUST only include Python code and a one line description of the Python code.\n",
    "\"\"\"\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "832b7cc9-4be0-41b9-a0d0-68be8e442812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Code:\n",
      "To define a new State in Langgraph, you can create a new class that inherits from TypedDict. Inside this class, you can define the fields that represent the state variables you want to use. Here is an example of how you can define a new State in Langgraph:\n",
      "\n",
      "```python\n",
      "from typing import TypedDict\n",
      "\n",
      "class MyState(TypedDict):\n",
      "    foo: int\n",
      "    bar: str\n",
      "    # Add more fields as needed\n",
      "```\n",
      "\n",
      "In this example, `MyState` is a new State class that has two fields: `foo` of type `int` and `bar` of type `str`. You can add more fields as needed for your specific use case.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "question = \"How do I define a new State in Langgraph?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "# Display the generated code\n",
    "print(\"Generated Code:\")\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ada2cc5c-952e-41e8-ab77-66356866eb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Similar Code Snippets:\n",
      "Function: rt_graph\n",
      "File: /Users/pragneshanekal/Documents/Local/Northeastern-Fall-2024/Big-Data/langgraph/libs/langgraph/tests/test_utils.py\n",
      "Code:\n",
      "def rt_graph() -> CompiledGraph:\n",
      "    class State(TypedDict):\n",
      "        foo: int\n",
      "        node_run_id: int\n",
      "\n",
      "    graph = StateGraph(State)\n",
      "    graph.add_node(node)\n",
      "    graph.set_entry_point(\"node\")\n",
      "    graph.add_edge(\"node\", END)\n",
      "    return graph.compile()\n",
      "\n",
      "Function: test_debug_subgraphs\n",
      "File: /Users/pragneshanekal/Documents/Local/Northeastern-Fall-2024/Big-Data/langgraph/libs/langgraph/tests/test_pregel.py\n",
      "Code:\n",
      "State(TypedDict): messages: Annotated[list[str], operator.add]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the top 5 similar code snippets\n",
    "print(\"\\nTop 5 Similar Code Snippets:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"Function: {doc.metadata['function_name']}\")\n",
    "    print(f\"File: {doc.metadata['file_path']}\")\n",
    "    print(f\"Code:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b26c92-c20d-4d6d-9f1a-d80144e9458a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Extract .md files from GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed7286ae-725a-4819-b894-0ceb7ed3ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Markdown element prefixes\n",
    "HEADING_PREFIXES = ['#', '##', '###', '####', '#####', '######']\n",
    "NEWLINE = '\\n'\n",
    "\n",
    "def get_markdown_elements(filepath):\n",
    "    \"\"\"\n",
    "    Extract structured elements (e.g., headings, paragraphs) from a Markdown (.md) file.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        all_lines = file.read().split(NEWLINE)\n",
    "        for line in all_lines:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                if any(line.startswith(prefix) for prefix in HEADING_PREFIXES):\n",
    "                    # Extract heading level and text\n",
    "                    heading_level = len(line.split(' ')[0])\n",
    "                    heading_text = line[heading_level:].strip()\n",
    "                    yield {\n",
    "                        'type': 'heading',\n",
    "                        'level': heading_level,\n",
    "                        'content': heading_text,\n",
    "                        'filepath': filepath,\n",
    "                    }\n",
    "                else:\n",
    "                    # Treat non-heading lines as paragraph content\n",
    "                    yield {\n",
    "                        'type': 'paragraph',\n",
    "                        'content': line,\n",
    "                        'filepath': filepath,\n",
    "                    }\n",
    "\n",
    "def extract_markdown_from_repo(md_root):\n",
    "    \"\"\"\n",
    "    Extract all elements from .md files in the repository.\n",
    "    \"\"\"\n",
    "    md_files = list(md_root.glob('**/*.md'))\n",
    "\n",
    "    num_files = len(md_files)\n",
    "    print(f'Total number of .md files: {num_files}')\n",
    "\n",
    "    if num_files == 0:\n",
    "        print('Verify the repository exists and md_root is set correctly.')\n",
    "        return None\n",
    "\n",
    "    all_elements = [\n",
    "        element\n",
    "        for md_file in md_files\n",
    "        for element in get_markdown_elements(md_file)\n",
    "    ]\n",
    "\n",
    "    num_elements = len(all_elements)\n",
    "    print(f'Total number of elements extracted: {num_elements}')\n",
    "\n",
    "    return all_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2157272-c8bb-49f6-b7c8-efee79b1292a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of .md files: 101\n",
      "Total number of elements extracted: 10848\n"
     ]
    }
   ],
   "source": [
    "# Extract all functions from the repository\n",
    "all_funcs = extract_markdown_from_repo(code_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24b48430-d467-40ec-a86d-1caa623e7d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>level</th>\n",
       "      <th>content</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heading</td>\n",
       "      <td>1.0</td>\n",
       "      <td>🦜🕸️LangGraph</td>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>![Version](https://img.shields.io/pypi/v/langg...</td>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[![Downloads](https://static.pepy.tech/badge/l...</td>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[![Open Issues](https://img.shields.io/github/...</td>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[![Docs](https://img.shields.io/badge/docs-lat...</td>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type  level                                            content  \\\n",
       "0    heading    1.0                                       🦜🕸️LangGraph   \n",
       "1  paragraph    NaN  ![Version](https://img.shields.io/pypi/v/langg...   \n",
       "2  paragraph    NaN  [![Downloads](https://static.pepy.tech/badge/l...   \n",
       "3  paragraph    NaN  [![Open Issues](https://img.shields.io/github/...   \n",
       "4  paragraph    NaN  [![Docs](https://img.shields.io/badge/docs-lat...   \n",
       "\n",
       "                                            filepath  \n",
       "0  /Users/pragneshanekal/Documents/Local/Northeas...  \n",
       "1  /Users/pragneshanekal/Documents/Local/Northeas...  \n",
       "2  /Users/pragneshanekal/Documents/Local/Northeas...  \n",
       "3  /Users/pragneshanekal/Documents/Local/Northeas...  \n",
       "4  /Users/pragneshanekal/Documents/Local/Northeas...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_funcs)\n",
    "data = df.to_dict('records')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4637fb-a3d9-4e2f-ae29-3837019a5d7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Extract information from .ipynb notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "839759be-354d-4f4d-b879-8c3ce6eca30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def convert_html_to_markdown(html_content):\n",
    "    \"\"\"\n",
    "    Converts HTML content in markdown cells to plain markdown and extracts links.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    plain_text = soup.get_text()  # Extract text without HTML tags\n",
    "\n",
    "    # Extract hyperlinks and format them\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        links.append(f\"[{a_tag.text}]({a_tag['href']})\")\n",
    "\n",
    "    # Combine text and hyperlinks into a markdown-like format\n",
    "    markdown_content = plain_text.strip() + \"\\n\" + \"\\n\".join(links)\n",
    "    return markdown_content\n",
    "\n",
    "def extract_and_format_with_full_markdown_context(file_path):\n",
    "    \"\"\"\n",
    "    Process a single .ipynb file, extracting code cells with surrounding markdown context.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        notebook_data = json.load(f)\n",
    "\n",
    "    processed_cells = []  \n",
    "    markdown_buffer = []  \n",
    "\n",
    "    # Iterate through cells and process them\n",
    "    for index, cell in enumerate(notebook_data.get('cells', [])):\n",
    "        cell_type = cell.get('cell_type')\n",
    "        source_content = ''.join(cell.get('source', []))\n",
    "\n",
    "        if cell_type == 'markdown':\n",
    "            markdown_text = convert_html_to_markdown(source_content)\n",
    "            markdown_buffer.append(markdown_text)  # Add to the markdown buffer\n",
    "        elif cell_type == 'code':\n",
    "            # Determine markdown above and below the code cell\n",
    "            markdown_above = '\\n'.join(markdown_buffer) if markdown_buffer else None\n",
    "\n",
    "            # Look ahead to find markdown below the code cell\n",
    "            markdown_below = None\n",
    "            for next_index in range(index + 1, len(notebook_data['cells'])):\n",
    "                next_cell = notebook_data['cells'][next_index]\n",
    "                if next_cell['cell_type'] == 'markdown':\n",
    "                    markdown_below = convert_html_to_markdown(''.join(next_cell.get('source', [])))\n",
    "                    break\n",
    "                elif next_cell['cell_type'] == 'code':\n",
    "                    break\n",
    "\n",
    "            # Add the processed cell to the list\n",
    "            processed_cells.append({\n",
    "                \"file_path\": file_path,\n",
    "                \"cell_number\": index + 1,  # Cell number (1-indexed)\n",
    "                \"code\": source_content,\n",
    "                \"markdown_above\": markdown_above,\n",
    "                \"markdown_below\": markdown_below\n",
    "            })\n",
    "\n",
    "            # Clear the markdown buffer after processing a code cell\n",
    "            markdown_buffer = []\n",
    "\n",
    "    return processed_cells\n",
    "\n",
    "def extract_notebooks_from_repo(repo_path):\n",
    "    \"\"\"\n",
    "    Process all .ipynb files in a GitHub repository.\n",
    "    \"\"\"\n",
    "    repo_path = Path(repo_path)\n",
    "    notebook_files = list(repo_path.glob('**/*.ipynb'))\n",
    "\n",
    "    print(f\"Found {len(notebook_files)} Jupyter notebooks in the repository.\")\n",
    "\n",
    "    all_cells = []\n",
    "    for notebook_file in notebook_files:\n",
    "        processed_cells = extract_and_format_with_full_markdown_context(notebook_file)\n",
    "        all_cells.extend(processed_cells)\n",
    "\n",
    "    print(f\"Extracted {len(all_cells)} cells from notebooks.\")\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(all_cells)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "021aefda-f19b-4bab-b3e2-752286a73f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 165 Jupyter notebooks in the repository.\n",
      "Extracted 1191 cells from notebooks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gs/w6x0hhbx3jb0kwqhpdt7f9sr0000gn/T/ipykernel_17563/220044497.py:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_content, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "# Extract all functions from the repository\n",
    "all_funcs = extract_notebooks_from_repo(code_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0d08caa-6bf3-4543-96d1-f41858268071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>cell_number</th>\n",
       "      <th>code</th>\n",
       "      <th>markdown_above</th>\n",
       "      <th>markdown_below</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "      <td>2</td>\n",
       "      <td>%%capture --no-stderr\\n%pip install -U langgra...</td>\n",
       "      <td># LangGraph Quick Start\\n\\nIn this comprehensi...</td>\n",
       "      <td>Next, set your API keys:\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "      <td>4</td>\n",
       "      <td>import getpass\\nimport os\\n\\n\\ndef _set_env(va...</td>\n",
       "      <td>Next, set your API keys:\\n</td>\n",
       "      <td>Set up LangSmith for LangGraph development\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "      <td>7</td>\n",
       "      <td>from typing import Annotated\\n\\nfrom typing_ex...</td>\n",
       "      <td>Set up LangSmith for LangGraph development\\n\\n...</td>\n",
       "      <td>Note\\n\\n    The first thing you do when you de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "      <td>10</td>\n",
       "      <td>from langchain_anthropic import ChatAnthropic\\...</td>\n",
       "      <td>Note\\n\\n    The first thing you do when you de...</td>\n",
       "      <td>**Notice** how the `chatbot` node function tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/pragneshanekal/Documents/Local/Northeas...</td>\n",
       "      <td>12</td>\n",
       "      <td>graph_builder.add_edge(START, \"chatbot\")</td>\n",
       "      <td>**Notice** how the `chatbot` node function tak...</td>\n",
       "      <td>Similarly, set a `finish` point. This instruct...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  cell_number  \\\n",
       "0  /Users/pragneshanekal/Documents/Local/Northeas...            2   \n",
       "1  /Users/pragneshanekal/Documents/Local/Northeas...            4   \n",
       "2  /Users/pragneshanekal/Documents/Local/Northeas...            7   \n",
       "3  /Users/pragneshanekal/Documents/Local/Northeas...           10   \n",
       "4  /Users/pragneshanekal/Documents/Local/Northeas...           12   \n",
       "\n",
       "                                                code  \\\n",
       "0  %%capture --no-stderr\\n%pip install -U langgra...   \n",
       "1  import getpass\\nimport os\\n\\n\\ndef _set_env(va...   \n",
       "2  from typing import Annotated\\n\\nfrom typing_ex...   \n",
       "3  from langchain_anthropic import ChatAnthropic\\...   \n",
       "4           graph_builder.add_edge(START, \"chatbot\")   \n",
       "\n",
       "                                      markdown_above  \\\n",
       "0  # LangGraph Quick Start\\n\\nIn this comprehensi...   \n",
       "1                         Next, set your API keys:\\n   \n",
       "2  Set up LangSmith for LangGraph development\\n\\n...   \n",
       "3  Note\\n\\n    The first thing you do when you de...   \n",
       "4  **Notice** how the `chatbot` node function tak...   \n",
       "\n",
       "                                      markdown_below  \n",
       "0                         Next, set your API keys:\\n  \n",
       "1  Set up LangSmith for LangGraph development\\n\\n...  \n",
       "2  Note\\n\\n    The first thing you do when you de...  \n",
       "3  **Notice** how the `chatbot` node function tak...  \n",
       "4  Similarly, set a `finish` point. This instruct...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_funcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45231d8a-9dce-4564-8fcd-edec6470327b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Airflow Documentation\n",
    "\n",
    "Details the process of scraping the documentation of Airflow and storing into Pinecone vector store and getting relevant context for further querying and response generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "497d5402-0b27-457f-8977-e878ed1fb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbefcb95-74e8-4551-af7c-97baa1d9f1cf",
   "metadata": {},
   "source": [
    "### Scraping the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "533ab215-387b-4914-a690-0ee1a0a9790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = RecursiveUrlLoader(\n",
    "    \"https://airflow.apache.org/docs/apache-airflow/stable/index.html\",\n",
    "    max_depth=5,\n",
    "    prevent_outside=True,\n",
    "    extractor=lambda x: Soup(x, \"html.parser\").text,\n",
    "    base_url=\"https://airflow.apache.org/docs/apache-airflow/stable/\"\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72ba509c-e0b2-412c-bb1d-2f3c85f3714a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://airflow.apache.org/docs/apache-airflow/stable/ui.html',\n",
       " 'content_type': 'text/html',\n",
       " 'title': 'UI / Screenshots — Airflow Documentation',\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[30].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aba94814-5d60-4f6d-b71c-e72fa5e4aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "52c814ff-40ea-4d7a-985e-d52b10d87d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Pinecone API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter the OpenAI API key securely\n",
    "api_key = getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"PINECONE_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "946fefde-6f1e-438e-916b-d92811f20a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c6ca0f-0f82-4a98-b794-b3ac7eff0415",
   "metadata": {},
   "source": [
    "### Creating embedding and storing into Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d896b0ef-e5b3-49e1-916c-5f2b667f90c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY)\n",
    "pinecone_api_key = PINECONE_API_KEY\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "69c7e663-19b4-4c62-bb4f-76e68b940e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 43021, which is longer than the specified 10000\n",
      "Created a chunk of size 25000, which is longer than the specified 10000\n",
      "Created a chunk of size 11084, which is longer than the specified 10000\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 10000,\n",
    "    chunk_overlap  = 200\n",
    ")\n",
    "text = text_splitter.create_documents([concatenated_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5555f58c-f134-4804-80fb-c85e3cb6dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = [chunk for chunk in text if len(chunk.page_content) <= 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b1e47ebe-25fe-4edc-8682-72e3a91b05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "                name='airflow-docs-2',\n",
    "                dimension=3072,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b7b80c22-e8b1-4183-aad2-add1d1eb227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'airflow-docs-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "937f8683-4eb5-4d0b-8cad-bedba1759cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "615c741e-f746-42ef-bb79-9c152e86047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dcadff32-223c-4876-bacb-8d551be927c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3832f592-0f2f-4af9-95fb-8de0517c078a',\n",
       " 'f8dd91d4-3802-47c8-b6c3-27c7ad173cb7',\n",
       " 'a0bf628d-f80e-453c-8956-ad13e9ce23b4',\n",
       " 'ace800f6-9be5-4bbe-a61e-b997a5a7bd0d',\n",
       " '4050627a-7fda-4513-97ca-e6380f8560ea',\n",
       " 'f19d8c38-f295-4c16-be50-59b84d6197e2',\n",
       " 'a090e9f4-19b0-4ab4-8cb9-48514af6873e',\n",
       " 'b2d0f997-2876-4753-826b-e8714dea5ecc',\n",
       " '2752bfff-2da7-45f3-add2-b639d9099680',\n",
       " 'c719d4ac-7ee4-4011-ad61-c3a921fbb7b3',\n",
       " 'cf4e8d4d-d1c9-42a7-b2f9-5527d0f2bb32',\n",
       " 'eafa962f-0507-42b2-a5ac-61016303ab95',\n",
       " 'e6dc935a-d9fd-44b4-a34e-ce6f2028a956',\n",
       " '18d8ca63-9211-432b-bf0d-ee4cf79f2eab',\n",
       " 'e54fcf65-34db-4bae-8d21-217d6eea70c7',\n",
       " '744ca851-7401-4db7-ab7d-0a29a5ddb7a1',\n",
       " 'c29dcb38-b7eb-41c6-a491-fb9fd0da55c3',\n",
       " '385bdc75-f5bf-4234-bdc8-6041eea82d0a',\n",
       " 'd7916a5d-264f-4972-972c-cc11ea723c12',\n",
       " '0a3d54c5-ffd9-4988-8153-48e3d99d905d',\n",
       " 'd7991e33-f5d9-42a7-b3f9-8f9efeef69b2',\n",
       " '67c9af2c-e260-4160-bc2b-520be0fd22d4',\n",
       " 'ba6160b1-259c-493a-82d5-0b24ecc0b9b3',\n",
       " '924c2849-2ca1-4781-90a5-238c8e338e4a',\n",
       " '08f9ed36-d4e5-4c05-9efb-8ff5f2ed501c',\n",
       " '2cc94b4b-4d5f-4add-b6ba-5dc3da4bae23',\n",
       " '5a1bf433-0870-403e-aac1-abcc6af47fa0',\n",
       " '34b90d20-82e8-4ad2-83ff-3bae2f5ef888',\n",
       " '3157e15f-dab7-4c6c-8da9-155b6b762781',\n",
       " '1a51a59e-a1be-4e9a-814e-5cfd8230816f',\n",
       " '46127d96-b0e3-4cd8-9bb5-825b9ff73b8d',\n",
       " 'a349e32c-9341-417b-a7db-91f92f426f89',\n",
       " '6bcf57e9-28f5-41e0-bc93-8cc0e4f9d5ba',\n",
       " '2a2c5dc1-99d0-4be6-86f7-c0c68197d9b2',\n",
       " 'b8e240d9-7418-4cd6-a269-bb96531c170b',\n",
       " '3a1b0a96-af3f-4966-a4b7-f422ea050fa0',\n",
       " 'f8275f62-be49-42f9-b696-6c6c1802c690',\n",
       " 'c5ab62ab-6ff3-4e23-9629-0f3a82ffb71e',\n",
       " '6ad63f54-4557-46e1-847c-e708243c9f62',\n",
       " 'a254d02a-685d-499a-abce-586ead8bfdd8',\n",
       " 'cc59d883-b5da-43ac-a896-4c7993989b5c',\n",
       " '3562f1e7-c334-4787-ba17-2de59a8006f0',\n",
       " '3663acec-4565-413c-8e70-c867fd407432',\n",
       " 'fdeae65e-e319-4554-8cfe-c4094a62e284',\n",
       " 'ae7cacf2-eace-437c-9aef-2f9d3d007e07',\n",
       " '4b8960d0-da37-496e-9708-93c9d133f267',\n",
       " '8c9ec0e0-e861-4478-8bf6-cb4ad9c64eba',\n",
       " '6b9a0573-6820-4475-b40f-bfc1935ac45d',\n",
       " 'c8c5fbfa-c455-4272-9212-3c3a2b7dd002',\n",
       " '3884ca10-f94d-4f70-8872-0268b9a11869',\n",
       " 'b0695253-129a-45a9-a71c-b8fe6f3350e0',\n",
       " '83750d56-459e-4191-9d50-323d07f6cb8e',\n",
       " '6cf80b36-47eb-4c58-9bd6-c57317e26c12',\n",
       " '7f6b3ae0-0e6b-4a4f-9ceb-ebdbfb8f196a',\n",
       " '0caed096-b3a8-4ebf-8336-73f92990c4be',\n",
       " '300c1911-c1c9-442c-aa98-83ade5a70c33',\n",
       " 'e7156b05-bbfd-4965-9f74-59b02b89498f',\n",
       " 'dd13e481-c6b0-4bdb-a8ba-1134b54afbc5',\n",
       " 'dc02a596-97a0-4808-aa41-55ccef5c7388',\n",
       " 'b5a00d86-d253-46ca-b115-f7fa670ce760',\n",
       " '29fa9441-4206-4e00-817e-f26fe580f081',\n",
       " 'd3cd5f10-5724-4588-8080-603b06c4981c',\n",
       " 'bd278bbd-3511-4583-9d2c-c37ec7488e2e',\n",
       " '6061f1de-83ea-4e76-9253-0b9f1216c83a',\n",
       " 'ced6a005-f5a5-4978-8314-557a6a7e73af',\n",
       " '536cd7ce-1d20-4cea-b2df-d867b4459ba2',\n",
       " 'd62d52be-94fb-4fce-afa6-55cc55204a0f',\n",
       " 'e6e24b15-e155-4422-af51-4fdffb1286d4',\n",
       " '1b857d9e-2b3c-4e29-8e1f-27d17317022d',\n",
       " '454ae52c-0e42-4e23-8b22-fb2f26d90db6',\n",
       " 'e2e07918-0f8f-4cf5-acd8-0fd8bbe1634b',\n",
       " '982e269c-b406-414e-a9fa-9bda9dbf4136',\n",
       " '8122b16e-c754-455e-b717-5211412af29e',\n",
       " '4edc9214-9963-4feb-9b5f-e1dcb14c095b',\n",
       " 'c544fca0-3abc-43b6-9bdf-74a61c7baa42',\n",
       " '954a016c-5b41-4cc6-a4a4-8174bf0fbf78',\n",
       " '9611b4ca-242c-48ac-93dc-02775cb5a9f6',\n",
       " '53482b9b-464e-4da2-a26d-b2f255c70ecc',\n",
       " '9d4d5ec0-1c88-468a-8f6e-24aa8eb4a764',\n",
       " '4b8a2015-2ed6-4fd3-8c2f-b2430fca1824',\n",
       " '6ebe51d3-17c7-41bf-b87b-750d78012822',\n",
       " '05fd970d-88c2-478b-a5c6-2cd5f37d00d6',\n",
       " 'ad63a3da-aaa3-4e30-9ae6-7e347702e80d',\n",
       " 'd815a232-ac85-4a3a-8751-de5ea624c6b0',\n",
       " '4704d132-0480-4773-a001-f82c284719c9',\n",
       " '7d15bd60-229d-4447-b7d2-46fb0adb65ca',\n",
       " '7470ad04-c25a-4383-ab6d-0d13e3a6f67b',\n",
       " '06870bad-2c5e-49b9-b07f-caf74b0e2220',\n",
       " '95aaca34-9b12-4993-9c65-d1d53b57a3fa',\n",
       " 'a1ad2ba8-eacf-4cea-a01d-6adc117ccff0',\n",
       " 'd89222a3-4e89-4541-9139-679733e3943f',\n",
       " '7799cf2b-d053-4da5-aa16-b1bd841ad2bd',\n",
       " 'c896a552-6aa0-4162-b178-82a73018d884',\n",
       " '1473bd5b-58fa-4c2c-9481-bb7b8b65d942',\n",
       " '06e551a7-89cb-40ae-884f-ece0002fc959',\n",
       " 'afa4e9b8-a2e1-4a3e-b781-59699b9d0547',\n",
       " '639d82c2-a3a7-448b-8342-2eb33800445e',\n",
       " '949d0411-d86a-4ef0-b0fc-cde249cf31e5',\n",
       " '11c581f2-1b3f-4a7d-b890-2294700ab458',\n",
       " '5afa1634-a7a3-4fa3-8d86-cb48a06d104e',\n",
       " '341f62cd-2d5b-40fd-9841-60c5beb3a1b8',\n",
       " 'a3e3d2df-7bea-4bd2-85a2-9e89eaff6f6b',\n",
       " '1e2d0c08-cd63-477e-bad1-72261a9f138f',\n",
       " '139106f7-6c33-4afb-8567-4f0359abd89f',\n",
       " '79bed83d-9b9e-4caf-ac7e-291edfbac193',\n",
       " 'e68399d4-28b7-4004-8b1b-7fca3c73d52a',\n",
       " '220e1673-f3d8-4eb9-b996-4a76e1d13a61',\n",
       " 'de1116a6-c041-4412-9fde-0986cde9f950',\n",
       " '6bdc9aa3-dc38-42e5-bc9f-93ff9c3d4083',\n",
       " 'ca3d74f6-4b22-48e8-b0e3-35736d106bac',\n",
       " '7387591a-81ac-4445-a0df-bb7b0601e5c0',\n",
       " '5eece8b7-7367-4821-a5b1-3589fa0a6cd9',\n",
       " 'c4a6bc13-5d31-42be-88bf-2c989276602a',\n",
       " '0232edca-1ad7-4dee-a00e-2685f8b1c636',\n",
       " '24cc8766-828d-4643-a0ef-34afae200d23',\n",
       " 'ef11d26e-9548-41d5-bced-6de5fa6839da',\n",
       " '111eb644-3755-4d36-af74-7291b8e61edc',\n",
       " 'b5690496-7a02-4127-b1c6-b6f16a9ee69e',\n",
       " 'ba61ec65-d4c8-4fe8-84ab-828dae477ead',\n",
       " '6d32a03d-1da6-486b-88b4-9241fae4bc63',\n",
       " '2167898c-9648-4ffa-ac3e-551b7d88263a',\n",
       " '68212ea3-1e9c-41c8-a5e6-ae7b1671e076',\n",
       " '6f9a0258-0a28-4d8b-bf02-6d134cd9d946',\n",
       " 'fca29a98-be9a-4832-996b-1ae287482fac',\n",
       " 'd71e8277-248e-40c4-b585-e86cf14d89c4',\n",
       " 'a2a2406e-6b0d-47c2-8f9a-384670040dec',\n",
       " 'cb729a13-5c60-4e22-92c9-b21db88d4256',\n",
       " '7bbe051e-b76a-448c-895e-82cf4e63f642',\n",
       " 'f5cd01f7-44cc-46c2-96f7-8cf012d1d2c9',\n",
       " 'd0257c6a-17ea-45a0-8089-3c9a062db47f',\n",
       " '689cd468-7299-4273-a8d2-c87601c8be36',\n",
       " '28e9e1df-8ecf-4d1d-905c-75eee1d9115e',\n",
       " '2617eeff-c4d2-4483-9f8d-89996fbd4759',\n",
       " 'bf1fda5a-bc5a-47c9-ba92-24c89f0cfd01',\n",
       " 'e7d5a344-966f-43f1-bfac-70efd3b85a65',\n",
       " '83950c9b-9168-4495-b76d-b478574400df',\n",
       " 'edf23e9a-6a33-4486-ad04-aff30ce9282e',\n",
       " 'e836b398-4846-4815-8955-26dfa269c3d7',\n",
       " 'bd73e812-ffcd-486a-a30e-8b4d862a03c7',\n",
       " '57960e7e-9c53-4ab8-b581-47a63dbce2e5',\n",
       " 'c34e118d-6fd0-4792-9063-aa5b1501ea43']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d12b006e-7d23-4f29-914c-e04606a16203",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "context = retriever.invoke(\"What is the latest Version in Airflow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d93d865e-448c-4c25-b21d-bddb103e7660",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ad63a3da-aaa3-4e30-9ae6-7e347702e80d', metadata={}, page_content='Generating Cluster Config\\nairflow.providers.google.cloud.operators.bigquery.BigQueryGetDatasetTablesOperator\\n\\n\\nChanges in amazon provider package\\nMigration of AWS components\\nairflow.providers.amazon.aws.hooks.emr.EmrHook\\nairflow.providers.amazon.aws.operators.emr_add_steps.EmrAddStepsOperator\\nairflow.providers.amazon.aws.operators.emr_create_job_flow.EmrCreateJobFlowOperator\\nairflow.providers.amazon.aws.operators.emr_terminate_job_flow.EmrTerminateJobFlowOperator\\nairflow.providers.amazon.aws.operators.batch.AwsBatchOperator\\nairflow.providers.amazon.aws.sensors.athena.AthenaSensor\\nairflow.providers.amazon.aws.hooks.s3.S3Hook\\n\\n\\nChanges in other provider packages\\nChanged return type of list_prefixes and list_keys methods in S3Hook\\nRemoved HipChat integration\\nairflow.providers.salesforce.hooks.salesforce.SalesforceHook\\nairflow.providers.apache.pinot.hooks.pinot.PinotAdminHook.create_segment\\nairflow.providers.apache.hive.hooks.hive.HiveMetastoreHook.get_partitions\\nairflow.providers.ftp.hooks.ftp.FTPHook.list_directory\\nairflow.providers.postgres.hooks.postgres.PostgresHook.copy_expert\\nairflow.providers.opsgenie.operators.opsgenie_alert.OpsgenieAlertOperator\\nairflow.providers.imap.hooks.imap.ImapHook\\nairflow.providers.imap.sensors.imap_attachment.ImapAttachmentSensor\\nairflow.providers.http.hooks.http.HttpHook\\nairflow.providers.cloudant.hooks.cloudant.CloudantHook\\nairflow.providers.snowflake\\n\\n\\nOther changes\\nStandardized “extra” requirements\\nSimplify the response payload of endpoints /dag_stats and /task_stats\\n\\n\\nAirflow 1.10.15 (2021-03-17)\\nSignificant Changes\\nBug Fixes\\nImprovements\\nDoc only changes\\n\\n\\nAirflow 1.10.14 (2020-12-10)\\nSignificant Changes\\n[scheduler] max_threads config has been renamed to [scheduler] parsing_processes\\nAirflow CLI changes in line with 2.0\\n\\n\\nBug Fixes\\nImprovements\\nDoc only changes\\n\\n\\nAirflow 1.10.13 (2020-11-25)\\nSignificant Changes\\nTimeSensor is now timezone aware\\nRemoved Kerberos support for HDFS hook\\nUnify user session lifetime configuration\\nAdding Operators, Hooks and Sensors via Airflow Plugins is deprecated\\n\\n\\nNew Features\\nBug Fixes\\nImprovements\\nDeprecations\\nDoc only changes\\n\\n\\nAirflow 1.10.12 (2020-08-25)\\nSignificant Changes\\nClearing tasks skipped by SkipMixin will skip them\\nThe pod_mutation_hook function will now accept a kubernetes V1Pod object\\npod_template_file option now available in the KubernetesPodOperator\\n\\n\\nNew Features\\nBug Fixes\\nImprovements\\nDoc only changes\\n\\n\\nAirflow 1.10.11 (2020-07-10)\\nSignificant Changes\\nUse NULL as default value for dag.description\\nRestrict editing DagRun State in the old UI (Flask-admin based UI)\\nExperimental API will deny all request by default.\\nXCom Values can no longer be added or changed from the Webserver\\nDefault for run_as_user configured has been changed to 50000 from 0\\n\\n\\nNew Features\\nBug Fixes\\nImprovements\\nDoc only changes\\n\\n\\nAirflow 1.10.10 (2020-04-09)\\nSignificant Changes\\nSetting Empty string to a Airflow Variable will return an empty string\\nMake behavior of none_failed trigger rule consistent with documentation\\nAdd new trigger rule none_failed_or_skipped\\nSuccess Callback will be called when a task in marked as success from UI\\n\\n\\nNew Features\\nBug Fixes\\nImprovements\\nMisc/Internal\\nDoc only changes\\n\\n\\nAirflow 1.10.9 (2020-02-07)\\nSignificant Changes\\nBug Fixes\\n\\n\\nAirflow 1.10.8 (2020-02-07)\\nSignificant Changes\\nFailure callback will be called when task is marked failed\\n\\n\\nNew Features\\nImprovements\\nBug Fixes\\nMisc/Internal\\nDoc only changes\\n\\n\\nAirflow 1.10.7 (2019-12-24)\\nSignificant Changes\\nChanges in experimental API execution_date microseconds replacement\\nInfinite pool size and pool size query optimization\\nViewer won’t have edit permissions on DAG view.\\nGoogle Cloud Storage Hook\\n\\n\\nNew Features\\nImprovements\\nBug Fixes\\nMisc/Internal\\nDoc only changes\\n\\n\\nAirflow 1.10.6 (2019-10-28)\\nSignificant Changes\\nBaseOperator::render_template function signature changed\\nChanges to aws_default Connection’s default region\\nSome DAG Processing metrics have been renamed\\n\\n\\nNew Features\\nImprovements\\nBug Fixes\\nDoc-only changes\\nMisc/Internal\\n\\n\\nAirflow 1.10.5 (2019-09-04)\\nSignificant Changes\\nNew Features\\nImprovements\\nBug fixes\\nMisc/Internal\\nDoc-only changes\\n\\n\\nAirflow 1.10.4 (2019-08-06)\\nSignificant Changes\\nExport MySQL timestamps as UTC\\nChanges to DatastoreHook\\nChanges to GoogleCloudStorageHook\\nChanges in writing Logs to Elasticsearch\\nRemoval of non_pooled_task_slot_count and non_pooled_backfill_task_slot_count\\npool config option in Celery section to support different Celery pool implementation\\nChange to method signature in BaseOperator and DAG classes\\nFor BaseOperator\\nFor DAG\\n\\n\\nNew Features\\nImprovement\\nBug fixes\\nMisc/Internal\\nDoc-only changes\\n\\n\\nAirflow 1.10.3 (2019-04-09)\\nSignificant Changes\\nNew dag_discovery_safe_mode config option\\nRedisPy dependency updated to v3 series\\nSLUGIFY_USES_TEXT_UNIDECODE or AIRFLOW_GPL_UNIDECODE no longer required\\nNew sync_parallelism config option in [celery] section\\nRename of BashTaskRunner to StandardTaskRunner\\nModification to config file discovery\\nChanges in Google Cloud related operators\\nChanges in Google Cloud related hooks\\nChanged behaviour of using default value when accessing variables\\nRemoval of airflow_home config setting\\nChange of two methods signatures in GCPTransferServiceHook\\nMoved two classes to different modules\\nFixed typo in –driver-class-path in SparkSubmitHook\\n\\n\\nNew Feature\\nImprovement\\nBug fixes\\nMisc/Internal\\nDoc-only changes\\n\\n\\nAirflow 1.10.2 (2019-01-19)\\nSignificant Changes\\nNew dag_processor_manager_log_location config option\\nDAG level Access Control for new RBAC UI\\nModification to ts_nodash macro\\nSemantics of next_ds/prev_ds changed for manually triggered runs\\nUser model changes\\nCustom auth backends interface change\\nSupport autodetected schemas to GoogleCloudStorageToBigQueryOperator\\n\\n\\nNew features\\nImprovements\\nBug fixes\\nDoc-only changes\\n\\n\\nAirflow 1.10.1 (2018-11-13)\\nSignificant Changes\\nmin_file_parsing_loop_time config option temporarily disabled\\nStatsD Metrics\\nEMRHook now passes all of connection’s extra to CreateJobFlow API\\nLDAP Auth Backend now requires TLS\\n\\n\\nNew features\\nImprovements\\nDoc-only changes\\nBug fixes\\n\\n\\nAirflow 1.10.0 (2018-08-03)\\nSignificant Changes\\nReplace DataProcHook.await calls to DataProcHook.wait\\nSetting UTF-8 as default mime_charset in email utils\\nAdd a configuration variable(default_dag_run_display_number) to control numbers of dag run for display\\nDefault executor for SubDagOperator is changed to SequentialExecutor\\nNew Webserver UI with Role-Based Access Control\\nSetting up Authentication\\nCreating an Admin Account\\nUsing your new UI\\nBreaking changes\\n\\n\\nairflow.contrib.sensors.hdfs_sensors renamed to airflow.contrib.sensors.hdfs_sensor\\nMySQL setting required\\nCelery config\\nGCP Dataflow Operators\\nBigQuery Hooks and Operator\\nRedshift to S3 Operator\\nGoogle cloud connection string\\nLogging Configuration\\nChange of per-task log path\\n\\n\\nChangelog\\n\\n\\nAirflow 1.9.0 (2018-01-02)\\nSignificant Changes\\nSSH Hook updates, along with new SSH Operator & SFTP Operator\\nS3Hook switched to use Boto3\\nLogging update\\nA quick recap about logging\\nChanges in Airflow Logging\\nTemplate path of the file_task_handler\\nI’m using S3Log or GCSLogs, what do I do!?\\n\\n\\nNew Features\\nDask Executor\\n\\n\\nDeprecated Features\\n\\n\\nChangelog\\n\\n\\nAirflow 1.8.2 (2017-09-04)\\nSignificant Changes\\nChangelog\\n\\n\\nAirflow 1.8.1 (2017-05-09)\\nSignificant Changes\\nChangelog\\n\\n\\nAirflow 1.8.0 (2017-03-12)\\nSignificant Changes\\nDatabase\\nUpgrade systemd unit files\\nTasks not starting although dependencies are met due to stricter pool checking\\nLess forgiving scheduler on dynamic start_date\\nNew and updated scheduler options\\nchild_process_log_directory\\nrun_duration\\nnum_runs\\nmin_file_process_interval\\nmin_file_parsing_loop_time\\ndag_dir_list_interval\\ncatchup_by_default\\n\\n\\nFaulty DAGs do not show an error in the Web UI\\nNew DAGs are paused by default\\nAirflow Context variable are passed to Hive config if conf is specified\\nGoogle Cloud Operator and Hook alignment\\nDeprecated Features\\nKnown Issues\\n\\n\\nChangelog\\n\\n\\nAirflow 1.7.1.2 (2016-05-20)\\nSignificant Changes\\nChanges to Configuration\\nEmail configuration change\\nS3 configuration change\\n\\n\\nChangelog\\n\\n\\nAirflow 1.7.1 (2016-05-19)\\n\\n\\nSuggest a change on this page\\n\\nWant to be a part of Apache Airflow?\\n\\nJoin community\\n\\n© The Apache Software Foundation \\n\\n\\nLicense\\n\\n\\nDonate\\n\\n\\nThanks\\n\\n\\nSecurity\\n\\n\\n            Apache Airflow, Apache, Airflow, the Airflow logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation.\\n            All other products or name brands are trademarks of their respective holders, including The Apache Software Foundation.\\n        \\n\\n\\n --- \\n\\n  \\n\\n\\nAirflow’s release process and version policy — Airflow Documentation\\n\\n                            Community\\n                        \\n\\n                            Meetups\\n                        \\n\\n                            Documentation\\n                        \\n\\n                            Use Cases\\n                        \\n\\n                            Announcements\\n                        \\n\\n                            Blog\\n                        \\n\\n                            Ecosystem\\n                        \\n\\n                                Community\\n                            \\n\\n                                Meetups\\n                            \\n\\n                                Documentation\\n                            \\n\\n                                Use Cases\\n                            \\n\\n                                Announcements\\n                            \\n\\n                                Blog\\n                            \\n\\n                                Ecosystem\\n                            \\n\\n\\nContent\\n\\n\\nVersion: 2.10.3\\n\\n\\nContent\\n\\nOverview\\nQuick Start\\nInstallation of Airflow®\\nSecurity\\nTutorials\\nHow-to Guides\\nUI / Screenshots\\nCore Concepts\\nAuthoring and Scheduling\\nAdministration and Deployment\\nIntegration\\nPublic Interface of Airflow\\nBest Practices\\nFAQ\\nTroubleshooting\\nRelease Policies\\nDeprecation policy\\nExperimental features'),\n",
       " Document(id='4b8a2015-2ed6-4fd3-8c2f-b2430fca1824', metadata={}, page_content='Airflow 1.7.1 (2016-05-19)¶\\n\\nFix : Don’t treat premature tasks as could_not_run tasks\\nAIRFLOW-92 Avoid unneeded upstream_failed session closes apache/airflow#1485\\nAdd logic to lock DB and avoid race condition\\nHandle queued tasks from multiple jobs/executors\\nAIRFLOW-52 Warn about overwriting tasks in a DAG\\nFix corner case with joining processes/queues (#1473)\\n[AIRFLOW-52] Fix bottlenecks when working with many tasks\\nAdd columns to toggle extra detail in the connection list view.\\nLog the number of errors when importing DAGs\\nLog dagbag metrics duplicate messages in queue into StatsD (#1406)\\nClean up issue template (#1419)\\ncorrect missed arg.foreground to arg.daemon in cli\\nReinstate imports for github enterprise auth\\nUse os.execvp instead of subprocess.Popen for the webserver\\nRevert from using “–foreground” to “–daemon”\\nImplement a Cloudant hook\\nAdd missing args to airflow clear\\nFixed a bug in the scheduler: num_runs used where runs intended\\nAdd multiprocessing support to the scheduler\\nPartial fix to make sure next_run_date cannot be None\\nSupport list/get/set variables in the CLI\\nProperly handle BigQuery booleans in BigQuery hook.\\nAdded the ability to view XCom variables in webserver\\nChange DAG.tasks from a list to a dict\\nAdd support for zipped dags\\nStop creating hook on instantiating of S3 operator\\nUser subquery in views to find running DAGs\\nPrevent DAGs from being reloaded on every scheduler iteration\\nAdd a missing word to docs\\nDocument the parameters of DbApiHook\\nadded oracle operator with existing oracle hook\\nAdd PyOpenSSL to Google Cloud gcp_api.\\nRemove executor error unit test\\nAdd DAG inference, deferral, and context manager\\nDon’t return error when writing files to Google cloud storage.\\nFix GCS logging for gcp_api.\\nEnsure attr is in scope for error message\\nFixing misnamed PULL_REQUEST_TEMPLATE\\nExtract non_pooled_task_slot_count into a configuration param\\nUpdate plugins.rst for clarity on the example (#1309)\\nFix s3 logging issue\\nAdd twitter feed example dag\\nGitHub ISSUE_TEMPLATE & PR_TEMPLATE cleanup\\nReduce logger verbosity\\nAdding a PR Template\\nAdd Lucid to list of users\\nFix usage of asciiart\\nUse session instead of outdated main_session for are_dependencies_met\\nFix celery flower port allocation\\nFix for missing edit actions due to flask-admin upgrade\\nFix typo in comment in prioritize_queued method\\nAdd HipchatOperator\\nInclude all example dags in backfill unit test\\nMake sure skipped jobs are actually skipped\\nFixing a broken example dag, example_skip_dag.py\\nAdd consistent and thorough signal handling and logging\\nAllow Operators to specify SKIPPED status internally\\nUpdate docstring for executor trap unit test\\nDoc: explain the usage of Jinja templating for templated params\\nDon’t schedule runs before the DAG’s start_date\\nFix infinite retries with pools, with test\\nFix handling of deadlocked jobs\\nShow only Airflow’s deprecation warnings\\nSet DAG_FOLDER for unit tests\\nMissing comma in setup.py\\nDeprecate args and kwargs in BaseOperator\\nRaise deep scheduler exceptions to force a process restart.\\nChange inconsistent example DAG owners\\nFix module path of send_email_smtp in configuration\\nadded Gentner Lab to list of users\\nIncrease timeout time for unit test\\nFix reading strings from conf\\nCHORE - Remove Trailing Spaces\\nFix SSHExecuteOperator crash when using a custom ssh port\\nAdd note about Airflow components to template\\nRewrite BackfillJob logic for clarity\\nAdd unit tests\\nFix miscellaneous bugs and clean up code\\nFix logic for determining DagRun states\\nMake SchedulerJob not run EVERY queued task\\nImprove BackfillJob handling of queued/deadlocked tasks\\nIntroduce ignore_depends_on_past parameters\\nUse Popen with CeleryExecutor\\nRename user table to users to avoid conflict with postgres\\nBeware of negative pool slots.\\nAdd support for calling_format from boto to S3_Hook\\nAdd PyPI meta data and sync version number\\nSet dags_are_paused_at_creation’s default value to True\\nResurface S3Log class eaten by rebase/push -f\\nAdd missing session.commit() at end of initdb\\nValidate that subdag tasks have pool slots available, and test\\nUse urlparse for remote GCS logs, and add unit tests\\nMake webserver worker timeout configurable\\nFixed scheduling for @once interval\\nUse psycopg2’s API for serializing postgres cell values\\nMake the provide_session decorator more robust\\nupdate link to Lyft’s website\\nuse num_shards instead of partitions to be consistent with batch ingestion\\nAdd documentation links to README\\nUpdate docs with separate configuration section\\nFix airflow.utils deprecation warning code being Python 3 incompatible\\nExtract dbapi cell serialization into its own method\\nSet Postgres autocommit as supported only if server version is < 7.4\\nUse refactored utils module in unit test imports\\nAdd changelog for 1.7.0\\nUse LocalExecutor on Travis if possible\\nremove unused logging,errno, MiniHiveCluster imports\\nremove extra import of logging lib\\nFix required gcloud version\\nRefactoring utils into smaller submodules\\nProperly measure number of task retry attempts\\nAdd function to get configuration as dict, plus unit tests\\nMerge branch ‘master’ into hivemeta_sasl\\nAdd wiki link to README.md\\n[hotfix] make email.Utils > email.utils for py3\\nAdd the missing “Date” header to the warning e-mails\\nAdd the missing “Date” header to the warning e-mails\\nCheck name of SubDag class instead of class itself\\n[hotfix] removing repo_token from .coveralls.yml\\nSet the service_name in coverals.yml\\nFixes #1223\\nUpdate Airflow docs for remote logging\\nAdd unit tests for trapping Executor errors\\nMake sure Executors properly trap errors\\nFix HttpOpSensorTest to use fake request session\\nLinting\\nAdd an example on pool usage in the documentation\\nAdd two methods to bigquery hook’s base cursor: run_table_upsert, which adds a table or updates an existing table; and run_grant_dataset_view_access, which grants view access to a given dataset for a given table.\\nTasks references upstream and downstream tasks using strings instead of references\\nFix typos in models.py\\nFix broken links in documentation\\n[hotfix] fixing the Scheduler CLI to make dag_id optional\\nUpdate link to Common Pitfalls wiki page in README\\nAllow disabling periodic committing when inserting rows with DbApiHook\\nadded Glassdoor to “who uses airflow”\\nFix typo preventing from launching webserver\\nDocumentation badge\\nFixing ISSUE_TEMPLATE name to include .md suffix\\nAdding an ISSUE_TEMPLATE to ensure that issues are adequately defined\\nLinting & debugging\\nRefactoring the CLI to be data-driven\\nUpdating the Bug Reporting protocol in the Contributing.md file\\nFixing the docs\\nClean up references to old session\\nremove session reference\\nresolve conflict\\nclear xcom data when task instance starts\\nreplace main_session with @provide_session\\nAdd extras to installation.rst\\nChanges to Contributing to reflect more closely the current state of development.\\nModifying README to link to the wiki committer list\\ndocs: fixes a spelling mistake in default config\\nSet killMode to ‘control-group’ for webservice.service\\nSet KillMode to ‘control-group’ for worker.service\\nLinting\\nFix WebHdfsSensor\\nAdding more licenses to pass checks\\nfixing landscape’s config\\n[hotfix] typo that made it in master\\n[hotfix] fixing landscape requirement detection\\nMake testing on hive conditional\\nMerge remote-tracking branch ‘upstream/master’ into minicluster\\nUpdate README.md\\nThrowing in a few license to pass the build\\nAdding a reqs.txt for landscape.io\\nPointing to a reqs file\\nSome linting\\nAdding a .landscape.yml file\\nbadge for PyPI version\\nAdd license and ignore for sql and csv\\nUse correct connection id\\nUse correct table name\\nProvide data for ci tests\\nNew badge for showing staleness of reqs\\nRemoving requirements.txt as it is uni-dimensional\\nMake it work on py3\\nRemove decode for logging\\nAlso keep py2 compatible\\nMore py3 fixes\\nConvert to bytes for py3 compat\\nMake sure to be py3 compatible\\nUse unicodecsv to make it py3 compatible\\nReplace tab with spaces Remove unused import\\nMerge remote-tracking branch ‘upstream/master’\\nSupport decimal types in MySQL to GCS\\nMake sure to write binary as string can be unicode\\nIgnore metastore\\nMore impyla fixes\\nTest HivemetaStore if Python 2\\nAllow users to set hdfs_namenode_principal in HDFSHook config\\nAdd tests for Hiveserver2 and fix some issues from impyla\\nMerge branch ‘impyla’ into minicluster\\nThis patch allows for testing of hive operators and hooks. Sasl is used (NoSasl in connection string is not possible). Tests have been adjusted.\\nTreat SKIPPED and SUCCESS the same way when evaluating depends_on_past=True\\nfix bigquery hook\\nVersion cap for gcp_api\\nFix typo when returning VerticaHook\\nAdding fernet key to use it as part of stdout commands\\nAdding support for ssl parameters.  (picking up from jthomas123)\\nMore detail in error message.\\nMake sure paths don’t conflict bc of trailing /\\nChange gcs_hook to self.hook\\nRefactor remote log read/write and add GCS support\\nOnly use multipart upload in S3Hook if file is large enough\\nMerge branch ‘airbnb/master’\\nAdd GSSAPI SASL to HiveMetaStoreHook.\\nAdd warning for deprecated setting\\nUse kerberos_service_name = ‘hive’ as standard instead of ‘impala’.\\nUse GSSAPI instead of KERBEROS and provide backwards compatibility\\nISSUE-1123 Use impyla instead of pyhs2\\nSet celery_executor to use queue name as exchange\\n\\nPrevious\\n\\n\\nNext\\n\\n\\nWas this entry helpful?\\n\\n\\nRelease Notes\\nAirflow 2.10.3 (2024-11-04)\\nSignificant Changes\\nBug Fixes\\nMiscellaneous\\nDoc Only Changes\\n\\n\\nAirflow 2.10.2 (2024-09-18)\\nSignificant Changes\\nBug Fixes\\nMiscellaneous\\nDoc Only Changes\\n\\n\\nAirflow 2.10.1 (2024-09-05)\\nSignificant Changes\\nBug Fixes\\nMiscellaneous\\nDoc Only Changes'),\n",
       " Document(id='954a016c-5b41-4cc6-a4a4-8174bf0fbf78', metadata={}, page_content='Copy the logging configuration from airflow/config_templates/airflow_logging_settings.py.\\nPlace it in a directory inside the Python import path PYTHONPATH. If you are using Python 2.7, ensuring that any __init__.py files exist so that it is importable.\\nUpdate the config by setting the path of REMOTE_BASE_LOG_FOLDER explicitly in the config. The REMOTE_BASE_LOG_FOLDER key is not used anymore.\\nSet the logging_config_class to the filename and dict. For example, if you place custom_logging_config.py on the base of your PYTHONPATH, you will need to set logging_config_class = custom_logging_config.LOGGING_CONFIG in your config as Airflow 1.8.\\n\\n\\nNew Features¶\\n\\nDask Executor¶\\nA new DaskExecutor allows Airflow tasks to be run in Dask Distributed clusters.\\n\\nDeprecated Features¶\\nThese features are marked for deprecation. They may still work (and raise a DeprecationWarning), but are no longer\\nsupported and will be removed entirely in Airflow 2.0\\n\\nIf you’re using the google_cloud_conn_id or dataproc_cluster argument names explicitly in contrib.operators.Dataproc{*}Operator(s), be sure to rename them to gcp_conn_id or cluster_name, respectively. We’ve renamed these arguments for consistency. (AIRFLOW-1323)\\npost_execute() hooks now take two arguments, context and result\\n(AIRFLOW-886)\\nPreviously, post_execute() only took one argument, context.\\n\\ncontrib.hooks.gcp_dataflow_hook.DataFlowHook starts to use --runner=DataflowRunner instead of DataflowPipelineRunner, which is removed from the package google-cloud-dataflow-0.6.0.\\nThe pickle type for XCom messages has been replaced by json to prevent RCE attacks.\\nNote that JSON serialization is stricter than pickling, so if you want to e.g. pass\\nraw bytes through XCom you must encode them using an encoding like base64.\\nBy default pickling is still enabled until Airflow 2.0. To disable it\\nset enable_xcom_pickling = False in your Airflow config.\\n\\n\\nChangelog¶')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f61c5f-5075-4dfa-9967-e9cdbec3cf31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Querying the Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cc654af4-eeeb-4c4f-92c4-bc13cf88513e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='The latest version of Airflow mentioned in the provided documentation is 2.10.3, released on 2024-11-04.', imports='', code=\"latest_version = '2.10.3'\")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is a full set of Airflow documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n",
    "    question based only on the above provided documentation. Ensure any code you provide can be executed \\n \n",
    "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
    "    Then list the imports. And finally list the functioning code block. If you are unable to answer from the context give I don't know Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "\n",
    "expt_llm = \"gpt-4o\"\n",
    "llm = ChatOpenAI(temperature=0, model=expt_llm, api_key=OPENAI_API_KEY)\n",
    "code_gen_chain_oai = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"What is the latest Version in Airflow?\"\n",
    "solution = code_gen_chain_oai.invoke(\n",
    "    {\"context\": context, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
